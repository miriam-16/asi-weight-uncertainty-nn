{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [ASI Project] Weight Uncertainty in Neural Networks  \n**Authors**: Miriam Lamari, Francesco Giannuzzo  \n","metadata":{}},{"cell_type":"code","source":"import csv\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport wandb\n\n#from torchwu.bayes_linear import BayesLinear\n#from torchwu.utils.minibatch_weighting import minibatch_weight\n#from torchwu.utils.variational_approximator import variational_approximator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.707314Z","iopub.execute_input":"2025-05-17T12:25:13.708007Z","iopub.status.idle":"2025-05-17T12:25:13.712293Z","shell.execute_reply.started":"2025-05-17T12:25:13.707984Z","shell.execute_reply":"2025-05-17T12:25:13.711489Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Minibatches\n**minibatch_weight**(batch_idx: int, num_batches: int)","metadata":{}},{"cell_type":"code","source":"def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.713564Z","iopub.execute_input":"2025-05-17T12:25:13.714079Z","iopub.status.idle":"2025-05-17T12:25:13.726535Z","shell.execute_reply.started":"2025-05-17T12:25:13.714055Z","shell.execute_reply":"2025-05-17T12:25:13.725853Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Variational Approximation","metadata":{}},{"cell_type":"markdown","source":"## Variational Inference","metadata":{}},{"cell_type":"code","source":"from typing import Any, Optional\n\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass BayesianModule(nn.Module):\n\n    \"\"\"Base class for BNN to enable certain behaviour.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def kld(self, *args):\n        raise NotImplementedError('BayesianModule::kld()')\n\n\ndef variational_approximator(model: nn.Module) -> nn.Module:\n\n    def kl_divergence(self) -> Tensor:\n\n        kl = 0\n        for module in self.modules():\n            if isinstance(module, BayesianModule):\n                kl += module.kl_divergence\n\n        return kl\n\n    # add `kl_divergence` to the model\n    setattr(model, 'kl_divergence', kl_divergence)\n\n    def elbo(self,\n             inputs: Tensor,\n             targets: Tensor,\n             criterion: Any,\n             n_samples: int,\n             w_complexity: Optional[float] = 1.0) -> Tensor:\n\n        loss = 0\n        for sample in range(n_samples):\n            outputs = self(inputs)\n            loss += criterion(outputs, targets)\n            loss += self.kl_divergence() * w_complexity\n\n        return loss / n_samples\n\n    # add `elbo` to the model\n    setattr(model, 'elbo', elbo)\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.727524Z","iopub.execute_input":"2025-05-17T12:25:13.727789Z","iopub.status.idle":"2025-05-17T12:25:13.741872Z","shell.execute_reply.started":"2025-05-17T12:25:13.727773Z","shell.execute_reply":"2025-05-17T12:25:13.741098Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Scale Mixture Prior","metadata":{}},{"cell_type":"code","source":"import functools as ft\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass ScaleMixture(nn.Module):\n\n    \"\"\"Scale Mixture Prior.\n\n    Section 3.3 of the 'Weight Uncertainty in Neural Networks' paper\n    proposes the use of a Scale Mixture prior for use in variational\n    inference - this being a fixed-form prior.\n\n    The authors note that, should the parameters be allowed to adjust\n    during training, the prior changes rapidly and attempts to capture\n    the empirical distribution of the weights. As a result the prior\n    learns to fit poor initial parameters and struggles to improve.\n    \"\"\"\n\n    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n\n        \"\"\"Scale Mixture Prior.\n\n        The authors of 'Weight Uncertainty in Neural Networks' note:\n\n            sigma1 > sigma2:\n                provides a heavier tail in the prior density than is\n                seen in a plain Gaussian prior.\n            sigma2 << 1.0:\n                causes many of the weights to a priori tightly\n                concentrate around zero.\n\n        Parameters\n        ----------\n        pi : float\n            Parameter used to scale the two Gaussian distributions.\n        sigma1 : float\n            Standard deviation of the first normal distribution.\n        sigma2 : float\n            Standard deviation of the second normal distribution.\n        \"\"\"\n\n        super().__init__()\n\n        self.pi = pi\n        self.sigma1 = sigma1\n        self.sigma2 = sigma2\n\n        self.normal1 = torch.distributions.Normal(0, sigma1)\n        self.normal2 = torch.distributions.Normal(0, sigma2)\n\n    def log_prior(self, w: Tensor) -> Tensor:\n\n        \"\"\"Log Likelihood of the weight according to the prior.\n\n        Calculates the log likelihood of the supplied weight given the\n        prior distribution - the scale mixture of two Gaussians.\n\n        Parameters\n        ----------\n        w : Tensor\n            Weight to be used to calculate the log likelihood.\n\n        Returns\n        -------\n        Tensor\n            Log likelihood of the weights from the prior distribution.\n        \"\"\"\n\n        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n\n        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n        log_prob = torch.log(p_scalemixture).sum()\n\n        return log_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.742596Z","iopub.execute_input":"2025-05-17T12:25:13.742813Z","iopub.status.idle":"2025-05-17T12:25:13.756689Z","shell.execute_reply.started":"2025-05-17T12:25:13.742793Z","shell.execute_reply":"2025-05-17T12:25:13.756120Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Gaussian Variational Inference","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass GaussianVariational(nn.Module):\n    def __init__(self, mu: Tensor, rho: Tensor) -> None:\n\n        super().__init__()\n\n        self.mu = nn.Parameter(mu)\n        self.rho = nn.Parameter(rho)\n\n        self.w = None\n        self.sigma = None\n\n        self.normal = torch.distributions.Normal(0, 1)\n\n    def sample(self) -> Tensor:\n        device = self.mu.device\n        epsilon = self.normal.sample(self.mu.size()).to(device)\n        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n        self.w = self.mu + self.sigma * epsilon\n\n        return self.w\n\n    def log_posterior(self) -> Tensor:\n\n        if self.w is None:\n            raise ValueError('self.w must have a value.')\n\n        log_const = np.log(np.sqrt(2 * np.pi))\n        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n\n        return log_posterior.sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.963531Z","iopub.execute_input":"2025-05-17T12:25:13.963805Z","iopub.status.idle":"2025-05-17T12:25:13.971312Z","shell.execute_reply.started":"2025-05-17T12:25:13.963783Z","shell.execute_reply":"2025-05-17T12:25:13.970587Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Bayesian Linear Layer ##","metadata":{}},{"cell_type":"code","source":"from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n#from .base_bayesian import BayesianModule\n#from .samplers.gaussian_variational import GaussianVariational\n#from .samplers.scale_mixture import ScaleMixture\n\n\nclass BayesLinear(BayesianModule):\n\n    \"\"\"Bayesian Linear Layer.\n\n    Implementation of a Bayesian Linear Layer as described in the\n    'Weight Uncertainty in Neural Networks' paper.\n    \"\"\"\n\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 prior_pi: Optional[float] = 0.5,\n                 prior_sigma1: Optional[float] = 1.0,\n                 prior_sigma2: Optional[float] = 0.0025) -> None:\n\n        super().__init__()\n\n        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2)\n        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0)\n\n        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2)\n        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0)\n\n        self.w_posterior = GaussianVariational(w_mu, w_rho)\n        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n\n        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n\n        self.kl_divergence = 0.0\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        w = self.w_posterior.sample()\n        b = self.bias_posterior.sample()\n\n        w_log_prior = self.w_prior.log_prior(w)\n        b_log_prior = self.bias_prior.log_prior(b)\n\n        w_log_posterior = self.w_posterior.log_posterior()\n        b_log_posterior = self.bias_posterior.log_posterior()\n\n        total_log_prior = w_log_prior + b_log_prior\n        total_log_posterior = w_log_posterior + b_log_posterior\n        self.kl_divergence = self.kld(total_log_prior, total_log_posterior)\n\n        return F.linear(x, w, b)\n\n    def kld(self, log_prior: Tensor, log_posterior: Tensor) -> Tensor:\n        return log_posterior - log_prior","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.972349Z","iopub.execute_input":"2025-05-17T12:25:13.972596Z","iopub.status.idle":"2025-05-17T12:25:13.987756Z","shell.execute_reply.started":"2025-05-17T12:25:13.972526Z","shell.execute_reply":"2025-05-17T12:25:13.987132Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n\n# define transforms\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:13.988491Z","iopub.execute_input":"2025-05-17T12:25:13.988986Z","iopub.status.idle":"2025-05-17T12:25:14.002954Z","shell.execute_reply.started":"2025-05-17T12:25:13.988967Z","shell.execute_reply":"2025-05-17T12:25:14.002334Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# load / process data\ntrainset = datasets.MNIST('./data', train=True, download=True,transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=128, **kwargs)\n\ntestset = datasets.MNIST('./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=128, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:14.004103Z","iopub.execute_input":"2025-05-17T12:25:14.004308Z","iopub.status.idle":"2025-05-17T12:25:14.105013Z","shell.execute_reply.started":"2025-05-17T12:25:14.004293Z","shell.execute_reply":"2025-05-17T12:25:14.104215Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"@variational_approximator\nclass BayesianNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.bl1 = BayesLinear(input_dim, 1200)\n        self.bl2 = BayesLinear(1200, 1200)\n        self.bl3 = BayesLinear(1200, output_dim)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n\n        x = F.relu(self.bl1(x))\n        x = F.relu(self.bl2(x))\n        x = self.bl3(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:25:14.105780Z","iopub.execute_input":"2025-05-17T12:25:14.106025Z","iopub.status.idle":"2025-05-17T12:25:14.111078Z","shell.execute_reply.started":"2025-05-17T12:25:14.106003Z","shell.execute_reply":"2025-05-17T12:25:14.110348Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model = BayesianNetwork(28 * 28, 10).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss(reduction='sum')\n\n# prepare results file\nwith open('results.csv', 'w+', newline=\"\") as f_out:\n    writer = csv.writer(f_out, delimiter=',')\n    writer.writerow(['epoch', 'train_loss', 'test_loss', 'accuracy'])\n\nmin_test_loss = np.Inf\nfor epoch in range(2):\n\n    train_loss = 0.0\n    test_loss = 0.0\n\n    model.train()\n    for batch_idx, (data, labels) in enumerate(trainloader):\n        data, labels = data.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n\n        loss = model.elbo(\n            inputs=data,\n            targets=labels,\n            criterion=criterion,\n            n_samples=3,\n            w_complexity=pi_weight\n        )\n\n        train_loss += loss.item() * data.size(0)\n\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 1000 == 0:\n            print(f'Train Epoch: {epoch} '\n                  f'[{batch_idx * len(data):05}/{len(trainloader.dataset)} '\n                  f'({100 * batch_idx / len(trainloader.dataset):.2f}%)]'\n                  f'\\tLoss: {loss.item():.6f}')\n\n    correct = 0\n    total = 0\n\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(testloader):\n            data, labels = data.to(device), labels.to(device)\n\n            outputs = model(data)\n\n            pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n\n            loss = model.elbo(\n                inputs=data,\n                targets=labels,\n                criterion=criterion,\n                n_samples=3,\n                w_complexity=pi_weight\n            )\n\n            test_loss += loss.item() * data.size(0)\n\n            probabilities = F.softmax(outputs)\n            _, predicted = torch.max(probabilities.data, 1)\n\n            total += labels.size(0)\n            correct += torch.eq(predicted, labels).sum().item()\n\n    accuracy = 100 * correct / total\n    train_loss /= len(trainloader.dataset)\n    test_loss /= len(testloader.dataset)\n\n    if test_loss < min_test_loss:\n        print('\\nValidation Loss Decreased: {:.6f} -> {:.6f}\\n'\n              ''.format(min_test_loss, test_loss))\n\n        min_test_loss = test_loss\n        torch.save(model.state_dict(), 'mnistBNN_checkpoint.pt')\n\n    _results = [epoch, train_loss, test_loss, accuracy]\n\n    print(f'Epoch: {epoch:03} | '\n          f'Train Loss: {train_loss:.3f} |'\n          f'Test Loss: {test_loss:.3f} |'\n          f'Accuracy: {accuracy:.3f} %\\n')\n\n    # write results to file\n    with open('results.csv', 'a', newline=\"\") as f_out:\n        writer = csv.writer(f_out, delimiter=',')\n        writer.writerow(_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:37:26.026226Z","iopub.execute_input":"2025-05-17T12:37:26.026784Z","iopub.status.idle":"2025-05-17T12:39:43.524665Z","shell.execute_reply.started":"2025-05-17T12:37:26.026761Z","shell.execute_reply":"2025-05-17T12:39:43.523877Z"}},"outputs":[{"name":"stdout","text":"Train Epoch: 0 [00000/60000 (0.00%)]\tLoss: 10845382.000000\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/3618660317.py:65: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  probabilities = F.softmax(outputs)\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Loss Decreased: inf -> 272816.319054\n\nEpoch: 000 | Train Loss: 46370.326 |Test Loss: 272816.319 |Accuracy: 95.310 %\n\nTrain Epoch: 1 [00000/60000 (0.00%)]\tLoss: 10654612.000000\n\nValidation Loss Decreased: 272816.319054 -> 270878.387274\n\nEpoch: 001 | Train Loss: 45381.997 |Test Loss: 270878.387 |Accuracy: 96.450 %\n\n","output_type":"stream"}],"execution_count":37}]}