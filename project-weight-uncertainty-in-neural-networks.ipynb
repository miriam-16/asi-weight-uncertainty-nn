{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ASI Project] Weight Uncertainty in Neural Networks  \n",
    "**Authors**: Miriam Lamari, Francesco Giannuzzo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "\n",
    "#from torchwu.bayes_linear import BayesLinear\n",
    "#from torchwu.utils.minibatch_weighting import minibatch_weight\n",
    "#from torchwu.utils.variational_approximator import variational_approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatches\n",
    "**minibatch_weight**(batch_idx: int, num_batches: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class BayesianModule(nn.Module):\n",
    "\n",
    "    \"\"\"Base class for BNN to enable certain behaviour.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def kld(self, *args):\n",
    "        raise NotImplementedError('BayesianModule::kld()')\n",
    "\n",
    "\n",
    "def variational_approximator(model: nn.Module) -> nn.Module:\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    # add `kl_divergence` to the model\n",
    "    setattr(model, 'kl_divergence', kl_divergence)\n",
    "\n",
    "    def elbo(self,\n",
    "             inputs: Tensor,\n",
    "             targets: Tensor,\n",
    "             criterion: Any,\n",
    "             n_samples: int,\n",
    "             w_complexity: Optional[float] = 1.0) -> Tensor:\n",
    "\n",
    "        loss = 0\n",
    "        for sample in range(n_samples):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, targets)\n",
    "            loss += self.kl_divergence() * w_complexity\n",
    "\n",
    "        return loss / n_samples\n",
    "\n",
    "    # add `elbo` to the model\n",
    "    setattr(model, 'elbo', elbo)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Mixture Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ScaleMixture(nn.Module):\n",
    "\n",
    "    \"\"\"Scale Mixture Prior.\n",
    "\n",
    "    Section 3.3 of the 'Weight Uncertainty in Neural Networks' paper\n",
    "    proposes the use of a Scale Mixture prior for use in variational\n",
    "    inference - this being a fixed-form prior.\n",
    "\n",
    "    The authors note that, should the parameters be allowed to adjust\n",
    "    during training, the prior changes rapidly and attempts to capture\n",
    "    the empirical distribution of the weights. As a result the prior\n",
    "    learns to fit poor initial parameters and struggles to improve.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
    "\n",
    "        \"\"\"Scale Mixture Prior.\n",
    "\n",
    "        The authors of 'Weight Uncertainty in Neural Networks' note:\n",
    "\n",
    "            sigma1 > sigma2:\n",
    "                provides a heavier tail in the prior density than is\n",
    "                seen in a plain Gaussian prior.\n",
    "            sigma2 << 1.0:\n",
    "                causes many of the weights to a priori tightly\n",
    "                concentrate around zero.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Parameter used to scale the two Gaussian distributions.\n",
    "        sigma1 : float\n",
    "            Standard deviation of the first normal distribution.\n",
    "        sigma2 : float\n",
    "            Standard deviation of the second normal distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w: Tensor) -> Tensor:\n",
    "\n",
    "        \"\"\"Log Likelihood of the weight according to the prior.\n",
    "\n",
    "        Calculates the log likelihood of the supplied weight given the\n",
    "        prior distribution - the scale mixture of two Gaussians.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : Tensor\n",
    "            Weight to be used to calculate the log likelihood.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Log likelihood of the weights from the prior distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n",
    "        log_prob = torch.log(p_scalemixture).sum()\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class GaussianVariational(nn.Module):\n",
    "    def __init__(self, mu: Tensor, rho: Tensor) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self) -> Tensor:\n",
    "        device = self.mu.device\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self) -> Tensor:\n",
    "\n",
    "        if self.w is None:\n",
    "            raise ValueError('self.w must have a value.')\n",
    "\n",
    "        log_const = np.log(np.sqrt(2 * np.pi))\n",
    "        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n",
    "\n",
    "        return log_posterior.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from .base_bayesian import BayesianModule\n",
    "#from .samplers.gaussian_variational import GaussianVariational\n",
    "#from .samplers.scale_mixture import ScaleMixture\n",
    "\n",
    "\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    \"\"\"Bayesian Linear Layer.\n",
    "\n",
    "    Implementation of a Bayesian Linear Layer as described in the\n",
    "    'Weight Uncertainty in Neural Networks' paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 prior_pi: Optional[float] = 0.5,\n",
    "                 prior_sigma1: Optional[float] = 1.0,\n",
    "                 prior_sigma2: Optional[float] = 0.0025) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2)\n",
    "        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2)\n",
    "        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        self.w_posterior = GaussianVariational(w_mu, w_rho)\n",
    "        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "        self.kl_divergence = 0.0\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.bias_posterior.sample()\n",
    "\n",
    "        w_log_prior = self.w_prior.log_prior(w)\n",
    "        b_log_prior = self.bias_prior.log_prior(b)\n",
    "\n",
    "        w_log_posterior = self.w_posterior.log_posterior()\n",
    "        b_log_posterior = self.bias_posterior.log_posterior()\n",
    "\n",
    "        total_log_prior = w_log_prior + b_log_prior\n",
    "        total_log_posterior = w_log_posterior + b_log_posterior\n",
    "        self.kl_divergence = self.kld(total_log_prior, total_log_posterior)\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "    def kld(self, log_prior: Tensor, log_posterior: Tensor) -> Tensor:\n",
    "        return log_posterior - log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "# define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load / process data\n",
    "trainset = datasets.MNIST('./data', train=True, download=True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, **kwargs)\n",
    "\n",
    "testset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@variational_approximator\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.bl1 = BayesLinear(input_dim, 1200)\n",
    "        self.bl2 = BayesLinear(1200, 1200)\n",
    "        self.bl3 = BayesLinear(1200, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        x = F.relu(self.bl1(x))\n",
    "        x = F.relu(self.bl2(x))\n",
    "        x = self.bl3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = BayesianNetwork(28 * 28, 10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# prepare results file\n",
    "with open('results.csv', 'w+', newline=\"\") as f_out:\n",
    "    writer = csv.writer(f_out, delimiter=',')\n",
    "    writer.writerow(['epoch', 'train_loss', 'test_loss', 'accuracy'])\n",
    "\n",
    "min_test_loss = np.Inf\n",
    "for epoch in range(500):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n",
    "\n",
    "        loss = model.elbo(\n",
    "            inputs=data,\n",
    "            targets=labels,\n",
    "            criterion=criterion,\n",
    "            n_samples=3,\n",
    "            w_complexity=pi_weight\n",
    "        )\n",
    "\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f'Train Epoch: {epoch} '\n",
    "                  f'[{batch_idx * len(data):05}/{len(trainloader.dataset)} '\n",
    "                  f'({100 * batch_idx / len(trainloader.dataset):.2f}%)]'\n",
    "                  f'\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n",
    "\n",
    "            loss = model.elbo(\n",
    "                inputs=data,\n",
    "                targets=labels,\n",
    "                criterion=criterion,\n",
    "                n_samples=3,\n",
    "                w_complexity=pi_weight\n",
    "            )\n",
    "\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "\n",
    "            probabilities = F.softmax(outputs)\n",
    "            _, predicted = torch.max(probabilities.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += torch.eq(predicted, labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    test_loss /= len(testloader.dataset)\n",
    "\n",
    "    if test_loss < min_test_loss:\n",
    "        print('\\nValidation Loss Decreased: {:.6f} -> {:.6f}\\n'\n",
    "              ''.format(min_test_loss, test_loss))\n",
    "\n",
    "        min_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'mnistBNN_checkpoint.pt')\n",
    "\n",
    "    _results = [epoch, train_loss, test_loss, accuracy]\n",
    "\n",
    "    print(f'Epoch: {epoch:03} | '\n",
    "          f'Train Loss: {train_loss:.3f} |'\n",
    "          f'Test Loss: {test_loss:.3f} |'\n",
    "          f'Accuracy: {accuracy:.3f} %\\n')\n",
    "\n",
    "    # write results to file\n",
    "    with open('results.csv', 'a', newline=\"\") as f_out:\n",
    "        writer = csv.writer(f_out, delimiter=',')\n",
    "        writer.writerow(_results)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
