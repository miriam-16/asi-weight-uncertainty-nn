{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45e34b0",
   "metadata": {
    "papermill": {
     "duration": 0.003798,
     "end_time": "2025-05-26T10:57:57.647088",
     "exception": false,
     "start_time": "2025-05-26T10:57:57.643290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [ASI Project] Weight Uncertainty in Neural Networks  \n",
    "**Authors**: Miriam Lamari, Francesco Giannuzzo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027168fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:57:57.654074Z",
     "iopub.status.busy": "2025-05-26T10:57:57.653868Z",
     "iopub.status.idle": "2025-05-26T10:58:09.570149Z",
     "shell.execute_reply": "2025-05-26T10:58:09.569553Z"
    },
    "papermill": {
     "duration": 11.921201,
     "end_time": "2025-05-26T10:58:09.571539",
     "exception": false,
     "start_time": "2025-05-26T10:57:57.650338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import functools as ft\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192e6a9",
   "metadata": {},
   "source": [
    "### Wandb utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a349c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:09.579107Z",
     "iopub.status.busy": "2025-05-26T10:58:09.578779Z",
     "iopub.status.idle": "2025-05-26T10:58:10.610336Z",
     "shell.execute_reply": "2025-05-26T10:58:10.609624Z"
    },
    "papermill": {
     "duration": 1.036547,
     "end_time": "2025-05-26T10:58:10.611567",
     "exception": false,
     "start_time": "2025-05-26T10:58:09.575020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiriam-lamari2\u001b[0m (\u001b[33mmiriam-lamari2-eurecom\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wandb-api-key')\n",
    "\n",
    "wandb.login(key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a69dc",
   "metadata": {
    "papermill": {
     "duration": 0.00392,
     "end_time": "2025-05-26T10:58:10.619684",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.615764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Minibatches\n",
    "**minibatch_weight**(batch_idx: int, num_batches: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223640be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.628223Z",
     "iopub.status.busy": "2025-05-26T10:58:10.627967Z",
     "iopub.status.idle": "2025-05-26T10:58:10.631755Z",
     "shell.execute_reply": "2025-05-26T10:58:10.631097Z"
    },
    "papermill": {
     "duration": 0.009609,
     "end_time": "2025-05-26T10:58:10.633024",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.623415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - 1) # definition of pi_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b16cb",
   "metadata": {
    "papermill": {
     "duration": 0.003647,
     "end_time": "2025-05-26T10:58:10.640662",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.637015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Variational Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.649198Z",
     "iopub.status.busy": "2025-05-26T10:58:10.648960Z",
     "iopub.status.idle": "2025-05-26T10:58:10.655416Z",
     "shell.execute_reply": "2025-05-26T10:58:10.654856Z"
    },
    "papermill": {
     "duration": 0.012044,
     "end_time": "2025-05-26T10:58:10.656487",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.644443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base class for BNN to enable certain behaviour\n",
    "class BayesianModule(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def kld(self, *args):\n",
    "        raise NotImplementedError('BayesianModule::kld()')\n",
    "\n",
    "\n",
    "# Variational approximator for Bayesian Neural Networks\n",
    "def variational_approximator(model: nn.Module) -> nn.Module:\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def elbo(self, inputs: Tensor, targets: Tensor, criterion: Any, n_samples: int, w_complexity: Optional[float] = 1.0) -> Tensor:\n",
    "        loss = 0\n",
    "        for _ in range(n_samples):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, targets)\n",
    "            loss += self.kl_divergence() * w_complexity\n",
    "\n",
    "        return loss / n_samples\n",
    "\n",
    "    # include `kl_divergence` function to the model\n",
    "    setattr(model, 'kl_divergence', kl_divergence)\n",
    "\n",
    "    # include `elbo` function to the model\n",
    "    setattr(model, 'elbo', elbo)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6c319",
   "metadata": {
    "papermill": {
     "duration": 0.003589,
     "end_time": "2025-05-26T10:58:10.663915",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.660326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scale Mixture Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c2b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.672440Z",
     "iopub.status.busy": "2025-05-26T10:58:10.672163Z",
     "iopub.status.idle": "2025-05-26T10:58:10.678982Z",
     "shell.execute_reply": "2025-05-26T10:58:10.678364Z"
    },
    "papermill": {
     "duration": 0.012498,
     "end_time": "2025-05-26T10:58:10.680201",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.667703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale Mixture Prior for Bayesian Neural Networks\n",
    "class ScaleMixture(nn.Module):\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w: Tensor) -> Tensor:\n",
    "        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n",
    "\n",
    "        log_prob = torch.log(p_scalemixture).sum()\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc45a1b",
   "metadata": {},
   "source": [
    "### Variant: log-sum-exp rather than log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Mixture Prior for Bayesian Neural Networks\n",
    "class ScaleMixture(nn.Module):\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
    "        super().__init__()\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w: Tensor) -> Tensor:\n",
    "        log_prob_n1 = self.normal1.log_prob(w)\n",
    "        log_prob_n2 = self.normal2.log_prob(w)\n",
    "\n",
    "        log_mix1 = torch.log(self.pi) + log_prob_n1\n",
    "        log_mix2 = torch.log(1.0 - self.pi) + log_prob_n2\n",
    "\n",
    "        # Stable log-sum-exp over the two components\n",
    "        log_prob = torch.logsumexp(torch.stack([log_mix1, log_mix2]), dim=0).sum()\n",
    "\n",
    "        return log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41977ea9",
   "metadata": {
    "papermill": {
     "duration": 0.003672,
     "end_time": "2025-05-26T10:58:10.687713",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.684041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gaussian Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0431d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.696276Z",
     "iopub.status.busy": "2025-05-26T10:58:10.696002Z",
     "iopub.status.idle": "2025-05-26T10:58:10.704264Z",
     "shell.execute_reply": "2025-05-26T10:58:10.703629Z"
    },
    "papermill": {
     "duration": 0.013902,
     "end_time": "2025-05-26T10:58:10.705426",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.691524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gaussian Variational Approximation for Bayesian Neural Networks\n",
    "class GaussianVariational(nn.Module):\n",
    "    def __init__(self, mu: Tensor, rho: Tensor) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self) -> Tensor:\n",
    "        device = self.mu.device\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self) -> Tensor:\n",
    "        if self.w is None:\n",
    "            raise ValueError('self.w must have a value.')\n",
    "\n",
    "        log_const = np.log(np.sqrt(2 * np.pi))\n",
    "        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n",
    "\n",
    "        return log_posterior.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644a1d6",
   "metadata": {
    "papermill": {
     "duration": 0.003639,
     "end_time": "2025-05-26T10:58:10.712942",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.709303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bayesian Linear Layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f7ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.721273Z",
     "iopub.status.busy": "2025-05-26T10:58:10.721032Z",
     "iopub.status.idle": "2025-05-26T10:58:10.729025Z",
     "shell.execute_reply": "2025-05-26T10:58:10.728373Z"
    },
    "papermill": {
     "duration": 0.013667,
     "end_time": "2025-05-26T10:58:10.730256",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.716589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bayesian Linear Layer\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 prior_pi: Optional[float] = 0.5,\n",
    "                 prior_sigma1: Optional[float] = 1.0,\n",
    "                 prior_sigma2: Optional[float] = 0.0025) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2)\n",
    "        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2)\n",
    "        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        self.w_posterior = GaussianVariational(w_mu, w_rho)\n",
    "        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "        self.kl_divergence = 0.0\n",
    "\n",
    "    def kld(self, log_prior: Tensor, log_posterior: Tensor) -> Tensor:\n",
    "        return log_posterior - log_prior\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.bias_posterior.sample()\n",
    "\n",
    "        w_log_prior = self.w_prior.log_prior(w)\n",
    "        b_log_prior = self.bias_prior.log_prior(b)\n",
    "\n",
    "        w_log_posterior = self.w_posterior.log_posterior()\n",
    "        b_log_posterior = self.bias_posterior.log_posterior()\n",
    "\n",
    "        total_log_prior = w_log_prior + b_log_prior\n",
    "        total_log_posterior = w_log_posterior + b_log_posterior\n",
    "        self.kl_divergence = self.kld(total_log_prior, total_log_posterior)\n",
    "\n",
    "        return F.linear(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b230f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.738846Z",
     "iopub.status.busy": "2025-05-26T10:58:10.738566Z",
     "iopub.status.idle": "2025-05-26T10:58:10.797259Z",
     "shell.execute_reply": "2025-05-26T10:58:10.796645Z"
    },
    "papermill": {
     "duration": 0.064333,
     "end_time": "2025-05-26T10:58:10.798409",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.734076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "# define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6fe6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.806921Z",
     "iopub.status.busy": "2025-05-26T10:58:10.806677Z",
     "iopub.status.idle": "2025-05-26T10:58:10.811046Z",
     "shell.execute_reply": "2025-05-26T10:58:10.810504Z"
    },
    "papermill": {
     "duration": 0.009309,
     "end_time": "2025-05-26T10:58:10.812040",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.802731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfull_trainset = datasets.MNIST('./data', train=True, download=True, transform=transform)\\n\\n#train val split\\n\\ntrain_size = 50000\\nval_size = 10000\\ntrainset, valset = random_split(full_trainset, [train_size, val_size])\\n\\nkwargs = {'shuffle': True, 'num_workers': 2, 'pin_memory': True}\\ntrainloader = DataLoader(trainset, batch_size=128, **kwargs)\\nvalloader = DataLoader(valset, batch_size=128, **kwargs)\\n\\ntestset = datasets.MNIST('./data', train=False, download=True, transform=transform)\\ntestloader = DataLoader(testset, batch_size=128, **kwargs)\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load / process data for tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "full_trainset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "#train val split\n",
    "\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "kwargs = {'shuffle': True, 'num_workers': 2, 'pin_memory': True}\n",
    "trainloader = DataLoader(trainset, batch_size=128, **kwargs)\n",
    "valloader = DataLoader(valset, batch_size=128, **kwargs)\n",
    "\n",
    "testset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, **kwargs)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcc9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:10.819290Z",
     "iopub.status.busy": "2025-05-26T10:58:10.819115Z",
     "iopub.status.idle": "2025-05-26T10:58:20.656916Z",
     "shell.execute_reply": "2025-05-26T10:58:20.656109Z"
    },
    "papermill": {
     "duration": 9.842882,
     "end_time": "2025-05-26T10:58:20.658199",
     "exception": false,
     "start_time": "2025-05-26T10:58:10.815317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.48MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load / process data\n",
    "\n",
    "full_trainset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "#train-test split\n",
    "\n",
    "trainset = full_trainset\n",
    "\n",
    "kwargs = {'shuffle': True, 'num_workers': 2, 'pin_memory': True}\n",
    "trainloader = DataLoader(trainset, batch_size=128, **kwargs)\n",
    "\n",
    "testset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f82e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.668840Z",
     "iopub.status.busy": "2025-05-26T10:58:20.668587Z",
     "iopub.status.idle": "2025-05-26T10:58:20.678536Z",
     "shell.execute_reply": "2025-05-26T10:58:20.677875Z"
    },
    "papermill": {
     "duration": 0.01627,
     "end_time": "2025-05-26T10:58:20.679523",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.663253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(learning_rate, prior_pi, prior_sigma1, prior_sigma2, epochs=25):\n",
    "    run = wandb.init(project=\"Project-ASI\")\n",
    "    @variational_approximator\n",
    "    class BayesianNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.bl1 = BayesLinear(input_dim, 1200, prior_pi, prior_sigma1, prior_sigma2)\n",
    "            self.bl2 = BayesLinear(1200, 1200, prior_pi, prior_sigma1, prior_sigma2)\n",
    "            self.bl3 = BayesLinear(1200, output_dim, prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 28 * 28)\n",
    "\n",
    "            x = F.relu(self.bl1(x))\n",
    "            x = F.relu(self.bl2(x))\n",
    "            x = self.bl3(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    model = BayesianNetwork(28 * 28, 10).to(device)\n",
    "\n",
    "    run = wandb.init(project=\"Project-ASI\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "\n",
    "    min_test_loss = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n",
    "\n",
    "            loss = model.elbo(\n",
    "                inputs=data,\n",
    "                targets=labels,\n",
    "                criterion=criterion,\n",
    "                n_samples=3,\n",
    "                w_complexity=pi_weight\n",
    "            )\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, labels) in enumerate(testloader):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "\n",
    "                pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=128)\n",
    "\n",
    "                loss = model.elbo(\n",
    "                    inputs=data,\n",
    "                    targets=labels,\n",
    "                    criterion=criterion,\n",
    "                    n_samples=3,\n",
    "                    w_complexity=pi_weight\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "                probabilities = F.softmax(outputs)\n",
    "                _, predicted = torch.max(probabilities.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += torch.eq(predicted, labels).sum().item()\n",
    "\n",
    "        accuracy_val = correct / total\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        val_loss /= len(testloader.dataset)\n",
    "\n",
    "        metrics = {'train_loss': train_loss, 'val_loss': val_loss, 'accuracy_val': accuracy_val, \"test_error_val\": (1 - accuracy_val)  }\n",
    "        wandb.log(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdfa0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.689403Z",
     "iopub.status.busy": "2025-05-26T10:58:20.689040Z",
     "iopub.status.idle": "2025-05-26T10:58:20.692479Z",
     "shell.execute_reply": "2025-05-26T10:58:20.691840Z"
    },
    "papermill": {
     "duration": 0.009523,
     "end_time": "2025-05-26T10:58:20.693574",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.684051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_wrapper():\n",
    "    run = wandb.init(project=\"Project-ASI\")\n",
    "\n",
    "    return train_loop(\n",
    "        learning_rate = wandb.config.learning_rate,\n",
    "        prior_pi = wandb.config.prior_pi,\n",
    "        prior_sigma1=wandb.config.prior_sigma1,\n",
    "        prior_sigma2=wandb.config.prior_sigma2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb346707",
   "metadata": {
    "papermill": {
     "duration": 0.004265,
     "end_time": "2025-05-26T10:58:20.702542",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.698277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tuning Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "620b5f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.712292Z",
     "iopub.status.busy": "2025-05-26T10:58:20.711910Z",
     "iopub.status.idle": "2025-05-26T10:58:20.716113Z",
     "shell.execute_reply": "2025-05-26T10:58:20.715465Z"
    },
    "papermill": {
     "duration": 0.010288,
     "end_time": "2025-05-26T10:58:20.717183",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.706895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "     \"method\": \"grid\",\n",
    "     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "     'name': \"sweep-BBB-Gaussian\",\n",
    "     \"parameters\": {\n",
    "         \"learning_rate\": {'values': [1e-3]},\n",
    "         \"prior_pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "         \"prior_sigma1\": {'values': [1, math.exp(-1), math.exp(-2)]},\n",
    "         \"prior_sigma2\": {'values': [math.exp(-6), math.exp(-7), math.exp(-8)]},\n",
    "     },\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Project-ASI\")\n",
    "#wandb.agent(sweep_id, function=train_wrapper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a91e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.727064Z",
     "iopub.status.busy": "2025-05-26T10:58:20.726625Z",
     "iopub.status.idle": "2025-05-26T10:58:20.730469Z",
     "shell.execute_reply": "2025-05-26T10:58:20.729982Z"
    },
    "papermill": {
     "duration": 0.009627,
     "end_time": "2025-05-26T10:58:20.731383",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.721756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "     \"method\": \"grid\",\n",
    "     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "     'name': \"sweep-BBB-Gaussian\",\n",
    "     \"parameters\": {\n",
    "         \"learning_rate\": {'values': [1e-4]},\n",
    "         \"prior_pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "         \"prior_sigma1\": {'values': [1, math.exp(-1), math.exp(-2)]},\n",
    "         \"prior_sigma2\": {'values': [math.exp(-6), math.exp(-7), math.exp(-8)]},\n",
    "     },\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Project-ASI\")\n",
    "#wandb.agent(sweep_id, function=train_wrapper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6afd0d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.741567Z",
     "iopub.status.busy": "2025-05-26T10:58:20.741136Z",
     "iopub.status.idle": "2025-05-26T10:58:20.744153Z",
     "shell.execute_reply": "2025-05-26T10:58:20.743656Z"
    },
    "papermill": {
     "duration": 0.009193,
     "end_time": "2025-05-26T10:58:20.745093",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.735900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hyperparameters = {'learning_rate' : 1e-3, 'prior_pi' : 0.5, 'prior_sigma1' : 1.0, 'prior_sigma2' : math.exp(-7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0471852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T10:58:20.755476Z",
     "iopub.status.busy": "2025-05-26T10:58:20.755105Z",
     "iopub.status.idle": "2025-05-26T13:55:57.035765Z",
     "shell.execute_reply": "2025-05-26T13:55:57.034928Z"
    },
    "papermill": {
     "duration": 10656.287722,
     "end_time": "2025-05-26T13:55:57.037541",
     "exception": false,
     "start_time": "2025-05-26T10:58:20.749819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250526_105820-4nwgvpt3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-flower-83\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/4nwgvpt3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msnowy-flower-83\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/4nwgvpt3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250526_105820-4nwgvpt3/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250526_105822-86dzoiu8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlaced-tree-84\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/86dzoiu8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlaced-tree-84\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/86dzoiu8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250526_105822-86dzoiu8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250526_105825-k5u4yocv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-meadow-85\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/k5u4yocv\u001b[0m\n",
      "/tmp/ipykernel_19/689902517.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(outputs)\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Project-ASI\")\n",
    "\n",
    "train_loop(\n",
    "    learning_rate = best_hyperparameters['learning_rate'],\n",
    "    prior_pi = best_hyperparameters['prior_pi'],\n",
    "    prior_sigma1 = best_hyperparameters['prior_sigma1'],\n",
    "    prior_sigma2 = best_hyperparameters['prior_sigma2'],\n",
    "    epochs = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2250482-8163-4bbd-adb8-7ba8e40e9451",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10686.184822,
   "end_time": "2025-05-26T13:55:59.779577",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-26T10:57:53.594755",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
