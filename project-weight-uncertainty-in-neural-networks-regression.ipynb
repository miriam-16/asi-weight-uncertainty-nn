{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":10686.184822,"end_time":"2025-05-26T13:55:59.779577","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-26T10:57:53.594755","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b45e34b0","cell_type":"markdown","source":"# [ASI Project] Weight Uncertainty in Neural Networks  \n**Authors**: Miriam Lamari, Francesco Giannuzzo  \n","metadata":{"papermill":{"duration":0.003798,"end_time":"2025-05-26T10:57:57.647088","exception":false,"start_time":"2025-05-26T10:57:57.643290","status":"completed"},"tags":[]}},{"id":"027168fc","cell_type":"code","source":"import csv\nimport math\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.utils.data import DataLoader, random_split\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:00:59.549563Z","iopub.execute_input":"2025-06-08T09:00:59.550190Z","iopub.status.idle":"2025-06-08T09:00:59.554141Z","shell.execute_reply.started":"2025-06-08T09:00:59.550166Z","shell.execute_reply":"2025-06-08T09:00:59.553442Z"},"papermill":{"duration":11.921201,"end_time":"2025-05-26T10:58:09.571539","exception":false,"start_time":"2025-05-26T10:57:57.650338","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":29},{"id":"47a349c3","cell_type":"code","source":"user_secrets = UserSecretsClient()\nkey = user_secrets.get_secret('wandb-api-key')\n\nwandb.login(key=key)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:00:59.556075Z","iopub.execute_input":"2025-06-08T09:00:59.556248Z","iopub.status.idle":"2025-06-08T09:01:00.248599Z","shell.execute_reply.started":"2025-06-08T09:00:59.556235Z","shell.execute_reply":"2025-06-08T09:01:00.247870Z"},"papermill":{"duration":1.036547,"end_time":"2025-05-26T10:58:10.611567","exception":false,"start_time":"2025-05-26T10:58:09.575020","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":30},{"id":"554a69dc","cell_type":"markdown","source":"## Minibatches\n**minibatch_weight**(batch_idx: int, num_batches: int)","metadata":{"papermill":{"duration":0.00392,"end_time":"2025-05-26T10:58:10.619684","exception":false,"start_time":"2025-05-26T10:58:10.615764","status":"completed"},"tags":[]}},{"id":"223640be","cell_type":"code","source":"def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - 1)#batch_idx)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.249896Z","iopub.execute_input":"2025-06-08T09:01:00.250190Z","iopub.status.idle":"2025-06-08T09:01:00.253821Z","shell.execute_reply.started":"2025-06-08T09:01:00.250166Z","shell.execute_reply":"2025-06-08T09:01:00.253089Z"},"papermill":{"duration":0.009609,"end_time":"2025-05-26T10:58:10.633024","exception":false,"start_time":"2025-05-26T10:58:10.623415","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":31},{"id":"157b16cb","cell_type":"markdown","source":"## Variational Approximation","metadata":{"papermill":{"duration":0.003647,"end_time":"2025-05-26T10:58:10.640662","exception":false,"start_time":"2025-05-26T10:58:10.637015","status":"completed"},"tags":[]}},{"id":"4b7b2881","cell_type":"code","source":"from typing import Any, Optional\n\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass BayesianModule(nn.Module):\n\n    \"\"\"Base class for BNN to enable certain behaviour.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def kld(self, *args):\n        raise NotImplementedError('BayesianModule::kld()')\n\n\ndef variational_approximator(model: nn.Module) -> nn.Module:\n\n    def kl_divergence(self) -> Tensor:\n\n        kl = 0\n        for module in self.modules():\n            if isinstance(module, BayesianModule):\n                kl += module.kl_divergence\n\n        return kl\n\n    # add `kl_divergence` to the model\n    setattr(model, 'kl_divergence', kl_divergence)\n\n    def elbo(self,\n             inputs: Tensor,\n             targets: Tensor,\n             criterion: Any,\n             n_samples: int,\n             w_complexity: Optional[float] = 1.0) -> Tensor:\n\n        loss = 0\n        for sample in range(n_samples):\n            outputs = self(inputs)\n            loss += criterion(outputs, targets)\n            loss += self.kl_divergence() * w_complexity\n\n        return loss / n_samples\n\n    # add `elbo` to the model\n    setattr(model, 'elbo', elbo)\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.254464Z","iopub.execute_input":"2025-06-08T09:01:00.254671Z","iopub.status.idle":"2025-06-08T09:01:00.267003Z","shell.execute_reply.started":"2025-06-08T09:01:00.254655Z","shell.execute_reply":"2025-06-08T09:01:00.266434Z"},"papermill":{"duration":0.012044,"end_time":"2025-05-26T10:58:10.656487","exception":false,"start_time":"2025-05-26T10:58:10.644443","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":32},{"id":"30e6c319","cell_type":"markdown","source":"## Scale Mixture Prior","metadata":{"papermill":{"duration":0.003589,"end_time":"2025-05-26T10:58:10.663915","exception":false,"start_time":"2025-05-26T10:58:10.660326","status":"completed"},"tags":[]}},{"id":"834c2b8f","cell_type":"code","source":"import functools as ft\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass ScaleMixture(nn.Module):\n\n    \"\"\"Scale Mixture Prior.\n\n    Section 3.3 of the 'Weight Uncertainty in Neural Networks' paper\n    proposes the use of a Scale Mixture prior for use in variational\n    inference - this being a fixed-form prior.\n\n    The authors note that, should the parameters be allowed to adjust\n    during training, the prior changes rapidly and attempts to capture\n    the empirical distribution of the weights. As a result the prior\n    learns to fit poor initial parameters and struggles to improve.\n    \"\"\"\n\n    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n\n        \"\"\"Scale Mixture Prior.\n\n        The authors of 'Weight Uncertainty in Neural Networks' note:\n\n            sigma1 > sigma2:\n                provides a heavier tail in the prior density than is\n                seen in a plain Gaussian prior.\n            sigma2 << 1.0:\n                causes many of the weights to a priori tightly\n                concentrate around zero.\n\n        Parameters\n        ----------\n        pi : float\n            Parameter used to scale the two Gaussian distributions.\n        sigma1 : float\n            Standard deviation of the first normal distribution.\n        sigma2 : float\n            Standard deviation of the second normal distribution.\n        \"\"\"\n\n        super().__init__()\n\n        self.pi = pi\n        self.sigma1 = sigma1\n        self.sigma2 = sigma2\n\n        self.normal1 = torch.distributions.Normal(0, sigma1)\n        self.normal2 = torch.distributions.Normal(0, sigma2)\n\n    def log_prior(self, w: Tensor) -> Tensor:\n\n        \"\"\"Log Likelihood of the weight according to the prior.\n\n        Calculates the log likelihood of the supplied weight given the\n        prior distribution - the scale mixture of two Gaussians.\n\n        Parameters\n        ----------\n        w : Tensor\n            Weight to be used to calculate the log likelihood.\n\n        Returns\n        -------\n        Tensor\n            Log likelihood of the weights from the prior distribution.\n        \"\"\"\n\n        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n\n        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n        log_prob = torch.log(p_scalemixture).sum()\n\n        return log_prob","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.268853Z","iopub.execute_input":"2025-06-08T09:01:00.269082Z","iopub.status.idle":"2025-06-08T09:01:00.280498Z","shell.execute_reply.started":"2025-06-08T09:01:00.269057Z","shell.execute_reply":"2025-06-08T09:01:00.279876Z"},"papermill":{"duration":0.012498,"end_time":"2025-05-26T10:58:10.680201","exception":false,"start_time":"2025-05-26T10:58:10.667703","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":33},{"id":"41977ea9","cell_type":"markdown","source":"## Gaussian Variational Inference","metadata":{"papermill":{"duration":0.003672,"end_time":"2025-05-26T10:58:10.687713","exception":false,"start_time":"2025-05-26T10:58:10.684041","status":"completed"},"tags":[]}},{"id":"e2a0431d","cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass GaussianVariational(nn.Module):\n    def __init__(self, mu: Tensor, rho: Tensor) -> None:\n\n        super().__init__()\n\n        self.mu = nn.Parameter(mu)\n        self.rho = nn.Parameter(rho)\n\n        self.w = None\n        self.sigma = None\n\n        self.normal = torch.distributions.Normal(0, 1)\n\n    def sample(self) -> Tensor:\n        device = self.mu.device\n        epsilon = self.normal.sample(self.mu.size()).to(device)\n        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n        self.w = self.mu + self.sigma * epsilon\n\n        return self.w\n\n    def log_posterior(self) -> Tensor:\n\n        if self.w is None:\n            raise ValueError('self.w must have a value.')\n\n        log_const = np.log(np.sqrt(2 * np.pi))\n        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n\n        return log_posterior.sum()","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.281088Z","iopub.execute_input":"2025-06-08T09:01:00.281250Z","iopub.status.idle":"2025-06-08T09:01:00.290416Z","shell.execute_reply.started":"2025-06-08T09:01:00.281237Z","shell.execute_reply":"2025-06-08T09:01:00.289810Z"},"papermill":{"duration":0.013902,"end_time":"2025-05-26T10:58:10.705426","exception":false,"start_time":"2025-05-26T10:58:10.691524","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":34},{"id":"f644a1d6","cell_type":"markdown","source":"## Bayesian Linear Layer ##","metadata":{"papermill":{"duration":0.003639,"end_time":"2025-05-26T10:58:10.712942","exception":false,"start_time":"2025-05-26T10:58:10.709303","status":"completed"},"tags":[]}},{"id":"567f7ea3","cell_type":"code","source":"from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n#from .base_bayesian import BayesianModule\n#from .samplers.gaussian_variational import GaussianVariational\n#from .samplers.scale_mixture import ScaleMixture\n\n\nclass BayesLinear(BayesianModule):\n\n    \"\"\"Bayesian Linear Layer.\n\n    Implementation of a Bayesian Linear Layer as described in the\n    'Weight Uncertainty in Neural Networks' paper.\n    \"\"\"\n\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 prior_pi: Optional[float] = 0.5,\n                 prior_sigma1: Optional[float] = 1.0,\n                 prior_sigma2: Optional[float] = 0.0025) -> None:\n\n        super().__init__()\n\n        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2)\n        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0)\n\n        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2)\n        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0)\n\n        self.w_posterior = GaussianVariational(w_mu, w_rho)\n        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n\n        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n\n        self.kl_divergence = 0.0\n\n    def forward(self, x: Tensor) -> Tensor:\n\n        w = self.w_posterior.sample()\n        b = self.bias_posterior.sample()\n\n        w_log_prior = self.w_prior.log_prior(w)\n        b_log_prior = self.bias_prior.log_prior(b)\n\n        w_log_posterior = self.w_posterior.log_posterior()\n        b_log_posterior = self.bias_posterior.log_posterior()\n\n        total_log_prior = w_log_prior + b_log_prior\n        total_log_posterior = w_log_posterior + b_log_posterior\n        self.kl_divergence = self.kld(total_log_prior, total_log_posterior)\n\n        return F.linear(x, w, b)\n\n    def kld(self, log_prior: Tensor, log_posterior: Tensor) -> Tensor:\n        return log_posterior - log_prior","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.291192Z","iopub.execute_input":"2025-06-08T09:01:00.291376Z","iopub.status.idle":"2025-06-08T09:01:00.309020Z","shell.execute_reply.started":"2025-06-08T09:01:00.291354Z","shell.execute_reply":"2025-06-08T09:01:00.308245Z"},"papermill":{"duration":0.013667,"end_time":"2025-05-26T10:58:10.730256","exception":false,"start_time":"2025-05-26T10:58:10.716589","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":35},{"id":"7b230f35","cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.309965Z","iopub.execute_input":"2025-06-08T09:01:00.310300Z","iopub.status.idle":"2025-06-08T09:01:00.323210Z","shell.execute_reply.started":"2025-06-08T09:01:00.310273Z","shell.execute_reply":"2025-06-08T09:01:00.322644Z"},"papermill":{"duration":0.064333,"end_time":"2025-05-26T10:58:10.798409","exception":false,"start_time":"2025-05-26T10:58:10.734076","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":36},{"id":"2fdfa0fe","cell_type":"code","source":"def train_wrapper():\n    run = wandb.init(project=\"Project-ASI\")\n\n    return train_loop(\n        learning_rate = wandb.config.learning_rate,\n        prior_pi = wandb.config.prior_pi,\n        prior_sigma1=wandb.config.prior_sigma1,\n        prior_sigma2=wandb.config.prior_sigma2\n    )","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.323975Z","iopub.execute_input":"2025-06-08T09:01:00.324188Z","iopub.status.idle":"2025-06-08T09:01:00.334898Z","shell.execute_reply.started":"2025-06-08T09:01:00.324167Z","shell.execute_reply":"2025-06-08T09:01:00.334236Z"},"papermill":{"duration":0.009523,"end_time":"2025-05-26T10:58:20.693574","exception":false,"start_time":"2025-05-26T10:58:20.684051","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":37},{"id":"bb346707","cell_type":"markdown","source":"## Tuning Hyperparamters","metadata":{"papermill":{"duration":0.004265,"end_time":"2025-05-26T10:58:20.702542","exception":false,"start_time":"2025-05-26T10:58:20.698277","status":"completed"},"tags":[]}},{"id":"347ab255-077e-45cc-99d9-f8580994ec18","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom sklearn.metrics import mean_squared_error\n\ndef generate_data(n_samples=200):\n    x = np.linspace(-0.5, 2.5, n_samples)\n    eps = np.random.normal(0, 0.02, size=n_samples)\n    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps)) + eps\n    x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T09:01:00.335551Z","iopub.execute_input":"2025-06-08T09:01:00.335738Z","iopub.status.idle":"2025-06-08T09:01:00.347821Z","shell.execute_reply.started":"2025-06-08T09:01:00.335724Z","shell.execute_reply":"2025-06-08T09:01:00.347109Z"}},"outputs":[],"execution_count":38},{"id":"15fd4e5e-86d7-4872-b266-4c33afefcd5c","cell_type":"code","source":"# === TRAIN LOOP ===\ndef train_loop(learning_rate, prior_pi, prior_sigma1, prior_sigma2, epochs=100):\n    run = wandb.init(project=\"Project-ASI\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    @variational_approximator\n    class BayesianNetwork(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super().__init__()\n            self.bl1 = BayesLinear(input_dim, 1200, prior_pi, prior_sigma1, prior_sigma2)\n            self.bl2 = BayesLinear(1200, 1200, prior_pi, prior_sigma1, prior_sigma2)\n            self.bl3 = BayesLinear(1200, output_dim, prior_pi, prior_sigma1, prior_sigma2)\n    \n        def forward(self, x):\n            x = F.relu(self.bl1(x))\n            x = F.relu(self.bl2(x))\n            x = self.bl3(x)\n            \n            # Accumula KL divergence da tutti i layer\n            self.kl_divergence = (\n                self.bl1.kl_divergence +\n                self.bl2.kl_divergence +\n                self.bl3.kl_divergence\n            )\n            \n            return x  # output: (batch_size, 2) → [mean, raw_variance]\n        \n    # Data prep\n    x, y = generate_data()\n    full_dataset = TensorDataset(x, y)\n   \n    test_size = int(0.2 * len(full_dataset))\n    val_size = int(0.3 * (len(full_dataset) - test_size))\n    train_size = len(full_dataset) - test_size - val_size\n    \n    train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n    trainloader = DataLoader(train_set, batch_size=32, shuffle=True) #explain in the report\n    valloader = DataLoader(val_set, batch_size=32)\n    testloader = DataLoader(test_set, batch_size=32)\n\n    # Model\n    model = BayesianNetwork(1, 2).to(device)  # Output: [mean, logvar]\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.GaussianNLLLoss(full=True, reduction='sum')\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n\n        for batch_idx, (x_batch, y_batch) in enumerate(trainloader):\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=len(trainloader))\n\n            output = model(x_batch)\n            mean = output[:, 0:1]\n            variance = F.softplus(output[:, 1:2]) + 1e-6\n            \n            nll = criterion(mean, y_batch, variance)\n            kld = model.kl_divergence  # updated during forward\n            loss = nll + pi_weight * kld\n\n            train_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n\n        # === Validation ===\n        model.eval()\n        val_loss = 0.0\n        val_rmse = 0.0\n\n        with torch.no_grad():\n            for batch_idx, (x_batch, y_batch) in enumerate(valloader):\n                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\n                output = model(x_batch)\n                mean = output[:, 0:1]\n                variance = F.softplus(output[:, 1:2]) + 1e-6\n\n                nll = criterion(mean, y_batch, variance)\n                kld = model.kl_divergence\n                loss = nll + pi_weight * kld\n\n                val_loss += loss.item()\n                val_rmse += torch.sum((mean - y_batch) ** 2).item()\n\n        val_rmse = np.sqrt(val_rmse / len(val_set))\n\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.2f} | \"\n              f\"Val Loss: {val_loss:.2f} | Val RMSE: {val_rmse:.4f}\")\n\n    metrics = {'train_loss' : train_loss, 'val_loss' : val_loss, 'val_rmse' :  val_rmse}\n    wandb.log(metrics)\n\n\"\"\"\"\"\n    # === Test set evaluation ===\n    model.eval()\n    test_preds = []\n    test_targets = []\n\n    with torch.no_grad():\n        for x_batch, y_batch in testloader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            output = model(x_batch)\n            mean = output[:, 0:1]\n            test_preds.append(mean.cpu())\n            test_targets.append(y_batch.cpu())\n\n    test_preds = torch.cat(test_preds)\n    test_targets = torch.cat(test_targets)\n    test_rmse = torch.sqrt(F.mse_loss(test_preds, test_targets)).item()\n    print(f\"Test RMSE: {test_rmse:.4f}\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T09:01:00.349730Z","iopub.execute_input":"2025-06-08T09:01:00.349944Z","iopub.status.idle":"2025-06-08T09:01:00.366774Z","shell.execute_reply.started":"2025-06-08T09:01:00.349930Z","shell.execute_reply":"2025-06-08T09:01:00.366031Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'\"\"\\n    # === Test set evaluation ===\\n    model.eval()\\n    test_preds = []\\n    test_targets = []\\n\\n    with torch.no_grad():\\n        for x_batch, y_batch in testloader:\\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\\n            output = model(x_batch)\\n            mean = output[:, 0:1]\\n            test_preds.append(mean.cpu())\\n            test_targets.append(y_batch.cpu())\\n\\n    test_preds = torch.cat(test_preds)\\n    test_targets = torch.cat(test_targets)\\n    test_rmse = torch.sqrt(F.mse_loss(test_preds, test_targets)).item()\\n    print(f\"Test RMSE: {test_rmse:.4f}\")\\n'"},"metadata":{}}],"execution_count":39},{"id":"620b5f2a","cell_type":"code","source":"sweep_configuration = {\n     \"method\": \"grid\",\n     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n     'name': \"sweep-BBB-Gaussian_regression\",\n     \"parameters\": {\n         \"learning_rate\": {'values': [1e-3, 1e-4]},\n         \"prior_pi\": {'values': [0.25, 0.5]},\n         \"prior_sigma1\": {'values': [1, math.exp(-1)]},\n         \"prior_sigma2\": {'values': [math.exp(-6), math.exp(-7)]},\n     },\n}\n\nsweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Project-ASI\")\nwandb.agent(sweep_id, function=train_wrapper);","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:01:00.367480Z","iopub.execute_input":"2025-06-08T09:01:00.367706Z","iopub.status.idle":"2025-06-08T09:02:00.686218Z","shell.execute_reply.started":"2025-06-08T09:01:00.367681Z","shell.execute_reply":"2025-06-08T09:02:00.685468Z"},"papermill":{"duration":0.010288,"end_time":"2025-05-26T10:58:20.717183","exception":false,"start_time":"2025-05-26T10:58:20.706895","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Create sweep with ID: x7fwbh87\nSweep URL: https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hhocwy0h with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Project-ASI' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250608_090107-hhocwy0h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">dainty-sweep-1</a></strong> to <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Project-ASI' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dainty-sweep-1</strong> at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h</a><br> View project at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250608_090107-hhocwy0h/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250608_090113-hhocwy0h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">dainty-sweep-1</a></strong> to <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/2 | Train Loss: 14938158.19 | Val Loss: 1982334.75 | Val RMSE: 2.6505\nEpoch 2/2 | Train Loss: 14856628.06 | Val Loss: 1973423.06 | Val RMSE: 1.9034\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_rmse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>14856628.0625</td></tr><tr><td>val_loss</td><td>1973423.0625</td></tr><tr><td>val_rmse</td><td>1.90344</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dainty-sweep-1</strong> at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hhocwy0h</a><br> View project at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250608_090113-hhocwy0h/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ou9zhf91 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Project-ASI' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250608_090134-ou9zhf91</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">copper-sweep-2</a></strong> to <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'Project-ASI' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">copper-sweep-2</strong> at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91</a><br> View project at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250608_090134-ou9zhf91/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250608_090140-ou9zhf91</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">copper-sweep-2</a></strong> to <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/x7fwbh87</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/2 | Train Loss: 12297374.69 | Val Loss: 1629663.56 | Val RMSE: 0.3763\nEpoch 2/2 | Train Loss: 12210770.94 | Val Loss: 1620745.38 | Val RMSE: 0.5176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_rmse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>12210770.9375</td></tr><tr><td>val_loss</td><td>1620745.375</td></tr><tr><td>val_rmse</td><td>0.51759</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">copper-sweep-2</strong> at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ou9zhf91</a><br> View project at: <a href='https://wandb.ai/miriam-lamari2-eurecom/Project-ASI' target=\"_blank\">https://wandb.ai/miriam-lamari2-eurecom/Project-ASI</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250608_090140-ou9zhf91/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n","output_type":"stream"}],"execution_count":40},{"id":"c0471852","cell_type":"code","source":"#wandb.init(project=\"Project-ASI\")\n\n#train_loop(\n#    learning_rate = best_hyperparameters['learning_rate'],\n#    prior_pi = best_hyperparameters['prior_pi'],\n#    prior_sigma1 = best_hyperparameters['prior_sigma1'],\n#    prior_sigma2 = best_hyperparameters['prior_sigma2'],\n#    epochs = 2\n#)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T09:02:00.687913Z","iopub.execute_input":"2025-06-08T09:02:00.688295Z","iopub.status.idle":"2025-06-08T09:02:00.691383Z","shell.execute_reply.started":"2025-06-08T09:02:00.688275Z","shell.execute_reply":"2025-06-08T09:02:00.690797Z"},"papermill":{"duration":10656.287722,"end_time":"2025-05-26T13:55:57.037541","exception":false,"start_time":"2025-05-26T10:58:20.749819","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":41},{"id":"e2250482-8163-4bbd-adb8-7ba8e40e9451","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}