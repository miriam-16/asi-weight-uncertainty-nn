{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901279d6",
   "metadata": {
    "papermill": {
     "duration": 0.003982,
     "end_time": "2025-06-08T09:17:30.813243",
     "exception": false,
     "start_time": "2025-06-08T09:17:30.809261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [ASI Project] Weight Uncertainty in Neural Networks  \n",
    "**Authors**: Miriam Lamari, Francesco Giannuzzo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cc089a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:30.820164Z",
     "iopub.status.busy": "2025-06-08T09:17:30.819928Z",
     "iopub.status.idle": "2025-06-08T09:17:42.929039Z",
     "shell.execute_reply": "2025-06-08T09:17:42.928255Z"
    },
    "papermill": {
     "duration": 12.114111,
     "end_time": "2025-06-08T09:17:42.930470",
     "exception": false,
     "start_time": "2025-06-08T09:17:30.816359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9d4863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:42.937810Z",
     "iopub.status.busy": "2025-06-08T09:17:42.937486Z",
     "iopub.status.idle": "2025-06-08T09:17:44.012815Z",
     "shell.execute_reply": "2025-06-08T09:17:44.012158Z"
    },
    "papermill": {
     "duration": 1.080134,
     "end_time": "2025-06-08T09:17:44.014018",
     "exception": false,
     "start_time": "2025-06-08T09:17:42.933884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancescogiannuzzo2002-fg\u001b[0m (\u001b[33mmiriam-lamari2-eurecom\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wandb-api-key')\n",
    "\n",
    "wandb.login(key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f408a1",
   "metadata": {
    "papermill": {
     "duration": 0.003575,
     "end_time": "2025-06-08T09:17:44.021264",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.017689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Minibatches\n",
    "**minibatch_weight**(batch_idx: int, num_batches: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a3bfad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.028959Z",
     "iopub.status.busy": "2025-06-08T09:17:44.028465Z",
     "iopub.status.idle": "2025-06-08T09:17:44.031798Z",
     "shell.execute_reply": "2025-06-08T09:17:44.031270Z"
    },
    "papermill": {
     "duration": 0.008332,
     "end_time": "2025-06-08T09:17:44.032933",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.024601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - 1)#batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab631eaa",
   "metadata": {
    "papermill": {
     "duration": 0.002991,
     "end_time": "2025-06-08T09:17:44.039263",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.036272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Variational Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee83aad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.046721Z",
     "iopub.status.busy": "2025-06-08T09:17:44.046222Z",
     "iopub.status.idle": "2025-06-08T09:17:44.052307Z",
     "shell.execute_reply": "2025-06-08T09:17:44.051602Z"
    },
    "papermill": {
     "duration": 0.0109,
     "end_time": "2025-06-08T09:17:44.053340",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.042440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class BayesianModule(nn.Module):\n",
    "\n",
    "    \"\"\"Base class for BNN to enable certain behaviour.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def kld(self, *args):\n",
    "        raise NotImplementedError('BayesianModule::kld()')\n",
    "\n",
    "\n",
    "def variational_approximator(model: nn.Module) -> nn.Module:\n",
    "\n",
    "    def kl_divergence(self) -> Tensor:\n",
    "\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    # add `kl_divergence` to the model\n",
    "    setattr(model, 'kl_divergence', kl_divergence)\n",
    "\n",
    "    def elbo(self,\n",
    "             inputs: Tensor,\n",
    "             targets: Tensor,\n",
    "             criterion: Any,\n",
    "             n_samples: int,\n",
    "             w_complexity: Optional[float] = 1.0) -> Tensor:\n",
    "\n",
    "        loss = 0\n",
    "        for sample in range(n_samples):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, targets)\n",
    "            loss += self.kl_divergence() * w_complexity\n",
    "\n",
    "        return loss / n_samples\n",
    "\n",
    "    # add `elbo` to the model\n",
    "    setattr(model, 'elbo', elbo)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b197de",
   "metadata": {
    "papermill": {
     "duration": 0.003037,
     "end_time": "2025-06-08T09:17:44.059584",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.056547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scale Mixture Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8634ca85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.066857Z",
     "iopub.status.busy": "2025-06-08T09:17:44.066636Z",
     "iopub.status.idle": "2025-06-08T09:17:44.072334Z",
     "shell.execute_reply": "2025-06-08T09:17:44.071801Z"
    },
    "papermill": {
     "duration": 0.010506,
     "end_time": "2025-06-08T09:17:44.073354",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.062848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ScaleMixture(nn.Module):\n",
    "\n",
    "    \"\"\"Scale Mixture Prior.\n",
    "\n",
    "    Section 3.3 of the 'Weight Uncertainty in Neural Networks' paper\n",
    "    proposes the use of a Scale Mixture prior for use in variational\n",
    "    inference - this being a fixed-form prior.\n",
    "\n",
    "    The authors note that, should the parameters be allowed to adjust\n",
    "    during training, the prior changes rapidly and attempts to capture\n",
    "    the empirical distribution of the weights. As a result the prior\n",
    "    learns to fit poor initial parameters and struggles to improve.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
    "\n",
    "        \"\"\"Scale Mixture Prior.\n",
    "\n",
    "        The authors of 'Weight Uncertainty in Neural Networks' note:\n",
    "\n",
    "            sigma1 > sigma2:\n",
    "                provides a heavier tail in the prior density than is\n",
    "                seen in a plain Gaussian prior.\n",
    "            sigma2 << 1.0:\n",
    "                causes many of the weights to a priori tightly\n",
    "                concentrate around zero.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Parameter used to scale the two Gaussian distributions.\n",
    "        sigma1 : float\n",
    "            Standard deviation of the first normal distribution.\n",
    "        sigma2 : float\n",
    "            Standard deviation of the second normal distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w: Tensor) -> Tensor:\n",
    "\n",
    "        \"\"\"Log Likelihood of the weight according to the prior.\n",
    "\n",
    "        Calculates the log likelihood of the supplied weight given the\n",
    "        prior distribution - the scale mixture of two Gaussians.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : Tensor\n",
    "            Weight to be used to calculate the log likelihood.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Log likelihood of the weights from the prior distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n",
    "        log_prob = torch.log(p_scalemixture).sum()\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63d093",
   "metadata": {
    "papermill": {
     "duration": 0.003105,
     "end_time": "2025-06-08T09:17:44.079747",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.076642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gaussian Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e2c555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.086747Z",
     "iopub.status.busy": "2025-06-08T09:17:44.086547Z",
     "iopub.status.idle": "2025-06-08T09:17:44.093312Z",
     "shell.execute_reply": "2025-06-08T09:17:44.092783Z"
    },
    "papermill": {
     "duration": 0.011419,
     "end_time": "2025-06-08T09:17:44.094284",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.082865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class GaussianVariational(nn.Module):\n",
    "    def __init__(self, mu: Tensor, rho: Tensor) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self) -> Tensor:\n",
    "        device = self.mu.device\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self) -> Tensor:\n",
    "\n",
    "        if self.w is None:\n",
    "            raise ValueError('self.w must have a value.')\n",
    "\n",
    "        log_const = np.log(np.sqrt(2 * np.pi))\n",
    "        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n",
    "\n",
    "        return log_posterior.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857c832",
   "metadata": {
    "papermill": {
     "duration": 0.003092,
     "end_time": "2025-06-08T09:17:44.100552",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.097460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bayesian Linear Layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a36683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.107471Z",
     "iopub.status.busy": "2025-06-08T09:17:44.107280Z",
     "iopub.status.idle": "2025-06-08T09:17:44.113915Z",
     "shell.execute_reply": "2025-06-08T09:17:44.113349Z"
    },
    "papermill": {
     "duration": 0.011332,
     "end_time": "2025-06-08T09:17:44.114984",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.103652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from .base_bayesian import BayesianModule\n",
    "#from .samplers.gaussian_variational import GaussianVariational\n",
    "#from .samplers.scale_mixture import ScaleMixture\n",
    "\n",
    "\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    \"\"\"Bayesian Linear Layer.\n",
    "\n",
    "    Implementation of a Bayesian Linear Layer as described in the\n",
    "    'Weight Uncertainty in Neural Networks' paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 prior_pi: Optional[float] = 0.5,\n",
    "                 prior_sigma1: Optional[float] = 1.0,\n",
    "                 prior_sigma2: Optional[float] = 0.0025) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2)\n",
    "        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2)\n",
    "        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0)\n",
    "\n",
    "        self.w_posterior = GaussianVariational(w_mu, w_rho)\n",
    "        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "        self.kl_divergence = 0.0\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.bias_posterior.sample()\n",
    "\n",
    "        w_log_prior = self.w_prior.log_prior(w)\n",
    "        b_log_prior = self.bias_prior.log_prior(b)\n",
    "\n",
    "        w_log_posterior = self.w_posterior.log_posterior()\n",
    "        b_log_posterior = self.bias_posterior.log_posterior()\n",
    "\n",
    "        total_log_prior = w_log_prior + b_log_prior\n",
    "        total_log_posterior = w_log_posterior + b_log_posterior\n",
    "        self.kl_divergence = self.kld(total_log_prior, total_log_posterior)\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "    def kld(self, log_prior: Tensor, log_posterior: Tensor) -> Tensor:\n",
    "        return log_posterior - log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9548fe1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.121963Z",
     "iopub.status.busy": "2025-06-08T09:17:44.121746Z",
     "iopub.status.idle": "2025-06-08T09:17:44.185162Z",
     "shell.execute_reply": "2025-06-08T09:17:44.184582Z"
    },
    "papermill": {
     "duration": 0.068134,
     "end_time": "2025-06-08T09:17:44.186265",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.118131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7ef085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.194232Z",
     "iopub.status.busy": "2025-06-08T09:17:44.194029Z",
     "iopub.status.idle": "2025-06-08T09:17:44.197808Z",
     "shell.execute_reply": "2025-06-08T09:17:44.197126Z"
    },
    "papermill": {
     "duration": 0.009424,
     "end_time": "2025-06-08T09:17:44.199110",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.189686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_wrapper():\n",
    "    run = wandb.init(project=\"Project-ASI\")\n",
    "\n",
    "    return train_loop(\n",
    "        learning_rate = wandb.config.learning_rate,\n",
    "        prior_pi = wandb.config.prior_pi,\n",
    "        prior_sigma1=wandb.config.prior_sigma1,\n",
    "        prior_sigma2=wandb.config.prior_sigma2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8d288",
   "metadata": {
    "papermill": {
     "duration": 0.003009,
     "end_time": "2025-06-08T09:17:44.205362",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.202353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tuning Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d125bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:44.212307Z",
     "iopub.status.busy": "2025-06-08T09:17:44.212104Z",
     "iopub.status.idle": "2025-06-08T09:17:45.364598Z",
     "shell.execute_reply": "2025-06-08T09:17:45.364050Z"
    },
    "papermill": {
     "duration": 1.157504,
     "end_time": "2025-06-08T09:17:45.365969",
     "exception": false,
     "start_time": "2025-06-08T09:17:44.208465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def generate_data(n_samples=200):\n",
    "    x = np.linspace(-0.5, 2.5, n_samples)\n",
    "    eps = np.random.normal(0, 0.02, size=n_samples)\n",
    "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps)) + eps\n",
    "    x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab061e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:45.374153Z",
     "iopub.status.busy": "2025-06-08T09:17:45.373769Z",
     "iopub.status.idle": "2025-06-08T09:17:45.388454Z",
     "shell.execute_reply": "2025-06-08T09:17:45.387749Z"
    },
    "papermill": {
     "duration": 0.019964,
     "end_time": "2025-06-08T09:17:45.389498",
     "exception": false,
     "start_time": "2025-06-08T09:17:45.369534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n    # === Test set evaluation ===\\n    model.eval()\\n    test_preds = []\\n    test_targets = []\\n\\n    with torch.no_grad():\\n        for x_batch, y_batch in testloader:\\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\\n            output = model(x_batch)\\n            mean = output[:, 0:1]\\n            test_preds.append(mean.cpu())\\n            test_targets.append(y_batch.cpu())\\n\\n    test_preds = torch.cat(test_preds)\\n    test_targets = torch.cat(test_targets)\\n    test_rmse = torch.sqrt(F.mse_loss(test_preds, test_targets)).item()\\n    print(f\"Test RMSE: {test_rmse:.4f}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === TRAIN LOOP ===\n",
    "def train_loop(learning_rate, prior_pi, prior_sigma1, prior_sigma2, epochs=100):\n",
    "    run = wandb.init(project=\"Project-ASI\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    @variational_approximator\n",
    "    class BayesianNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.bl1 = BayesLinear(input_dim, 1200, prior_pi, prior_sigma1, prior_sigma2)\n",
    "            self.bl2 = BayesLinear(1200, 1200, prior_pi, prior_sigma1, prior_sigma2)\n",
    "            self.bl3 = BayesLinear(1200, output_dim, prior_pi, prior_sigma1, prior_sigma2)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bl1(x))\n",
    "            x = F.relu(self.bl2(x))\n",
    "            x = self.bl3(x)\n",
    "            \n",
    "            # Accumula KL divergence da tutti i layer\n",
    "            self.kl_divergence = (\n",
    "                self.bl1.kl_divergence +\n",
    "                self.bl2.kl_divergence +\n",
    "                self.bl3.kl_divergence\n",
    "            )\n",
    "            \n",
    "            return x  # output: (batch_size, 2) → [mean, raw_variance]\n",
    "        \n",
    "    # Data prep\n",
    "    x, y = generate_data()\n",
    "    full_dataset = TensorDataset(x, y)\n",
    "   \n",
    "    test_size = int(0.2 * len(full_dataset))\n",
    "    val_size = int(0.3 * (len(full_dataset) - test_size))\n",
    "    train_size = len(full_dataset) - test_size - val_size\n",
    "    \n",
    "    train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "    trainloader = DataLoader(train_set, batch_size=32, shuffle=True) #explain in the report\n",
    "    valloader = DataLoader(val_set, batch_size=32)\n",
    "    testloader = DataLoader(test_set, batch_size=32)\n",
    "\n",
    "    # Model\n",
    "    model = BayesianNetwork(1, 2).to(device)  # Output: [mean, logvar]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.GaussianNLLLoss(full=True, reduction='sum')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(trainloader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pi_weight = minibatch_weight(batch_idx=batch_idx, num_batches=len(trainloader))\n",
    "\n",
    "            output = model(x_batch)\n",
    "            mean = output[:, 0:1]\n",
    "            variance = F.softplus(output[:, 1:2]) + 1e-6\n",
    "            \n",
    "            nll = criterion(mean, y_batch, variance)\n",
    "            kld = model.kl_divergence  # updated during forward\n",
    "            loss = nll + pi_weight * kld\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_rmse = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x_batch, y_batch) in enumerate(valloader):\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                output = model(x_batch)\n",
    "                mean = output[:, 0:1]\n",
    "                variance = F.softplus(output[:, 1:2]) + 1e-6\n",
    "\n",
    "                nll = criterion(mean, y_batch, variance)\n",
    "                kld = model.kl_divergence\n",
    "                loss = nll + pi_weight * kld\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_rmse += torch.sum((mean - y_batch) ** 2).item()\n",
    "\n",
    "        val_rmse = np.sqrt(val_rmse / len(val_set))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.2f} | \"\n",
    "              f\"Val Loss: {val_loss:.2f} | Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    metrics = {'train_loss' : train_loss, 'val_loss' : val_loss, 'val_rmse' :  val_rmse}\n",
    "    wandb.log(metrics)\n",
    "\n",
    "\"\"\"\"\"\n",
    "    # === Test set evaluation ===\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in testloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            mean = output[:, 0:1]\n",
    "            test_preds.append(mean.cpu())\n",
    "            test_targets.append(y_batch.cpu())\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_targets = torch.cat(test_targets)\n",
    "    test_rmse = torch.sqrt(F.mse_loss(test_preds, test_targets)).item()\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ae9fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:17:45.397387Z",
     "iopub.status.busy": "2025-06-08T09:17:45.396760Z",
     "iopub.status.idle": "2025-06-08T09:24:42.383044Z",
     "shell.execute_reply": "2025-06-08T09:24:42.382032Z"
    },
    "papermill": {
     "duration": 416.991629,
     "end_time": "2025-06-08T09:24:42.384600",
     "exception": false,
     "start_time": "2025-06-08T09:17:45.392971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6pjj1s0i\n",
      "Sweep URL: https://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n7yh3yry with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091747-n7yh3yry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/n7yh3yry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msunny-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/n7yh3yry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091747-n7yh3yry/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091748-n7yh3yry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/n7yh3yry\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 14940277.00 | Val Loss: 1982882.56 | Val RMSE: 3.9028\n",
      "Epoch 2/100 | Train Loss: 14859162.50 | Val Loss: 1974179.81 | Val RMSE: 4.7342\n",
      "Epoch 3/100 | Train Loss: 14791836.38 | Val Loss: 1965393.44 | Val RMSE: 4.7683\n",
      "Epoch 4/100 | Train Loss: 14727640.38 | Val Loss: 1956439.50 | Val RMSE: 3.6601\n",
      "Epoch 5/100 | Train Loss: 14664721.38 | Val Loss: 1948379.00 | Val RMSE: 2.6705\n",
      "Epoch 6/100 | Train Loss: 14601707.62 | Val Loss: 1939686.38 | Val RMSE: 1.5529\n",
      "Epoch 7/100 | Train Loss: 14532875.31 | Val Loss: 1930251.25 | Val RMSE: 1.2101\n",
      "Epoch 8/100 | Train Loss: 14465265.19 | Val Loss: 1920989.12 | Val RMSE: 1.1063\n",
      "Epoch 9/100 | Train Loss: 14396127.19 | Val Loss: 1912294.50 | Val RMSE: 0.9736\n",
      "Epoch 10/100 | Train Loss: 14328198.00 | Val Loss: 1902370.44 | Val RMSE: 0.8563\n",
      "Epoch 11/100 | Train Loss: 14251797.56 | Val Loss: 1892482.12 | Val RMSE: 0.5224\n",
      "Epoch 12/100 | Train Loss: 14180437.06 | Val Loss: 1882174.50 | Val RMSE: 0.8456\n",
      "Epoch 13/100 | Train Loss: 14098597.75 | Val Loss: 1871597.12 | Val RMSE: 0.8498\n",
      "Epoch 14/100 | Train Loss: 14024839.88 | Val Loss: 1861957.25 | Val RMSE: 1.1916\n",
      "Epoch 15/100 | Train Loss: 13943971.75 | Val Loss: 1851043.88 | Val RMSE: 0.8113\n",
      "Epoch 16/100 | Train Loss: 13863881.56 | Val Loss: 1840512.06 | Val RMSE: 0.5404\n",
      "Epoch 17/100 | Train Loss: 13787553.00 | Val Loss: 1828856.25 | Val RMSE: 0.7256\n",
      "Epoch 18/100 | Train Loss: 13706213.00 | Val Loss: 1818534.56 | Val RMSE: 0.5399\n",
      "Epoch 19/100 | Train Loss: 13628514.44 | Val Loss: 1808381.62 | Val RMSE: 0.5268\n",
      "Epoch 20/100 | Train Loss: 13540807.31 | Val Loss: 1797327.12 | Val RMSE: 0.7677\n",
      "Epoch 21/100 | Train Loss: 13463844.50 | Val Loss: 1786793.56 | Val RMSE: 0.4504\n",
      "Epoch 22/100 | Train Loss: 13386271.62 | Val Loss: 1776119.50 | Val RMSE: 0.3533\n",
      "Epoch 23/100 | Train Loss: 13306266.56 | Val Loss: 1765965.19 | Val RMSE: 0.7326\n",
      "Epoch 24/100 | Train Loss: 13227524.44 | Val Loss: 1755524.25 | Val RMSE: 0.3686\n",
      "Epoch 25/100 | Train Loss: 13154473.75 | Val Loss: 1745324.81 | Val RMSE: 0.4689\n",
      "Epoch 26/100 | Train Loss: 13072325.81 | Val Loss: 1735748.44 | Val RMSE: 0.5995\n",
      "Epoch 27/100 | Train Loss: 12999695.44 | Val Loss: 1725045.38 | Val RMSE: 0.3613\n",
      "Epoch 28/100 | Train Loss: 12922321.81 | Val Loss: 1714883.81 | Val RMSE: 0.6150\n",
      "Epoch 29/100 | Train Loss: 12848367.94 | Val Loss: 1704593.88 | Val RMSE: 0.5622\n",
      "Epoch 30/100 | Train Loss: 12769867.38 | Val Loss: 1694938.75 | Val RMSE: 0.3312\n",
      "Epoch 31/100 | Train Loss: 12696083.50 | Val Loss: 1686005.12 | Val RMSE: 0.3657\n",
      "Epoch 32/100 | Train Loss: 12626168.25 | Val Loss: 1675946.75 | Val RMSE: 0.3700\n",
      "Epoch 33/100 | Train Loss: 12554475.12 | Val Loss: 1666383.69 | Val RMSE: 0.5281\n",
      "Epoch 34/100 | Train Loss: 12480381.50 | Val Loss: 1656706.88 | Val RMSE: 0.3232\n",
      "Epoch 35/100 | Train Loss: 12408275.06 | Val Loss: 1647153.56 | Val RMSE: 0.6093\n",
      "Epoch 36/100 | Train Loss: 12338138.31 | Val Loss: 1637984.44 | Val RMSE: 0.4315\n",
      "Epoch 37/100 | Train Loss: 12266176.06 | Val Loss: 1628492.50 | Val RMSE: 1.2213\n",
      "Epoch 38/100 | Train Loss: 12199948.44 | Val Loss: 1619207.62 | Val RMSE: 0.8222\n",
      "Epoch 39/100 | Train Loss: 12127542.44 | Val Loss: 1609017.06 | Val RMSE: 0.4903\n",
      "Epoch 40/100 | Train Loss: 12055255.06 | Val Loss: 1600353.75 | Val RMSE: 0.8752\n",
      "Epoch 41/100 | Train Loss: 11990751.00 | Val Loss: 1591470.62 | Val RMSE: 1.0065\n",
      "Epoch 42/100 | Train Loss: 11922118.81 | Val Loss: 1581839.12 | Val RMSE: 0.8020\n",
      "Epoch 43/100 | Train Loss: 11857888.06 | Val Loss: 1573518.88 | Val RMSE: 1.1835\n",
      "Epoch 44/100 | Train Loss: 11789346.50 | Val Loss: 1565849.38 | Val RMSE: 1.0517\n",
      "Epoch 45/100 | Train Loss: 11726437.88 | Val Loss: 1557004.31 | Val RMSE: 1.1124\n",
      "Epoch 46/100 | Train Loss: 11664150.62 | Val Loss: 1548555.81 | Val RMSE: 0.5363\n",
      "Epoch 47/100 | Train Loss: 11602981.38 | Val Loss: 1540185.00 | Val RMSE: 0.3481\n",
      "Epoch 48/100 | Train Loss: 11536937.25 | Val Loss: 1532271.56 | Val RMSE: 0.5763\n",
      "Epoch 49/100 | Train Loss: 11479589.06 | Val Loss: 1522975.19 | Val RMSE: 0.3297\n",
      "Epoch 50/100 | Train Loss: 11418423.06 | Val Loss: 1516046.94 | Val RMSE: 0.3284\n",
      "Epoch 51/100 | Train Loss: 11360502.31 | Val Loss: 1507727.62 | Val RMSE: 0.3679\n",
      "Epoch 52/100 | Train Loss: 11292490.06 | Val Loss: 1500154.00 | Val RMSE: 0.3348\n",
      "Epoch 53/100 | Train Loss: 11232621.75 | Val Loss: 1490662.25 | Val RMSE: 0.3947\n",
      "Epoch 54/100 | Train Loss: 11173888.06 | Val Loss: 1484138.62 | Val RMSE: 0.3478\n",
      "Epoch 55/100 | Train Loss: 11109688.12 | Val Loss: 1475912.69 | Val RMSE: 0.5760\n",
      "Epoch 56/100 | Train Loss: 11056615.00 | Val Loss: 1467786.75 | Val RMSE: 0.4798\n",
      "Epoch 57/100 | Train Loss: 11000967.94 | Val Loss: 1459935.00 | Val RMSE: 0.6496\n",
      "Epoch 58/100 | Train Loss: 10940824.31 | Val Loss: 1453265.12 | Val RMSE: 0.7188\n",
      "Epoch 59/100 | Train Loss: 10880389.75 | Val Loss: 1445113.50 | Val RMSE: 0.7290\n",
      "Epoch 60/100 | Train Loss: 10824912.88 | Val Loss: 1438062.25 | Val RMSE: 0.4576\n",
      "Epoch 61/100 | Train Loss: 10764963.25 | Val Loss: 1429148.06 | Val RMSE: 0.3622\n",
      "Epoch 62/100 | Train Loss: 10714382.44 | Val Loss: 1422560.06 | Val RMSE: 1.3120\n",
      "Epoch 63/100 | Train Loss: 10655971.94 | Val Loss: 1413997.69 | Val RMSE: 0.5920\n",
      "Epoch 64/100 | Train Loss: 10599118.38 | Val Loss: 1407979.88 | Val RMSE: 0.7430\n",
      "Epoch 65/100 | Train Loss: 10547728.94 | Val Loss: 1399933.56 | Val RMSE: 2.1530\n",
      "Epoch 66/100 | Train Loss: 10488074.00 | Val Loss: 1393169.38 | Val RMSE: 0.6444\n",
      "Epoch 67/100 | Train Loss: 10441721.88 | Val Loss: 1386272.50 | Val RMSE: 1.2322\n",
      "Epoch 68/100 | Train Loss: 10388708.19 | Val Loss: 1379455.88 | Val RMSE: 1.8685\n",
      "Epoch 69/100 | Train Loss: 10332036.94 | Val Loss: 1372235.69 | Val RMSE: 1.9823\n",
      "Epoch 70/100 | Train Loss: 10288416.06 | Val Loss: 1365558.06 | Val RMSE: 1.7640\n",
      "Epoch 71/100 | Train Loss: 10233119.44 | Val Loss: 1359288.00 | Val RMSE: 1.4940\n",
      "Epoch 72/100 | Train Loss: 10185915.81 | Val Loss: 1352555.19 | Val RMSE: 0.7766\n",
      "Epoch 73/100 | Train Loss: 10136823.81 | Val Loss: 1347124.56 | Val RMSE: 0.5734\n",
      "Epoch 74/100 | Train Loss: 10096317.06 | Val Loss: 1341351.75 | Val RMSE: 0.7601\n",
      "Epoch 75/100 | Train Loss: 10055228.12 | Val Loss: 1335398.56 | Val RMSE: 0.6141\n",
      "Epoch 76/100 | Train Loss: 10002710.44 | Val Loss: 1329184.38 | Val RMSE: 0.7275\n",
      "Epoch 77/100 | Train Loss: 9959556.25 | Val Loss: 1324036.19 | Val RMSE: 0.4303\n",
      "Epoch 78/100 | Train Loss: 9925261.38 | Val Loss: 1319342.94 | Val RMSE: 0.8338\n",
      "Epoch 79/100 | Train Loss: 9883935.00 | Val Loss: 1314619.25 | Val RMSE: 0.9639\n",
      "Epoch 80/100 | Train Loss: 9848345.06 | Val Loss: 1309230.31 | Val RMSE: 0.4445\n",
      "Epoch 81/100 | Train Loss: 9817096.38 | Val Loss: 1305639.00 | Val RMSE: 0.5360\n",
      "Epoch 82/100 | Train Loss: 9790304.19 | Val Loss: 1301588.31 | Val RMSE: 0.5046\n",
      "Epoch 83/100 | Train Loss: 9757098.50 | Val Loss: 1297873.88 | Val RMSE: 0.4952\n",
      "Epoch 84/100 | Train Loss: 9730026.00 | Val Loss: 1294081.31 | Val RMSE: 0.8153\n",
      "Epoch 85/100 | Train Loss: 9700435.06 | Val Loss: 1292460.69 | Val RMSE: 0.3294\n",
      "Epoch 86/100 | Train Loss: 9686248.69 | Val Loss: 1288087.56 | Val RMSE: 0.3751\n",
      "Epoch 87/100 | Train Loss: 9657669.19 | Val Loss: 1284589.88 | Val RMSE: 0.2977\n",
      "Epoch 88/100 | Train Loss: 9634378.06 | Val Loss: 1282423.44 | Val RMSE: 0.3037\n",
      "Epoch 89/100 | Train Loss: 9606924.12 | Val Loss: 1279368.75 | Val RMSE: 0.3187\n",
      "Epoch 90/100 | Train Loss: 9592321.94 | Val Loss: 1276136.31 | Val RMSE: 0.5397\n",
      "Epoch 91/100 | Train Loss: 9574294.69 | Val Loss: 1274257.50 | Val RMSE: 0.4654\n",
      "Epoch 92/100 | Train Loss: 9549939.56 | Val Loss: 1271662.44 | Val RMSE: 0.3285\n",
      "Epoch 93/100 | Train Loss: 9533394.00 | Val Loss: 1269212.06 | Val RMSE: 0.2842\n",
      "Epoch 94/100 | Train Loss: 9511259.81 | Val Loss: 1266736.56 | Val RMSE: 0.9528\n",
      "Epoch 95/100 | Train Loss: 9497918.50 | Val Loss: 1265108.94 | Val RMSE: 0.5113\n",
      "Epoch 96/100 | Train Loss: 9474340.88 | Val Loss: 1261739.81 | Val RMSE: 0.3060\n",
      "Epoch 97/100 | Train Loss: 9455481.31 | Val Loss: 1258959.75 | Val RMSE: 0.5610\n",
      "Epoch 98/100 | Train Loss: 9435401.88 | Val Loss: 1255734.38 | Val RMSE: 0.4036\n",
      "Epoch 99/100 | Train Loss: 9415977.12 | Val Loss: 1254188.62 | Val RMSE: 0.3121\n",
      "Epoch 100/100 | Train Loss: 9402503.94 | Val Loss: 1252236.62 | Val RMSE: 0.3642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 9402503.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1252236.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.36418\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msunny-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/n7yh3yry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091748-n7yh3yry/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: opahk6sc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091817-opahk6sc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/opahk6sc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/opahk6sc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091817-opahk6sc/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091818-opahk6sc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/opahk6sc\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 15312938.88 | Val Loss: 2037285.12 | Val RMSE: 1.8420\n",
      "Epoch 2/100 | Train Loss: 15279642.62 | Val Loss: 2033380.56 | Val RMSE: 2.1400\n",
      "Epoch 3/100 | Train Loss: 15246114.62 | Val Loss: 2029604.12 | Val RMSE: 3.0930\n",
      "Epoch 4/100 | Train Loss: 15220165.44 | Val Loss: 2026001.00 | Val RMSE: 2.6451\n",
      "Epoch 5/100 | Train Loss: 15192272.94 | Val Loss: 2022939.38 | Val RMSE: 2.0092\n",
      "Epoch 6/100 | Train Loss: 15165617.75 | Val Loss: 2019779.62 | Val RMSE: 1.2148\n",
      "Epoch 7/100 | Train Loss: 15140811.56 | Val Loss: 2015970.62 | Val RMSE: 0.7178\n",
      "Epoch 8/100 | Train Loss: 15111967.75 | Val Loss: 2012445.81 | Val RMSE: 0.6369\n",
      "Epoch 9/100 | Train Loss: 15088758.06 | Val Loss: 2008874.56 | Val RMSE: 0.6955\n",
      "Epoch 10/100 | Train Loss: 15059880.31 | Val Loss: 2005151.19 | Val RMSE: 0.4960\n",
      "Epoch 11/100 | Train Loss: 15033962.31 | Val Loss: 2001482.94 | Val RMSE: 0.4573\n",
      "Epoch 12/100 | Train Loss: 15004777.62 | Val Loss: 1998788.31 | Val RMSE: 0.7335\n",
      "Epoch 13/100 | Train Loss: 14981109.19 | Val Loss: 1994892.12 | Val RMSE: 0.6050\n",
      "Epoch 14/100 | Train Loss: 14954711.50 | Val Loss: 1990675.56 | Val RMSE: 0.4586\n",
      "Epoch 15/100 | Train Loss: 14923442.50 | Val Loss: 1987781.38 | Val RMSE: 0.5503\n",
      "Epoch 16/100 | Train Loss: 14901972.81 | Val Loss: 1983670.94 | Val RMSE: 0.5380\n",
      "Epoch 17/100 | Train Loss: 14871265.62 | Val Loss: 1980235.06 | Val RMSE: 0.3804\n",
      "Epoch 18/100 | Train Loss: 14843370.25 | Val Loss: 1976463.44 | Val RMSE: 0.3594\n",
      "Epoch 19/100 | Train Loss: 14818515.56 | Val Loss: 1973026.94 | Val RMSE: 0.5190\n",
      "Epoch 20/100 | Train Loss: 14787910.00 | Val Loss: 1969487.44 | Val RMSE: 0.7752\n",
      "Epoch 21/100 | Train Loss: 14766647.19 | Val Loss: 1966049.12 | Val RMSE: 0.9366\n",
      "Epoch 22/100 | Train Loss: 14736791.50 | Val Loss: 1962425.12 | Val RMSE: 1.0612\n",
      "Epoch 23/100 | Train Loss: 14713473.69 | Val Loss: 1958783.25 | Val RMSE: 0.8956\n",
      "Epoch 24/100 | Train Loss: 14685278.56 | Val Loss: 1956003.44 | Val RMSE: 1.0871\n",
      "Epoch 25/100 | Train Loss: 14662809.81 | Val Loss: 1952482.00 | Val RMSE: 0.7795\n",
      "Epoch 26/100 | Train Loss: 14638884.88 | Val Loss: 1949176.56 | Val RMSE: 0.5068\n",
      "Epoch 27/100 | Train Loss: 14614637.44 | Val Loss: 1946569.69 | Val RMSE: 0.6702\n",
      "Epoch 28/100 | Train Loss: 14590330.19 | Val Loss: 1942775.06 | Val RMSE: 0.4276\n",
      "Epoch 29/100 | Train Loss: 14568436.25 | Val Loss: 1939777.62 | Val RMSE: 0.7941\n",
      "Epoch 30/100 | Train Loss: 14543060.12 | Val Loss: 1936381.12 | Val RMSE: 0.5572\n",
      "Epoch 31/100 | Train Loss: 14521399.44 | Val Loss: 1933701.88 | Val RMSE: 0.4205\n",
      "Epoch 32/100 | Train Loss: 14501538.94 | Val Loss: 1930902.94 | Val RMSE: 1.0945\n",
      "Epoch 33/100 | Train Loss: 14475144.12 | Val Loss: 1928099.81 | Val RMSE: 0.3066\n",
      "Epoch 34/100 | Train Loss: 14458480.06 | Val Loss: 1925219.69 | Val RMSE: 0.5209\n",
      "Epoch 35/100 | Train Loss: 14433712.75 | Val Loss: 1921851.69 | Val RMSE: 0.8187\n",
      "Epoch 36/100 | Train Loss: 14410027.38 | Val Loss: 1919119.69 | Val RMSE: 0.8800\n",
      "Epoch 37/100 | Train Loss: 14391130.19 | Val Loss: 1916762.06 | Val RMSE: 0.6694\n",
      "Epoch 38/100 | Train Loss: 14369548.56 | Val Loss: 1913753.62 | Val RMSE: 1.4948\n",
      "Epoch 39/100 | Train Loss: 14349168.25 | Val Loss: 1910626.62 | Val RMSE: 1.4678\n",
      "Epoch 40/100 | Train Loss: 14324465.75 | Val Loss: 1908008.12 | Val RMSE: 0.3765\n",
      "Epoch 41/100 | Train Loss: 14305839.81 | Val Loss: 1905343.25 | Val RMSE: 0.6280\n",
      "Epoch 42/100 | Train Loss: 14286373.06 | Val Loss: 1903007.19 | Val RMSE: 0.5473\n",
      "Epoch 43/100 | Train Loss: 14267243.00 | Val Loss: 1900095.94 | Val RMSE: 0.3502\n",
      "Epoch 44/100 | Train Loss: 14246297.75 | Val Loss: 1897664.88 | Val RMSE: 0.2820\n",
      "Epoch 45/100 | Train Loss: 14225460.44 | Val Loss: 1895399.69 | Val RMSE: 0.7995\n",
      "Epoch 46/100 | Train Loss: 14206798.44 | Val Loss: 1892686.62 | Val RMSE: 0.4722\n",
      "Epoch 47/100 | Train Loss: 14189340.31 | Val Loss: 1889591.19 | Val RMSE: 0.4376\n",
      "Epoch 48/100 | Train Loss: 14175995.06 | Val Loss: 1887707.50 | Val RMSE: 0.6857\n",
      "Epoch 49/100 | Train Loss: 14151841.62 | Val Loss: 1885563.06 | Val RMSE: 0.4636\n",
      "Epoch 50/100 | Train Loss: 14135208.81 | Val Loss: 1883219.69 | Val RMSE: 0.9075\n",
      "Epoch 51/100 | Train Loss: 14117967.19 | Val Loss: 1880015.81 | Val RMSE: 1.1279\n",
      "Epoch 52/100 | Train Loss: 14099121.38 | Val Loss: 1877841.25 | Val RMSE: 0.7044\n",
      "Epoch 53/100 | Train Loss: 14081805.00 | Val Loss: 1875892.19 | Val RMSE: 0.3472\n",
      "Epoch 54/100 | Train Loss: 14061861.62 | Val Loss: 1873898.06 | Val RMSE: 0.3734\n",
      "Epoch 55/100 | Train Loss: 14047497.75 | Val Loss: 1871615.44 | Val RMSE: 0.4046\n",
      "Epoch 56/100 | Train Loss: 14029623.25 | Val Loss: 1868270.81 | Val RMSE: 0.7180\n",
      "Epoch 57/100 | Train Loss: 14010759.88 | Val Loss: 1866940.44 | Val RMSE: 0.4465\n",
      "Epoch 58/100 | Train Loss: 13998004.62 | Val Loss: 1863567.00 | Val RMSE: 0.4287\n",
      "Epoch 59/100 | Train Loss: 13976737.06 | Val Loss: 1862160.25 | Val RMSE: 0.4163\n",
      "Epoch 60/100 | Train Loss: 13965596.06 | Val Loss: 1860003.69 | Val RMSE: 0.3470\n",
      "Epoch 61/100 | Train Loss: 13946582.62 | Val Loss: 1858238.88 | Val RMSE: 0.5454\n",
      "Epoch 62/100 | Train Loss: 13929241.50 | Val Loss: 1855592.38 | Val RMSE: 0.5658\n",
      "Epoch 63/100 | Train Loss: 13910446.50 | Val Loss: 1853594.00 | Val RMSE: 0.7332\n",
      "Epoch 64/100 | Train Loss: 13894882.25 | Val Loss: 1851371.44 | Val RMSE: 1.1868\n",
      "Epoch 65/100 | Train Loss: 13881162.31 | Val Loss: 1849077.50 | Val RMSE: 0.6837\n",
      "Epoch 66/100 | Train Loss: 13864671.12 | Val Loss: 1847001.81 | Val RMSE: 1.0988\n",
      "Epoch 67/100 | Train Loss: 13850848.56 | Val Loss: 1846348.75 | Val RMSE: 1.2749\n",
      "Epoch 68/100 | Train Loss: 13832845.44 | Val Loss: 1843626.25 | Val RMSE: 0.4015\n",
      "Epoch 69/100 | Train Loss: 13823884.06 | Val Loss: 1841659.69 | Val RMSE: 0.3894\n",
      "Epoch 70/100 | Train Loss: 13807259.94 | Val Loss: 1840130.00 | Val RMSE: 0.6429\n",
      "Epoch 71/100 | Train Loss: 13793151.88 | Val Loss: 1837151.62 | Val RMSE: 0.5511\n",
      "Epoch 72/100 | Train Loss: 13777573.56 | Val Loss: 1835050.25 | Val RMSE: 0.3561\n",
      "Epoch 73/100 | Train Loss: 13763026.38 | Val Loss: 1833714.19 | Val RMSE: 0.3094\n",
      "Epoch 74/100 | Train Loss: 13750846.38 | Val Loss: 1832550.62 | Val RMSE: 0.3375\n",
      "Epoch 75/100 | Train Loss: 13733166.62 | Val Loss: 1830578.75 | Val RMSE: 0.4002\n",
      "Epoch 76/100 | Train Loss: 13725266.81 | Val Loss: 1828405.81 | Val RMSE: 0.4154\n",
      "Epoch 77/100 | Train Loss: 13714844.12 | Val Loss: 1827173.62 | Val RMSE: 0.5606\n",
      "Epoch 78/100 | Train Loss: 13704479.50 | Val Loss: 1825964.75 | Val RMSE: 0.7250\n",
      "Epoch 79/100 | Train Loss: 13685943.06 | Val Loss: 1823085.38 | Val RMSE: 0.3807\n",
      "Epoch 80/100 | Train Loss: 13680900.06 | Val Loss: 1822816.94 | Val RMSE: 0.3612\n",
      "Epoch 81/100 | Train Loss: 13668751.31 | Val Loss: 1820496.75 | Val RMSE: 0.3263\n",
      "Epoch 82/100 | Train Loss: 13653139.19 | Val Loss: 1819425.75 | Val RMSE: 1.3370\n",
      "Epoch 83/100 | Train Loss: 13647639.38 | Val Loss: 1817875.56 | Val RMSE: 1.9575\n",
      "Epoch 84/100 | Train Loss: 13631229.81 | Val Loss: 1817152.31 | Val RMSE: 1.1494\n",
      "Epoch 85/100 | Train Loss: 13627206.56 | Val Loss: 1816067.38 | Val RMSE: 0.4013\n",
      "Epoch 86/100 | Train Loss: 13618375.88 | Val Loss: 1815285.31 | Val RMSE: 0.6454\n",
      "Epoch 87/100 | Train Loss: 13611425.25 | Val Loss: 1813644.81 | Val RMSE: 0.8732\n",
      "Epoch 88/100 | Train Loss: 13599358.69 | Val Loss: 1813083.81 | Val RMSE: 0.7669\n",
      "Epoch 89/100 | Train Loss: 13598323.94 | Val Loss: 1812173.00 | Val RMSE: 0.4774\n",
      "Epoch 90/100 | Train Loss: 13589435.56 | Val Loss: 1811371.69 | Val RMSE: 0.3679\n",
      "Epoch 91/100 | Train Loss: 13583959.94 | Val Loss: 1810013.81 | Val RMSE: 0.3292\n",
      "Epoch 92/100 | Train Loss: 13580841.00 | Val Loss: 1809401.56 | Val RMSE: 0.3072\n",
      "Epoch 93/100 | Train Loss: 13574011.56 | Val Loss: 1808891.12 | Val RMSE: 0.3237\n",
      "Epoch 94/100 | Train Loss: 13565903.19 | Val Loss: 1808503.62 | Val RMSE: 0.3633\n",
      "Epoch 95/100 | Train Loss: 13564239.81 | Val Loss: 1807980.88 | Val RMSE: 0.3341\n",
      "Epoch 96/100 | Train Loss: 13558115.31 | Val Loss: 1806942.69 | Val RMSE: 0.3837\n",
      "Epoch 97/100 | Train Loss: 13550715.62 | Val Loss: 1806678.25 | Val RMSE: 0.2959\n",
      "Epoch 98/100 | Train Loss: 13541069.75 | Val Loss: 1805728.69 | Val RMSE: 0.3467\n",
      "Epoch 99/100 | Train Loss: 13542443.56 | Val Loss: 1804543.31 | Val RMSE: 0.4192\n",
      "Epoch 100/100 | Train Loss: 13534419.31 | Val Loss: 1803966.69 | Val RMSE: 0.3988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml; uploading history steps 0-0, summary, console lines 0-99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 13534419.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1803966.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.39879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/opahk6sc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091818-opahk6sc/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w37u9sdl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091839-w37u9sdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/w37u9sdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33munique-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/w37u9sdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091839-w37u9sdl/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091840-w37u9sdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/w37u9sdl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 12301674.69 | Val Loss: 1631989.69 | Val RMSE: 2.7970\n",
      "Epoch 2/100 | Train Loss: 12228138.69 | Val Loss: 1623975.00 | Val RMSE: 4.5538\n",
      "Epoch 3/100 | Train Loss: 12170010.56 | Val Loss: 1615965.38 | Val RMSE: 4.8105\n",
      "Epoch 4/100 | Train Loss: 12107438.44 | Val Loss: 1608496.38 | Val RMSE: 4.5722\n",
      "Epoch 5/100 | Train Loss: 12047739.62 | Val Loss: 1600292.19 | Val RMSE: 4.0578\n",
      "Epoch 6/100 | Train Loss: 11990777.38 | Val Loss: 1591835.88 | Val RMSE: 3.7130\n",
      "Epoch 7/100 | Train Loss: 11926536.44 | Val Loss: 1583196.50 | Val RMSE: 2.9944\n",
      "Epoch 8/100 | Train Loss: 11863301.12 | Val Loss: 1574608.56 | Val RMSE: 2.6567\n",
      "Epoch 9/100 | Train Loss: 11798847.38 | Val Loss: 1565999.69 | Val RMSE: 1.7489\n",
      "Epoch 10/100 | Train Loss: 11733519.81 | Val Loss: 1557107.81 | Val RMSE: 1.6209\n",
      "Epoch 11/100 | Train Loss: 11665545.06 | Val Loss: 1547523.62 | Val RMSE: 1.4698\n",
      "Epoch 12/100 | Train Loss: 11593056.94 | Val Loss: 1537819.00 | Val RMSE: 1.1933\n",
      "Epoch 13/100 | Train Loss: 11519634.06 | Val Loss: 1528327.75 | Val RMSE: 1.2211\n",
      "Epoch 14/100 | Train Loss: 11448187.12 | Val Loss: 1518298.25 | Val RMSE: 0.8579\n",
      "Epoch 15/100 | Train Loss: 11372992.94 | Val Loss: 1508464.75 | Val RMSE: 1.0973\n",
      "Epoch 16/100 | Train Loss: 11297374.25 | Val Loss: 1498498.31 | Val RMSE: 0.8801\n",
      "Epoch 17/100 | Train Loss: 11223086.69 | Val Loss: 1488580.44 | Val RMSE: 0.7678\n",
      "Epoch 18/100 | Train Loss: 11148177.44 | Val Loss: 1478558.19 | Val RMSE: 0.9075\n",
      "Epoch 19/100 | Train Loss: 11073063.75 | Val Loss: 1467896.81 | Val RMSE: 0.7400\n",
      "Epoch 20/100 | Train Loss: 10997921.25 | Val Loss: 1458609.94 | Val RMSE: 0.8364\n",
      "Epoch 21/100 | Train Loss: 10924447.88 | Val Loss: 1448248.12 | Val RMSE: 0.8755\n",
      "Epoch 22/100 | Train Loss: 10850016.56 | Val Loss: 1438645.50 | Val RMSE: 0.6781\n",
      "Epoch 23/100 | Train Loss: 10774963.31 | Val Loss: 1429021.12 | Val RMSE: 0.8044\n",
      "Epoch 24/100 | Train Loss: 10705957.75 | Val Loss: 1419476.44 | Val RMSE: 0.5927\n",
      "Epoch 25/100 | Train Loss: 10632022.06 | Val Loss: 1409667.75 | Val RMSE: 0.5102\n",
      "Epoch 26/100 | Train Loss: 10557775.50 | Val Loss: 1399916.94 | Val RMSE: 0.5765\n",
      "Epoch 27/100 | Train Loss: 10483599.81 | Val Loss: 1390457.38 | Val RMSE: 0.5652\n",
      "Epoch 28/100 | Train Loss: 10416944.75 | Val Loss: 1381548.50 | Val RMSE: 0.5940\n",
      "Epoch 29/100 | Train Loss: 10347322.69 | Val Loss: 1372025.00 | Val RMSE: 0.6140\n",
      "Epoch 30/100 | Train Loss: 10275227.12 | Val Loss: 1362837.31 | Val RMSE: 0.6009\n",
      "Epoch 31/100 | Train Loss: 10207488.12 | Val Loss: 1354380.50 | Val RMSE: 0.7136\n",
      "Epoch 32/100 | Train Loss: 10138804.94 | Val Loss: 1344537.81 | Val RMSE: 0.6262\n",
      "Epoch 33/100 | Train Loss: 10074098.19 | Val Loss: 1336082.88 | Val RMSE: 0.7884\n",
      "Epoch 34/100 | Train Loss: 10002328.25 | Val Loss: 1326904.75 | Val RMSE: 0.6661\n",
      "Epoch 35/100 | Train Loss: 9940891.06 | Val Loss: 1318056.50 | Val RMSE: 0.4713\n",
      "Epoch 36/100 | Train Loss: 9872007.81 | Val Loss: 1309703.19 | Val RMSE: 0.6311\n",
      "Epoch 37/100 | Train Loss: 9809105.88 | Val Loss: 1301131.44 | Val RMSE: 0.6022\n",
      "Epoch 38/100 | Train Loss: 9739790.31 | Val Loss: 1292221.94 | Val RMSE: 0.6389\n",
      "Epoch 39/100 | Train Loss: 9675234.00 | Val Loss: 1284549.31 | Val RMSE: 0.5355\n",
      "Epoch 40/100 | Train Loss: 9616969.19 | Val Loss: 1275669.62 | Val RMSE: 0.4683\n",
      "Epoch 41/100 | Train Loss: 9556016.44 | Val Loss: 1267628.50 | Val RMSE: 0.4368\n",
      "Epoch 42/100 | Train Loss: 9493373.62 | Val Loss: 1258994.50 | Val RMSE: 0.5030\n",
      "Epoch 43/100 | Train Loss: 9431417.31 | Val Loss: 1250495.69 | Val RMSE: 0.5051\n",
      "Epoch 44/100 | Train Loss: 9371747.00 | Val Loss: 1242945.06 | Val RMSE: 0.4401\n",
      "Epoch 45/100 | Train Loss: 9314674.69 | Val Loss: 1234949.94 | Val RMSE: 0.5809\n",
      "Epoch 46/100 | Train Loss: 9250448.38 | Val Loss: 1227398.12 | Val RMSE: 0.4857\n",
      "Epoch 47/100 | Train Loss: 9190747.00 | Val Loss: 1219564.19 | Val RMSE: 0.4151\n",
      "Epoch 48/100 | Train Loss: 9135160.31 | Val Loss: 1211524.56 | Val RMSE: 0.5130\n",
      "Epoch 49/100 | Train Loss: 9077912.44 | Val Loss: 1204275.62 | Val RMSE: 0.4797\n",
      "Epoch 50/100 | Train Loss: 9012645.31 | Val Loss: 1196046.31 | Val RMSE: 0.3958\n",
      "Epoch 51/100 | Train Loss: 8958206.62 | Val Loss: 1188552.44 | Val RMSE: 0.4799\n",
      "Epoch 52/100 | Train Loss: 8900815.44 | Val Loss: 1181305.44 | Val RMSE: 0.4504\n",
      "Epoch 53/100 | Train Loss: 8841744.12 | Val Loss: 1173254.69 | Val RMSE: 0.3659\n",
      "Epoch 54/100 | Train Loss: 8787072.25 | Val Loss: 1166199.75 | Val RMSE: 0.3449\n",
      "Epoch 55/100 | Train Loss: 8734812.31 | Val Loss: 1158222.12 | Val RMSE: 0.4340\n",
      "Epoch 56/100 | Train Loss: 8677352.00 | Val Loss: 1151470.00 | Val RMSE: 0.4492\n",
      "Epoch 57/100 | Train Loss: 8626389.62 | Val Loss: 1143967.75 | Val RMSE: 0.5267\n",
      "Epoch 58/100 | Train Loss: 8576935.06 | Val Loss: 1137210.38 | Val RMSE: 0.4915\n",
      "Epoch 59/100 | Train Loss: 8512797.75 | Val Loss: 1128821.00 | Val RMSE: 0.3487\n",
      "Epoch 60/100 | Train Loss: 8466624.50 | Val Loss: 1123021.75 | Val RMSE: 0.5146\n",
      "Epoch 61/100 | Train Loss: 8402222.94 | Val Loss: 1115397.06 | Val RMSE: 0.5311\n",
      "Epoch 62/100 | Train Loss: 8354897.62 | Val Loss: 1108533.12 | Val RMSE: 0.3657\n",
      "Epoch 63/100 | Train Loss: 8301154.94 | Val Loss: 1101572.69 | Val RMSE: 0.3526\n",
      "Epoch 64/100 | Train Loss: 8252468.62 | Val Loss: 1093670.31 | Val RMSE: 0.3226\n",
      "Epoch 65/100 | Train Loss: 8202246.25 | Val Loss: 1087653.81 | Val RMSE: 0.4055\n",
      "Epoch 66/100 | Train Loss: 8146988.38 | Val Loss: 1081016.31 | Val RMSE: 0.5686\n",
      "Epoch 67/100 | Train Loss: 8100073.06 | Val Loss: 1073828.75 | Val RMSE: 0.3698\n",
      "Epoch 68/100 | Train Loss: 8052574.88 | Val Loss: 1067784.50 | Val RMSE: 0.3941\n",
      "Epoch 69/100 | Train Loss: 7996417.25 | Val Loss: 1061657.19 | Val RMSE: 0.6375\n",
      "Epoch 70/100 | Train Loss: 7942841.06 | Val Loss: 1054916.88 | Val RMSE: 1.0253\n",
      "Epoch 71/100 | Train Loss: 7895590.88 | Val Loss: 1049282.69 | Val RMSE: 0.6272\n",
      "Epoch 72/100 | Train Loss: 7852214.66 | Val Loss: 1043135.78 | Val RMSE: 0.5156\n",
      "Epoch 73/100 | Train Loss: 7811926.09 | Val Loss: 1036202.97 | Val RMSE: 0.5436\n",
      "Epoch 74/100 | Train Loss: 7770591.94 | Val Loss: 1030542.41 | Val RMSE: 0.4776\n",
      "Epoch 75/100 | Train Loss: 7729466.00 | Val Loss: 1025533.31 | Val RMSE: 0.3492\n",
      "Epoch 76/100 | Train Loss: 7680416.84 | Val Loss: 1019272.12 | Val RMSE: 0.5866\n",
      "Epoch 77/100 | Train Loss: 7638621.81 | Val Loss: 1014722.62 | Val RMSE: 0.4713\n",
      "Epoch 78/100 | Train Loss: 7601215.62 | Val Loss: 1009982.22 | Val RMSE: 0.3238\n",
      "Epoch 79/100 | Train Loss: 7559018.12 | Val Loss: 1004189.69 | Val RMSE: 0.3198\n",
      "Epoch 80/100 | Train Loss: 7529728.47 | Val Loss: 1000828.91 | Val RMSE: 0.3007\n",
      "Epoch 81/100 | Train Loss: 7498026.84 | Val Loss: 996085.88 | Val RMSE: 0.4973\n",
      "Epoch 82/100 | Train Loss: 7466658.97 | Val Loss: 993227.94 | Val RMSE: 0.3609\n",
      "Epoch 83/100 | Train Loss: 7439284.69 | Val Loss: 989362.72 | Val RMSE: 0.5663\n",
      "Epoch 84/100 | Train Loss: 7409371.91 | Val Loss: 986790.88 | Val RMSE: 0.6271\n",
      "Epoch 85/100 | Train Loss: 7397342.72 | Val Loss: 983928.38 | Val RMSE: 0.5510\n",
      "Epoch 86/100 | Train Loss: 7371071.72 | Val Loss: 980841.03 | Val RMSE: 0.4133\n",
      "Epoch 87/100 | Train Loss: 7348214.56 | Val Loss: 977766.94 | Val RMSE: 0.6072\n",
      "Epoch 88/100 | Train Loss: 7329436.91 | Val Loss: 974838.28 | Val RMSE: 0.5212\n",
      "Epoch 89/100 | Train Loss: 7306589.94 | Val Loss: 972542.03 | Val RMSE: 0.3883\n",
      "Epoch 90/100 | Train Loss: 7294222.19 | Val Loss: 969698.38 | Val RMSE: 0.4232\n",
      "Epoch 91/100 | Train Loss: 7271687.03 | Val Loss: 967306.25 | Val RMSE: 0.3794\n",
      "Epoch 92/100 | Train Loss: 7254919.03 | Val Loss: 965908.56 | Val RMSE: 0.3363\n",
      "Epoch 93/100 | Train Loss: 7239850.47 | Val Loss: 962972.66 | Val RMSE: 0.5269\n",
      "Epoch 94/100 | Train Loss: 7218077.91 | Val Loss: 961029.66 | Val RMSE: 0.3249\n",
      "Epoch 95/100 | Train Loss: 7208017.56 | Val Loss: 959093.81 | Val RMSE: 0.4767\n",
      "Epoch 96/100 | Train Loss: 7189009.47 | Val Loss: 956817.28 | Val RMSE: 0.3656\n",
      "Epoch 97/100 | Train Loss: 7173394.56 | Val Loss: 954266.47 | Val RMSE: 0.3174\n",
      "Epoch 98/100 | Train Loss: 7156637.56 | Val Loss: 953477.69 | Val RMSE: 0.3306\n",
      "Epoch 99/100 | Train Loss: 7142435.41 | Val Loss: 951430.19 | Val RMSE: 0.4889\n",
      "Epoch 100/100 | Train Loss: 7127207.31 | Val Loss: 949732.53 | Val RMSE: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 7127207.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 949732.53125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.31752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33munique-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/w37u9sdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091840-w37u9sdl/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i8gkdsfn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091905-i8gkdsfn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/i8gkdsfn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwarm-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/i8gkdsfn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091905-i8gkdsfn/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091906-i8gkdsfn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/i8gkdsfn\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 12594345.81 | Val Loss: 1674941.81 | Val RMSE: 3.4046\n",
      "Epoch 2/100 | Train Loss: 12552672.62 | Val Loss: 1670287.44 | Val RMSE: 4.1243\n",
      "Epoch 3/100 | Train Loss: 12518828.25 | Val Loss: 1666210.69 | Val RMSE: 4.0677\n",
      "Epoch 4/100 | Train Loss: 12487494.38 | Val Loss: 1661656.56 | Val RMSE: 3.1600\n",
      "Epoch 5/100 | Train Loss: 12459003.69 | Val Loss: 1657883.50 | Val RMSE: 2.7279\n",
      "Epoch 6/100 | Train Loss: 12428194.81 | Val Loss: 1654024.81 | Val RMSE: 2.1825\n",
      "Epoch 7/100 | Train Loss: 12398583.62 | Val Loss: 1649922.69 | Val RMSE: 1.3316\n",
      "Epoch 8/100 | Train Loss: 12370936.69 | Val Loss: 1645715.25 | Val RMSE: 1.2453\n",
      "Epoch 9/100 | Train Loss: 12340545.88 | Val Loss: 1642591.81 | Val RMSE: 0.7078\n",
      "Epoch 10/100 | Train Loss: 12308044.00 | Val Loss: 1637933.56 | Val RMSE: 0.5003\n",
      "Epoch 11/100 | Train Loss: 12281264.62 | Val Loss: 1634160.56 | Val RMSE: 0.7617\n",
      "Epoch 12/100 | Train Loss: 12253075.75 | Val Loss: 1630175.00 | Val RMSE: 1.0358\n",
      "Epoch 13/100 | Train Loss: 12219640.44 | Val Loss: 1626358.44 | Val RMSE: 0.7876\n",
      "Epoch 14/100 | Train Loss: 12196121.38 | Val Loss: 1622876.50 | Val RMSE: 0.5461\n",
      "Epoch 15/100 | Train Loss: 12162121.06 | Val Loss: 1618997.44 | Val RMSE: 0.6611\n",
      "Epoch 16/100 | Train Loss: 12133370.38 | Val Loss: 1614692.81 | Val RMSE: 0.6251\n",
      "Epoch 17/100 | Train Loss: 12100277.06 | Val Loss: 1610984.62 | Val RMSE: 0.5619\n",
      "Epoch 18/100 | Train Loss: 12077393.88 | Val Loss: 1606832.81 | Val RMSE: 0.7406\n",
      "Epoch 19/100 | Train Loss: 12048813.81 | Val Loss: 1603330.38 | Val RMSE: 0.4829\n",
      "Epoch 20/100 | Train Loss: 12024666.19 | Val Loss: 1600151.44 | Val RMSE: 0.5973\n",
      "Epoch 21/100 | Train Loss: 11993177.06 | Val Loss: 1596360.12 | Val RMSE: 0.6555\n",
      "Epoch 22/100 | Train Loss: 11969701.31 | Val Loss: 1593498.94 | Val RMSE: 0.5140\n",
      "Epoch 23/100 | Train Loss: 11942935.38 | Val Loss: 1589333.69 | Val RMSE: 0.4658\n",
      "Epoch 24/100 | Train Loss: 11917981.31 | Val Loss: 1585715.44 | Val RMSE: 0.5130\n",
      "Epoch 25/100 | Train Loss: 11887649.75 | Val Loss: 1582821.75 | Val RMSE: 0.5744\n",
      "Epoch 26/100 | Train Loss: 11864235.94 | Val Loss: 1579236.81 | Val RMSE: 0.6565\n",
      "Epoch 27/100 | Train Loss: 11842138.12 | Val Loss: 1576202.62 | Val RMSE: 0.6453\n",
      "Epoch 28/100 | Train Loss: 11817798.62 | Val Loss: 1572941.69 | Val RMSE: 0.4045\n",
      "Epoch 29/100 | Train Loss: 11790773.06 | Val Loss: 1570049.00 | Val RMSE: 0.6182\n",
      "Epoch 30/100 | Train Loss: 11770837.12 | Val Loss: 1566666.38 | Val RMSE: 0.4825\n",
      "Epoch 31/100 | Train Loss: 11746866.81 | Val Loss: 1563988.69 | Val RMSE: 0.7812\n",
      "Epoch 32/100 | Train Loss: 11720928.69 | Val Loss: 1559765.81 | Val RMSE: 0.4965\n",
      "Epoch 33/100 | Train Loss: 11700057.69 | Val Loss: 1557366.38 | Val RMSE: 0.9908\n",
      "Epoch 34/100 | Train Loss: 11679482.12 | Val Loss: 1554446.31 | Val RMSE: 0.5854\n",
      "Epoch 35/100 | Train Loss: 11652878.19 | Val Loss: 1551818.00 | Val RMSE: 0.7331\n",
      "Epoch 36/100 | Train Loss: 11632928.00 | Val Loss: 1548574.25 | Val RMSE: 0.6056\n",
      "Epoch 37/100 | Train Loss: 11614312.00 | Val Loss: 1546080.56 | Val RMSE: 0.3613\n",
      "Epoch 38/100 | Train Loss: 11587357.81 | Val Loss: 1543228.81 | Val RMSE: 1.6622\n",
      "Epoch 39/100 | Train Loss: 11571185.31 | Val Loss: 1540711.50 | Val RMSE: 0.5103\n",
      "Epoch 40/100 | Train Loss: 11548182.31 | Val Loss: 1538433.94 | Val RMSE: 1.3707\n",
      "Epoch 41/100 | Train Loss: 11530318.31 | Val Loss: 1535042.81 | Val RMSE: 0.9394\n",
      "Epoch 42/100 | Train Loss: 11509237.56 | Val Loss: 1532890.81 | Val RMSE: 0.4176\n",
      "Epoch 43/100 | Train Loss: 11491115.25 | Val Loss: 1530051.56 | Val RMSE: 0.5927\n",
      "Epoch 44/100 | Train Loss: 11471220.19 | Val Loss: 1526972.44 | Val RMSE: 0.3132\n",
      "Epoch 45/100 | Train Loss: 11453184.94 | Val Loss: 1525183.25 | Val RMSE: 0.5171\n",
      "Epoch 46/100 | Train Loss: 11436314.69 | Val Loss: 1522436.75 | Val RMSE: 0.3684\n",
      "Epoch 47/100 | Train Loss: 11413530.12 | Val Loss: 1519943.31 | Val RMSE: 0.4170\n",
      "Epoch 48/100 | Train Loss: 11396626.88 | Val Loss: 1517427.81 | Val RMSE: 0.6294\n",
      "Epoch 49/100 | Train Loss: 11381051.00 | Val Loss: 1515307.25 | Val RMSE: 1.7168\n",
      "Epoch 50/100 | Train Loss: 11360566.25 | Val Loss: 1513278.38 | Val RMSE: 0.4914\n",
      "Epoch 51/100 | Train Loss: 11344536.38 | Val Loss: 1510376.31 | Val RMSE: 0.9160\n",
      "Epoch 52/100 | Train Loss: 11329103.25 | Val Loss: 1508699.00 | Val RMSE: 0.9340\n",
      "Epoch 53/100 | Train Loss: 11313083.56 | Val Loss: 1505913.94 | Val RMSE: 0.7226\n",
      "Epoch 54/100 | Train Loss: 11293550.12 | Val Loss: 1503795.62 | Val RMSE: 0.4493\n",
      "Epoch 55/100 | Train Loss: 11277421.19 | Val Loss: 1501492.12 | Val RMSE: 0.4304\n",
      "Epoch 56/100 | Train Loss: 11259677.88 | Val Loss: 1499694.81 | Val RMSE: 0.4779\n",
      "Epoch 57/100 | Train Loss: 11244611.25 | Val Loss: 1496945.56 | Val RMSE: 0.5043\n",
      "Epoch 58/100 | Train Loss: 11227296.81 | Val Loss: 1495591.75 | Val RMSE: 0.3729\n",
      "Epoch 59/100 | Train Loss: 11213898.94 | Val Loss: 1492973.00 | Val RMSE: 0.4442\n",
      "Epoch 60/100 | Train Loss: 11192817.06 | Val Loss: 1491050.38 | Val RMSE: 0.3076\n",
      "Epoch 61/100 | Train Loss: 11175663.62 | Val Loss: 1489088.75 | Val RMSE: 0.4322\n",
      "Epoch 62/100 | Train Loss: 11165288.50 | Val Loss: 1486851.50 | Val RMSE: 0.3253\n",
      "Epoch 63/100 | Train Loss: 11147043.62 | Val Loss: 1485036.06 | Val RMSE: 0.3279\n",
      "Epoch 64/100 | Train Loss: 11136118.81 | Val Loss: 1482832.94 | Val RMSE: 0.3187\n",
      "Epoch 65/100 | Train Loss: 11120478.00 | Val Loss: 1481405.06 | Val RMSE: 0.3124\n",
      "Epoch 66/100 | Train Loss: 11109400.56 | Val Loss: 1479220.00 | Val RMSE: 0.3129\n",
      "Epoch 67/100 | Train Loss: 11089933.75 | Val Loss: 1477239.00 | Val RMSE: 0.3202\n",
      "Epoch 68/100 | Train Loss: 11076447.50 | Val Loss: 1475571.75 | Val RMSE: 0.5217\n",
      "Epoch 69/100 | Train Loss: 11066721.19 | Val Loss: 1474125.88 | Val RMSE: 0.6281\n",
      "Epoch 70/100 | Train Loss: 11052052.88 | Val Loss: 1472518.25 | Val RMSE: 0.7461\n",
      "Epoch 71/100 | Train Loss: 11036832.94 | Val Loss: 1469875.38 | Val RMSE: 0.9062\n",
      "Epoch 72/100 | Train Loss: 11023511.56 | Val Loss: 1468465.44 | Val RMSE: 0.6578\n",
      "Epoch 73/100 | Train Loss: 11013889.88 | Val Loss: 1466495.44 | Val RMSE: 0.5006\n",
      "Epoch 74/100 | Train Loss: 11000683.56 | Val Loss: 1464805.31 | Val RMSE: 0.4160\n",
      "Epoch 75/100 | Train Loss: 10987894.62 | Val Loss: 1464000.88 | Val RMSE: 0.3283\n",
      "Epoch 76/100 | Train Loss: 10971968.38 | Val Loss: 1462934.50 | Val RMSE: 0.3439\n",
      "Epoch 77/100 | Train Loss: 10962125.88 | Val Loss: 1460374.69 | Val RMSE: 0.3956\n",
      "Epoch 78/100 | Train Loss: 10955960.00 | Val Loss: 1460049.69 | Val RMSE: 0.3596\n",
      "Epoch 79/100 | Train Loss: 10946125.38 | Val Loss: 1457953.38 | Val RMSE: 0.5106\n",
      "Epoch 80/100 | Train Loss: 10933951.56 | Val Loss: 1456881.00 | Val RMSE: 0.3272\n",
      "Epoch 81/100 | Train Loss: 10924145.31 | Val Loss: 1455655.62 | Val RMSE: 0.3832\n",
      "Epoch 82/100 | Train Loss: 10919146.44 | Val Loss: 1454585.81 | Val RMSE: 0.3135\n",
      "Epoch 83/100 | Train Loss: 10905310.88 | Val Loss: 1453639.56 | Val RMSE: 0.2887\n",
      "Epoch 84/100 | Train Loss: 10897886.44 | Val Loss: 1452924.56 | Val RMSE: 0.3632\n",
      "Epoch 85/100 | Train Loss: 10891213.06 | Val Loss: 1451366.56 | Val RMSE: 0.3242\n",
      "Epoch 86/100 | Train Loss: 10881994.00 | Val Loss: 1450491.06 | Val RMSE: 0.3556\n",
      "Epoch 87/100 | Train Loss: 10875026.88 | Val Loss: 1449980.56 | Val RMSE: 0.4315\n",
      "Epoch 88/100 | Train Loss: 10872085.94 | Val Loss: 1449070.38 | Val RMSE: 0.3197\n",
      "Epoch 89/100 | Train Loss: 10867399.31 | Val Loss: 1448347.69 | Val RMSE: 0.2948\n",
      "Epoch 90/100 | Train Loss: 10863103.88 | Val Loss: 1448247.38 | Val RMSE: 0.2990\n",
      "Epoch 91/100 | Train Loss: 10858230.00 | Val Loss: 1447178.69 | Val RMSE: 0.2964\n",
      "Epoch 92/100 | Train Loss: 10853190.94 | Val Loss: 1446853.06 | Val RMSE: 0.5179\n",
      "Epoch 93/100 | Train Loss: 10854045.00 | Val Loss: 1446523.44 | Val RMSE: 0.4893\n",
      "Epoch 94/100 | Train Loss: 10848620.69 | Val Loss: 1446095.81 | Val RMSE: 0.3020\n",
      "Epoch 95/100 | Train Loss: 10843056.38 | Val Loss: 1445521.94 | Val RMSE: 0.3993\n",
      "Epoch 96/100 | Train Loss: 10840983.75 | Val Loss: 1445523.94 | Val RMSE: 0.4636\n",
      "Epoch 97/100 | Train Loss: 10836305.69 | Val Loss: 1444421.12 | Val RMSE: 0.3411\n",
      "Epoch 98/100 | Train Loss: 10833515.25 | Val Loss: 1444107.88 | Val RMSE: 0.3405\n",
      "Epoch 99/100 | Train Loss: 10834025.38 | Val Loss: 1443694.56 | Val RMSE: 0.3076\n",
      "Epoch 100/100 | Train Loss: 10827083.75 | Val Loss: 1443554.81 | Val RMSE: 0.4046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 10827083.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1443554.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.40458\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwarm-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/i8gkdsfn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091906-i8gkdsfn/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hfu3l93x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091931-hfu3l93x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33micy-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hfu3l93x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading summary, console lines 0-0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33micy-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hfu3l93x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091931-hfu3l93x/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091932-hfu3l93x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33micy-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hfu3l93x\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 13077320.69 | Val Loss: 1736073.69 | Val RMSE: 1.4643\n",
      "Epoch 2/100 | Train Loss: 13011703.06 | Val Loss: 1728800.69 | Val RMSE: 3.1297\n",
      "Epoch 3/100 | Train Loss: 12955409.25 | Val Loss: 1721682.38 | Val RMSE: 2.2543\n",
      "Epoch 4/100 | Train Loss: 12901240.94 | Val Loss: 1714570.06 | Val RMSE: 1.6758\n",
      "Epoch 5/100 | Train Loss: 12847597.81 | Val Loss: 1707765.62 | Val RMSE: 1.2332\n",
      "Epoch 6/100 | Train Loss: 12796854.38 | Val Loss: 1700374.25 | Val RMSE: 0.6002\n",
      "Epoch 7/100 | Train Loss: 12740854.44 | Val Loss: 1692659.94 | Val RMSE: 0.4420\n",
      "Epoch 8/100 | Train Loss: 12685615.12 | Val Loss: 1684901.38 | Val RMSE: 0.6465\n",
      "Epoch 9/100 | Train Loss: 12627045.62 | Val Loss: 1676920.50 | Val RMSE: 0.4130\n",
      "Epoch 10/100 | Train Loss: 12566652.25 | Val Loss: 1668651.19 | Val RMSE: 0.4113\n",
      "Epoch 11/100 | Train Loss: 12502250.94 | Val Loss: 1659813.56 | Val RMSE: 0.7535\n",
      "Epoch 12/100 | Train Loss: 12440332.50 | Val Loss: 1651700.38 | Val RMSE: 0.8377\n",
      "Epoch 13/100 | Train Loss: 12373156.06 | Val Loss: 1642826.62 | Val RMSE: 0.7100\n",
      "Epoch 14/100 | Train Loss: 12306126.88 | Val Loss: 1634529.38 | Val RMSE: 0.6546\n",
      "Epoch 15/100 | Train Loss: 12235807.12 | Val Loss: 1624567.44 | Val RMSE: 0.7995\n",
      "Epoch 16/100 | Train Loss: 12171188.19 | Val Loss: 1616043.31 | Val RMSE: 0.4119\n",
      "Epoch 17/100 | Train Loss: 12099910.44 | Val Loss: 1606845.75 | Val RMSE: 0.8443\n",
      "Epoch 18/100 | Train Loss: 12035900.50 | Val Loss: 1597510.56 | Val RMSE: 0.4250\n",
      "Epoch 19/100 | Train Loss: 11971213.12 | Val Loss: 1588786.31 | Val RMSE: 0.6106\n",
      "Epoch 20/100 | Train Loss: 11902921.62 | Val Loss: 1579967.69 | Val RMSE: 0.4015\n",
      "Epoch 21/100 | Train Loss: 11832447.25 | Val Loss: 1570869.38 | Val RMSE: 0.4304\n",
      "Epoch 22/100 | Train Loss: 11767432.69 | Val Loss: 1562801.12 | Val RMSE: 0.5442\n",
      "Epoch 23/100 | Train Loss: 11699228.06 | Val Loss: 1554492.06 | Val RMSE: 1.0272\n",
      "Epoch 24/100 | Train Loss: 11642147.81 | Val Loss: 1545324.25 | Val RMSE: 0.6815\n",
      "Epoch 25/100 | Train Loss: 11577408.81 | Val Loss: 1537326.06 | Val RMSE: 0.9713\n",
      "Epoch 26/100 | Train Loss: 11515441.81 | Val Loss: 1529188.75 | Val RMSE: 0.4822\n",
      "Epoch 27/100 | Train Loss: 11454379.00 | Val Loss: 1520312.25 | Val RMSE: 0.6868\n",
      "Epoch 28/100 | Train Loss: 11390725.25 | Val Loss: 1512944.88 | Val RMSE: 0.6092\n",
      "Epoch 29/100 | Train Loss: 11334488.69 | Val Loss: 1504551.50 | Val RMSE: 0.7069\n",
      "Epoch 30/100 | Train Loss: 11269499.38 | Val Loss: 1496064.25 | Val RMSE: 1.0179\n",
      "Epoch 31/100 | Train Loss: 11214007.06 | Val Loss: 1489188.19 | Val RMSE: 4.7349\n",
      "Epoch 32/100 | Train Loss: 11155259.31 | Val Loss: 1480839.94 | Val RMSE: 6.3999\n",
      "Epoch 33/100 | Train Loss: 11100055.56 | Val Loss: 1473855.06 | Val RMSE: 6.4790\n",
      "Epoch 34/100 | Train Loss: 11044217.56 | Val Loss: 1465925.50 | Val RMSE: 6.6190\n",
      "Epoch 35/100 | Train Loss: 10983081.38 | Val Loss: 1458522.06 | Val RMSE: 5.4150\n",
      "Epoch 36/100 | Train Loss: 10925126.31 | Val Loss: 1450380.44 | Val RMSE: 4.4447\n",
      "Epoch 37/100 | Train Loss: 10870887.44 | Val Loss: 1443762.06 | Val RMSE: 3.7392\n",
      "Epoch 38/100 | Train Loss: 10814423.69 | Val Loss: 1436428.88 | Val RMSE: 2.6427\n",
      "Epoch 39/100 | Train Loss: 10765116.12 | Val Loss: 1428630.94 | Val RMSE: 1.6197\n",
      "Epoch 40/100 | Train Loss: 10710771.31 | Val Loss: 1422567.06 | Val RMSE: 1.5187\n",
      "Epoch 41/100 | Train Loss: 10657502.75 | Val Loss: 1415530.69 | Val RMSE: 1.0024\n",
      "Epoch 42/100 | Train Loss: 10607261.75 | Val Loss: 1408436.81 | Val RMSE: 0.8728\n",
      "Epoch 43/100 | Train Loss: 10553446.25 | Val Loss: 1402313.38 | Val RMSE: 0.6993\n",
      "Epoch 44/100 | Train Loss: 10504989.81 | Val Loss: 1395577.19 | Val RMSE: 0.6451\n",
      "Epoch 45/100 | Train Loss: 10453001.06 | Val Loss: 1388824.81 | Val RMSE: 0.6981\n",
      "Epoch 46/100 | Train Loss: 10400649.38 | Val Loss: 1382414.88 | Val RMSE: 0.8053\n",
      "Epoch 47/100 | Train Loss: 10353017.75 | Val Loss: 1375665.94 | Val RMSE: 0.7031\n",
      "Epoch 48/100 | Train Loss: 10304926.19 | Val Loss: 1369040.25 | Val RMSE: 0.6640\n",
      "Epoch 49/100 | Train Loss: 10252922.38 | Val Loss: 1362545.50 | Val RMSE: 0.8894\n",
      "Epoch 50/100 | Train Loss: 10205444.56 | Val Loss: 1356521.12 | Val RMSE: 0.7899\n",
      "Epoch 51/100 | Train Loss: 10167370.56 | Val Loss: 1350054.38 | Val RMSE: 0.7820\n",
      "Epoch 52/100 | Train Loss: 10116924.75 | Val Loss: 1343698.50 | Val RMSE: 0.5227\n",
      "Epoch 53/100 | Train Loss: 10072595.69 | Val Loss: 1337438.81 | Val RMSE: 0.5561\n",
      "Epoch 54/100 | Train Loss: 10022017.75 | Val Loss: 1331771.31 | Val RMSE: 0.5660\n",
      "Epoch 55/100 | Train Loss: 9973389.31 | Val Loss: 1325155.56 | Val RMSE: 0.8005\n",
      "Epoch 56/100 | Train Loss: 9930617.31 | Val Loss: 1319119.31 | Val RMSE: 0.3921\n",
      "Epoch 57/100 | Train Loss: 9879861.00 | Val Loss: 1312615.62 | Val RMSE: 0.3897\n",
      "Epoch 58/100 | Train Loss: 9838734.88 | Val Loss: 1306711.56 | Val RMSE: 0.5284\n",
      "Epoch 59/100 | Train Loss: 9795598.19 | Val Loss: 1300859.62 | Val RMSE: 0.4617\n",
      "Epoch 60/100 | Train Loss: 9750164.38 | Val Loss: 1294948.38 | Val RMSE: 0.3646\n",
      "Epoch 61/100 | Train Loss: 9705010.00 | Val Loss: 1288627.00 | Val RMSE: 0.6647\n",
      "Epoch 62/100 | Train Loss: 9660020.12 | Val Loss: 1283042.50 | Val RMSE: 0.7080\n",
      "Epoch 63/100 | Train Loss: 9610596.44 | Val Loss: 1277669.81 | Val RMSE: 0.7086\n",
      "Epoch 64/100 | Train Loss: 9571307.06 | Val Loss: 1272128.50 | Val RMSE: 0.7690\n",
      "Epoch 65/100 | Train Loss: 9528613.81 | Val Loss: 1265660.69 | Val RMSE: 0.3998\n",
      "Epoch 66/100 | Train Loss: 9492137.94 | Val Loss: 1260385.44 | Val RMSE: 0.8736\n",
      "Epoch 67/100 | Train Loss: 9448247.88 | Val Loss: 1255252.88 | Val RMSE: 0.5427\n",
      "Epoch 68/100 | Train Loss: 9405329.62 | Val Loss: 1249119.69 | Val RMSE: 0.7738\n",
      "Epoch 69/100 | Train Loss: 9361170.19 | Val Loss: 1244554.56 | Val RMSE: 0.5607\n",
      "Epoch 70/100 | Train Loss: 9323571.69 | Val Loss: 1239016.56 | Val RMSE: 0.3903\n",
      "Epoch 71/100 | Train Loss: 9277790.81 | Val Loss: 1233559.50 | Val RMSE: 0.5669\n",
      "Epoch 72/100 | Train Loss: 9240721.56 | Val Loss: 1228421.38 | Val RMSE: 0.4102\n",
      "Epoch 73/100 | Train Loss: 9199466.62 | Val Loss: 1223678.81 | Val RMSE: 0.6614\n",
      "Epoch 74/100 | Train Loss: 9162144.69 | Val Loss: 1218252.81 | Val RMSE: 0.8812\n",
      "Epoch 75/100 | Train Loss: 9122969.44 | Val Loss: 1213555.06 | Val RMSE: 0.4867\n",
      "Epoch 76/100 | Train Loss: 9092985.69 | Val Loss: 1208297.44 | Val RMSE: 0.7649\n",
      "Epoch 77/100 | Train Loss: 9057735.94 | Val Loss: 1204340.19 | Val RMSE: 0.5394\n",
      "Epoch 78/100 | Train Loss: 9021586.12 | Val Loss: 1199858.62 | Val RMSE: 0.4295\n",
      "Epoch 79/100 | Train Loss: 8995910.06 | Val Loss: 1196392.69 | Val RMSE: 0.7239\n",
      "Epoch 80/100 | Train Loss: 8964647.81 | Val Loss: 1192066.56 | Val RMSE: 0.6109\n",
      "Epoch 81/100 | Train Loss: 8939308.00 | Val Loss: 1188650.06 | Val RMSE: 0.4560\n",
      "Epoch 82/100 | Train Loss: 8905516.75 | Val Loss: 1185611.44 | Val RMSE: 0.5057\n",
      "Epoch 83/100 | Train Loss: 8883018.44 | Val Loss: 1181333.56 | Val RMSE: 0.3594\n",
      "Epoch 84/100 | Train Loss: 8859918.25 | Val Loss: 1179668.00 | Val RMSE: 0.4289\n",
      "Epoch 85/100 | Train Loss: 8837236.62 | Val Loss: 1175991.56 | Val RMSE: 0.6788\n",
      "Epoch 86/100 | Train Loss: 8813940.50 | Val Loss: 1173681.38 | Val RMSE: 1.3358\n",
      "Epoch 87/100 | Train Loss: 8794600.19 | Val Loss: 1171128.69 | Val RMSE: 0.3993\n",
      "Epoch 88/100 | Train Loss: 8777968.56 | Val Loss: 1168753.94 | Val RMSE: 0.5996\n",
      "Epoch 89/100 | Train Loss: 8756191.00 | Val Loss: 1165617.88 | Val RMSE: 0.7568\n",
      "Epoch 90/100 | Train Loss: 8741063.12 | Val Loss: 1163597.75 | Val RMSE: 0.4384\n",
      "Epoch 91/100 | Train Loss: 8723344.12 | Val Loss: 1161511.62 | Val RMSE: 0.4789\n",
      "Epoch 92/100 | Train Loss: 8705981.44 | Val Loss: 1159276.56 | Val RMSE: 0.6983\n",
      "Epoch 93/100 | Train Loss: 8690308.19 | Val Loss: 1157360.19 | Val RMSE: 0.5296\n",
      "Epoch 94/100 | Train Loss: 8670154.62 | Val Loss: 1155031.50 | Val RMSE: 0.4984\n",
      "Epoch 95/100 | Train Loss: 8661091.06 | Val Loss: 1152456.19 | Val RMSE: 0.8424\n",
      "Epoch 96/100 | Train Loss: 8636610.75 | Val Loss: 1150505.94 | Val RMSE: 0.3817\n",
      "Epoch 97/100 | Train Loss: 8622767.69 | Val Loss: 1148302.25 | Val RMSE: 0.6550\n",
      "Epoch 98/100 | Train Loss: 8611366.50 | Val Loss: 1146213.81 | Val RMSE: 0.4876\n",
      "Epoch 99/100 | Train Loss: 8597096.75 | Val Loss: 1144600.06 | Val RMSE: 0.4828\n",
      "Epoch 100/100 | Train Loss: 8573448.00 | Val Loss: 1142481.38 | Val RMSE: 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 8573448.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1142481.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.3103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33micy-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/hfu3l93x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091932-hfu3l93x/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p8am1qk7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091957-p8am1qk7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrich-sweep-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/p8am1qk7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrich-sweep-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/p8am1qk7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091957-p8am1qk7/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_091958-p8am1qk7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrich-sweep-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/p8am1qk7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 13363156.38 | Val Loss: 1778302.44 | Val RMSE: 1.0356\n",
      "Epoch 2/100 | Train Loss: 13330088.00 | Val Loss: 1774810.94 | Val RMSE: 3.7628\n",
      "Epoch 3/100 | Train Loss: 13306204.31 | Val Loss: 1771272.44 | Val RMSE: 1.0325\n",
      "Epoch 4/100 | Train Loss: 13279790.81 | Val Loss: 1768401.94 | Val RMSE: 0.9962\n",
      "Epoch 5/100 | Train Loss: 13253268.12 | Val Loss: 1765011.25 | Val RMSE: 0.9025\n",
      "Epoch 6/100 | Train Loss: 13234703.44 | Val Loss: 1762079.69 | Val RMSE: 0.6198\n",
      "Epoch 7/100 | Train Loss: 13211265.06 | Val Loss: 1759262.31 | Val RMSE: 0.7517\n",
      "Epoch 8/100 | Train Loss: 13188828.50 | Val Loss: 1755974.62 | Val RMSE: 1.0147\n",
      "Epoch 9/100 | Train Loss: 13165530.50 | Val Loss: 1752825.94 | Val RMSE: 0.6824\n",
      "Epoch 10/100 | Train Loss: 13141661.44 | Val Loss: 1749485.00 | Val RMSE: 0.4695\n",
      "Epoch 11/100 | Train Loss: 13118101.44 | Val Loss: 1746140.12 | Val RMSE: 1.0898\n",
      "Epoch 12/100 | Train Loss: 13095583.00 | Val Loss: 1743695.88 | Val RMSE: 0.4785\n",
      "Epoch 13/100 | Train Loss: 13069676.50 | Val Loss: 1740483.94 | Val RMSE: 0.5491\n",
      "Epoch 14/100 | Train Loss: 13049306.44 | Val Loss: 1737805.75 | Val RMSE: 0.4801\n",
      "Epoch 15/100 | Train Loss: 13025362.38 | Val Loss: 1734394.62 | Val RMSE: 0.5870\n",
      "Epoch 16/100 | Train Loss: 13002787.88 | Val Loss: 1732147.19 | Val RMSE: 1.5731\n",
      "Epoch 17/100 | Train Loss: 12980896.12 | Val Loss: 1728042.56 | Val RMSE: 4.5548\n",
      "Epoch 18/100 | Train Loss: 12961940.31 | Val Loss: 1725028.50 | Val RMSE: 4.8312\n",
      "Epoch 19/100 | Train Loss: 12937376.25 | Val Loss: 1722818.94 | Val RMSE: 4.2340\n",
      "Epoch 20/100 | Train Loss: 12915424.06 | Val Loss: 1719906.56 | Val RMSE: 3.3542\n",
      "Epoch 21/100 | Train Loss: 12894604.81 | Val Loss: 1716846.44 | Val RMSE: 2.1961\n",
      "Epoch 22/100 | Train Loss: 12872715.50 | Val Loss: 1714089.19 | Val RMSE: 1.5440\n",
      "Epoch 23/100 | Train Loss: 12851077.88 | Val Loss: 1711251.62 | Val RMSE: 1.3846\n",
      "Epoch 24/100 | Train Loss: 12831661.06 | Val Loss: 1708701.94 | Val RMSE: 1.1352\n",
      "Epoch 25/100 | Train Loss: 12813547.62 | Val Loss: 1705933.94 | Val RMSE: 1.1141\n",
      "Epoch 26/100 | Train Loss: 12791526.06 | Val Loss: 1703704.25 | Val RMSE: 0.8237\n",
      "Epoch 27/100 | Train Loss: 12773992.12 | Val Loss: 1701231.12 | Val RMSE: 0.6362\n",
      "Epoch 28/100 | Train Loss: 12755228.31 | Val Loss: 1698397.44 | Val RMSE: 0.8338\n",
      "Epoch 29/100 | Train Loss: 12739966.81 | Val Loss: 1696533.56 | Val RMSE: 0.8190\n",
      "Epoch 30/100 | Train Loss: 12717640.69 | Val Loss: 1693462.75 | Val RMSE: 0.5071\n",
      "Epoch 31/100 | Train Loss: 12695926.81 | Val Loss: 1691341.81 | Val RMSE: 0.9948\n",
      "Epoch 32/100 | Train Loss: 12681235.44 | Val Loss: 1688568.56 | Val RMSE: 0.4859\n",
      "Epoch 33/100 | Train Loss: 12662399.31 | Val Loss: 1686394.44 | Val RMSE: 0.4624\n",
      "Epoch 34/100 | Train Loss: 12643493.75 | Val Loss: 1684582.25 | Val RMSE: 0.5739\n",
      "Epoch 35/100 | Train Loss: 12628517.31 | Val Loss: 1681846.25 | Val RMSE: 0.4241\n",
      "Epoch 36/100 | Train Loss: 12612516.00 | Val Loss: 1679815.62 | Val RMSE: 0.4603\n",
      "Epoch 37/100 | Train Loss: 12592828.81 | Val Loss: 1677545.00 | Val RMSE: 0.5041\n",
      "Epoch 38/100 | Train Loss: 12578034.69 | Val Loss: 1675034.19 | Val RMSE: 0.5169\n",
      "Epoch 39/100 | Train Loss: 12558247.25 | Val Loss: 1672739.62 | Val RMSE: 0.7322\n",
      "Epoch 40/100 | Train Loss: 12543132.94 | Val Loss: 1670208.56 | Val RMSE: 0.6097\n",
      "Epoch 41/100 | Train Loss: 12526274.94 | Val Loss: 1668715.12 | Val RMSE: 0.7021\n",
      "Epoch 42/100 | Train Loss: 12509223.38 | Val Loss: 1666472.25 | Val RMSE: 0.3861\n",
      "Epoch 43/100 | Train Loss: 12492003.69 | Val Loss: 1664240.75 | Val RMSE: 0.5963\n",
      "Epoch 44/100 | Train Loss: 12477696.00 | Val Loss: 1662185.75 | Val RMSE: 0.4195\n",
      "Epoch 45/100 | Train Loss: 12461856.38 | Val Loss: 1660287.19 | Val RMSE: 0.4317\n",
      "Epoch 46/100 | Train Loss: 12444846.62 | Val Loss: 1658355.12 | Val RMSE: 0.3536\n",
      "Epoch 47/100 | Train Loss: 12428480.25 | Val Loss: 1656241.19 | Val RMSE: 0.3895\n",
      "Epoch 48/100 | Train Loss: 12412607.06 | Val Loss: 1653302.56 | Val RMSE: 0.5929\n",
      "Epoch 49/100 | Train Loss: 12397358.06 | Val Loss: 1651149.44 | Val RMSE: 1.5955\n",
      "Epoch 50/100 | Train Loss: 12388039.88 | Val Loss: 1650258.19 | Val RMSE: 1.2771\n",
      "Epoch 51/100 | Train Loss: 12366132.38 | Val Loss: 1648219.62 | Val RMSE: 0.6824\n",
      "Epoch 52/100 | Train Loss: 12355882.44 | Val Loss: 1645742.69 | Val RMSE: 0.6746\n",
      "Epoch 53/100 | Train Loss: 12337908.75 | Val Loss: 1643856.69 | Val RMSE: 1.1908\n",
      "Epoch 54/100 | Train Loss: 12322893.38 | Val Loss: 1641959.94 | Val RMSE: 1.1880\n",
      "Epoch 55/100 | Train Loss: 12312277.56 | Val Loss: 1639429.75 | Val RMSE: 0.7817\n",
      "Epoch 56/100 | Train Loss: 12295649.69 | Val Loss: 1638036.75 | Val RMSE: 0.6420\n",
      "Epoch 57/100 | Train Loss: 12282364.88 | Val Loss: 1636566.56 | Val RMSE: 0.3346\n",
      "Epoch 58/100 | Train Loss: 12269548.19 | Val Loss: 1634811.75 | Val RMSE: 0.4000\n",
      "Epoch 59/100 | Train Loss: 12255757.94 | Val Loss: 1632265.81 | Val RMSE: 0.3979\n",
      "Epoch 60/100 | Train Loss: 12243344.38 | Val Loss: 1631178.19 | Val RMSE: 0.3633\n",
      "Epoch 61/100 | Train Loss: 12232339.56 | Val Loss: 1629529.31 | Val RMSE: 0.3581\n",
      "Epoch 62/100 | Train Loss: 12215855.88 | Val Loss: 1627839.56 | Val RMSE: 0.4985\n",
      "Epoch 63/100 | Train Loss: 12203444.44 | Val Loss: 1626097.00 | Val RMSE: 0.5972\n",
      "Epoch 64/100 | Train Loss: 12194283.94 | Val Loss: 1625008.12 | Val RMSE: 0.8098\n",
      "Epoch 65/100 | Train Loss: 12180866.38 | Val Loss: 1622940.56 | Val RMSE: 0.3705\n",
      "Epoch 66/100 | Train Loss: 12165410.88 | Val Loss: 1620916.31 | Val RMSE: 0.4747\n",
      "Epoch 67/100 | Train Loss: 12157408.25 | Val Loss: 1619519.12 | Val RMSE: 0.4435\n",
      "Epoch 68/100 | Train Loss: 12147038.38 | Val Loss: 1617238.19 | Val RMSE: 0.5861\n",
      "Epoch 69/100 | Train Loss: 12134654.75 | Val Loss: 1616533.56 | Val RMSE: 0.4348\n",
      "Epoch 70/100 | Train Loss: 12121773.81 | Val Loss: 1614807.62 | Val RMSE: 0.5463\n",
      "Epoch 71/100 | Train Loss: 12110523.00 | Val Loss: 1613573.62 | Val RMSE: 0.4508\n",
      "Epoch 72/100 | Train Loss: 12100307.06 | Val Loss: 1612132.81 | Val RMSE: 0.4060\n",
      "Epoch 73/100 | Train Loss: 12087971.75 | Val Loss: 1610949.31 | Val RMSE: 0.4720\n",
      "Epoch 74/100 | Train Loss: 12079102.00 | Val Loss: 1609441.25 | Val RMSE: 0.4455\n",
      "Epoch 75/100 | Train Loss: 12067661.06 | Val Loss: 1607689.50 | Val RMSE: 0.5002\n",
      "Epoch 76/100 | Train Loss: 12053552.94 | Val Loss: 1606895.25 | Val RMSE: 0.4341\n",
      "Epoch 77/100 | Train Loss: 12046005.88 | Val Loss: 1605556.75 | Val RMSE: 0.4694\n",
      "Epoch 78/100 | Train Loss: 12033064.75 | Val Loss: 1603801.00 | Val RMSE: 0.4552\n",
      "Epoch 79/100 | Train Loss: 12028905.31 | Val Loss: 1602897.75 | Val RMSE: 0.4985\n",
      "Epoch 80/100 | Train Loss: 12021592.38 | Val Loss: 1602101.00 | Val RMSE: 0.3901\n",
      "Epoch 81/100 | Train Loss: 12010260.12 | Val Loss: 1599959.69 | Val RMSE: 0.4538\n",
      "Epoch 82/100 | Train Loss: 12002489.94 | Val Loss: 1599496.69 | Val RMSE: 0.5504\n",
      "Epoch 83/100 | Train Loss: 11994648.56 | Val Loss: 1599062.94 | Val RMSE: 0.5117\n",
      "Epoch 84/100 | Train Loss: 11990773.44 | Val Loss: 1597175.56 | Val RMSE: 0.5113\n",
      "Epoch 85/100 | Train Loss: 11981249.44 | Val Loss: 1596166.19 | Val RMSE: 0.4601\n",
      "Epoch 86/100 | Train Loss: 11970804.62 | Val Loss: 1596039.12 | Val RMSE: 0.3926\n",
      "Epoch 87/100 | Train Loss: 11967148.19 | Val Loss: 1594530.88 | Val RMSE: 0.4713\n",
      "Epoch 88/100 | Train Loss: 11960916.44 | Val Loss: 1593549.44 | Val RMSE: 0.4245\n",
      "Epoch 89/100 | Train Loss: 11953819.50 | Val Loss: 1592667.25 | Val RMSE: 0.4939\n",
      "Epoch 90/100 | Train Loss: 11949424.31 | Val Loss: 1592751.25 | Val RMSE: 0.3863\n",
      "Epoch 91/100 | Train Loss: 11944579.00 | Val Loss: 1591924.19 | Val RMSE: 0.6489\n",
      "Epoch 92/100 | Train Loss: 11935438.19 | Val Loss: 1591134.12 | Val RMSE: 0.4349\n",
      "Epoch 93/100 | Train Loss: 11930810.62 | Val Loss: 1590217.50 | Val RMSE: 0.3945\n",
      "Epoch 94/100 | Train Loss: 11926100.19 | Val Loss: 1590246.69 | Val RMSE: 0.5415\n",
      "Epoch 95/100 | Train Loss: 11921961.31 | Val Loss: 1589411.50 | Val RMSE: 0.4450\n",
      "Epoch 96/100 | Train Loss: 11920930.50 | Val Loss: 1588048.31 | Val RMSE: 0.6025\n",
      "Epoch 97/100 | Train Loss: 11915083.25 | Val Loss: 1587511.81 | Val RMSE: 1.0566\n",
      "Epoch 98/100 | Train Loss: 11908828.19 | Val Loss: 1587532.62 | Val RMSE: 1.9353\n",
      "Epoch 99/100 | Train Loss: 11907335.81 | Val Loss: 1586397.12 | Val RMSE: 1.4856\n",
      "Epoch 100/100 | Train Loss: 11904636.38 | Val Loss: 1586333.62 | Val RMSE: 0.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 11904636.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1586333.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.98315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrich-sweep-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/p8am1qk7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_091958-p8am1qk7/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ah5b2lr4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092023-ah5b2lr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlaced-sweep-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ah5b2lr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlaced-sweep-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ah5b2lr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092023-ah5b2lr4/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092024-ah5b2lr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlaced-sweep-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ah5b2lr4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 10430865.31 | Val Loss: 1383132.00 | Val RMSE: 0.4645\n",
      "Epoch 2/100 | Train Loss: 10364387.88 | Val Loss: 1376010.62 | Val RMSE: 1.2189\n",
      "Epoch 3/100 | Train Loss: 10308663.50 | Val Loss: 1368991.19 | Val RMSE: 0.3012\n",
      "Epoch 4/100 | Train Loss: 10259222.25 | Val Loss: 1362335.62 | Val RMSE: 0.5206\n",
      "Epoch 5/100 | Train Loss: 10206975.62 | Val Loss: 1355183.38 | Val RMSE: 0.7884\n",
      "Epoch 6/100 | Train Loss: 10154637.31 | Val Loss: 1348378.38 | Val RMSE: 0.4353\n",
      "Epoch 7/100 | Train Loss: 10101289.06 | Val Loss: 1340821.50 | Val RMSE: 0.4336\n",
      "Epoch 8/100 | Train Loss: 10046068.12 | Val Loss: 1333567.19 | Val RMSE: 0.7850\n",
      "Epoch 9/100 | Train Loss: 9989925.31 | Val Loss: 1326002.25 | Val RMSE: 0.9329\n",
      "Epoch 10/100 | Train Loss: 9931971.62 | Val Loss: 1317765.56 | Val RMSE: 0.4598\n",
      "Epoch 11/100 | Train Loss: 9872646.50 | Val Loss: 1309825.31 | Val RMSE: 0.3896\n",
      "Epoch 12/100 | Train Loss: 9809175.25 | Val Loss: 1301534.88 | Val RMSE: 0.7121\n",
      "Epoch 13/100 | Train Loss: 9746044.94 | Val Loss: 1292895.88 | Val RMSE: 0.3269\n",
      "Epoch 14/100 | Train Loss: 9685824.50 | Val Loss: 1284073.56 | Val RMSE: 0.9549\n",
      "Epoch 15/100 | Train Loss: 9622953.94 | Val Loss: 1275901.06 | Val RMSE: 1.9159\n",
      "Epoch 16/100 | Train Loss: 9557684.88 | Val Loss: 1267234.31 | Val RMSE: 1.7297\n",
      "Epoch 17/100 | Train Loss: 9494932.56 | Val Loss: 1258608.62 | Val RMSE: 0.5973\n",
      "Epoch 18/100 | Train Loss: 9431089.75 | Val Loss: 1250315.00 | Val RMSE: 0.9255\n",
      "Epoch 19/100 | Train Loss: 9371241.44 | Val Loss: 1242237.69 | Val RMSE: 1.0701\n",
      "Epoch 20/100 | Train Loss: 9303804.62 | Val Loss: 1233528.31 | Val RMSE: 0.4512\n",
      "Epoch 21/100 | Train Loss: 9242333.19 | Val Loss: 1225012.94 | Val RMSE: 0.3213\n",
      "Epoch 22/100 | Train Loss: 9179830.94 | Val Loss: 1217042.19 | Val RMSE: 0.4155\n",
      "Epoch 23/100 | Train Loss: 9117239.56 | Val Loss: 1209247.81 | Val RMSE: 0.3636\n",
      "Epoch 24/100 | Train Loss: 9057949.44 | Val Loss: 1201036.06 | Val RMSE: 0.5193\n",
      "Epoch 25/100 | Train Loss: 8995278.25 | Val Loss: 1193483.88 | Val RMSE: 0.4460\n",
      "Epoch 26/100 | Train Loss: 8936803.38 | Val Loss: 1186006.44 | Val RMSE: 1.3384\n",
      "Epoch 27/100 | Train Loss: 8881913.88 | Val Loss: 1178277.12 | Val RMSE: 1.5106\n",
      "Epoch 28/100 | Train Loss: 8818698.50 | Val Loss: 1170219.25 | Val RMSE: 1.5640\n",
      "Epoch 29/100 | Train Loss: 8766689.00 | Val Loss: 1162257.81 | Val RMSE: 1.1262\n",
      "Epoch 30/100 | Train Loss: 8710483.44 | Val Loss: 1155149.88 | Val RMSE: 0.7932\n",
      "Epoch 31/100 | Train Loss: 8654046.62 | Val Loss: 1147857.94 | Val RMSE: 0.7755\n",
      "Epoch 32/100 | Train Loss: 8600405.12 | Val Loss: 1140169.75 | Val RMSE: 0.5496\n",
      "Epoch 33/100 | Train Loss: 8544074.12 | Val Loss: 1133896.88 | Val RMSE: 0.5557\n",
      "Epoch 34/100 | Train Loss: 8491031.38 | Val Loss: 1126924.38 | Val RMSE: 0.5056\n",
      "Epoch 35/100 | Train Loss: 8438108.06 | Val Loss: 1119688.69 | Val RMSE: 0.5141\n",
      "Epoch 36/100 | Train Loss: 8386182.94 | Val Loss: 1112556.81 | Val RMSE: 0.4410\n",
      "Epoch 37/100 | Train Loss: 8333433.25 | Val Loss: 1105995.31 | Val RMSE: 0.3704\n",
      "Epoch 38/100 | Train Loss: 8283159.50 | Val Loss: 1098165.88 | Val RMSE: 0.4456\n",
      "Epoch 39/100 | Train Loss: 8229256.75 | Val Loss: 1091745.12 | Val RMSE: 0.5133\n",
      "Epoch 40/100 | Train Loss: 8178223.06 | Val Loss: 1084819.31 | Val RMSE: 0.5337\n",
      "Epoch 41/100 | Train Loss: 8127483.44 | Val Loss: 1078614.62 | Val RMSE: 0.4358\n",
      "Epoch 42/100 | Train Loss: 8076298.62 | Val Loss: 1071969.94 | Val RMSE: 0.4446\n",
      "Epoch 43/100 | Train Loss: 8028677.81 | Val Loss: 1064994.06 | Val RMSE: 0.3426\n",
      "Epoch 44/100 | Train Loss: 7981535.19 | Val Loss: 1059741.44 | Val RMSE: 0.4566\n",
      "Epoch 45/100 | Train Loss: 7935704.19 | Val Loss: 1052906.44 | Val RMSE: 0.3572\n",
      "Epoch 46/100 | Train Loss: 7881198.84 | Val Loss: 1045717.72 | Val RMSE: 0.3939\n",
      "Epoch 47/100 | Train Loss: 7839815.75 | Val Loss: 1040286.38 | Val RMSE: 0.4602\n",
      "Epoch 48/100 | Train Loss: 7788972.50 | Val Loss: 1033808.34 | Val RMSE: 0.3971\n",
      "Epoch 49/100 | Train Loss: 7740287.75 | Val Loss: 1027141.22 | Val RMSE: 0.6145\n",
      "Epoch 50/100 | Train Loss: 7697259.97 | Val Loss: 1021566.22 | Val RMSE: 0.3879\n",
      "Epoch 51/100 | Train Loss: 7649309.41 | Val Loss: 1015365.81 | Val RMSE: 0.3451\n",
      "Epoch 52/100 | Train Loss: 7604876.84 | Val Loss: 1010007.53 | Val RMSE: 0.4050\n",
      "Epoch 53/100 | Train Loss: 7558548.03 | Val Loss: 1003695.69 | Val RMSE: 0.3633\n",
      "Epoch 54/100 | Train Loss: 7511587.81 | Val Loss: 997493.06 | Val RMSE: 0.4991\n",
      "Epoch 55/100 | Train Loss: 7471772.50 | Val Loss: 991653.72 | Val RMSE: 0.3135\n",
      "Epoch 56/100 | Train Loss: 7432433.09 | Val Loss: 986154.84 | Val RMSE: 0.3958\n",
      "Epoch 57/100 | Train Loss: 7384995.72 | Val Loss: 979728.94 | Val RMSE: 0.5190\n",
      "Epoch 58/100 | Train Loss: 7339751.72 | Val Loss: 974192.69 | Val RMSE: 0.3292\n",
      "Epoch 59/100 | Train Loss: 7296000.81 | Val Loss: 968424.72 | Val RMSE: 0.3081\n",
      "Epoch 60/100 | Train Loss: 7253702.41 | Val Loss: 962580.47 | Val RMSE: 0.5778\n",
      "Epoch 61/100 | Train Loss: 7212932.34 | Val Loss: 956844.94 | Val RMSE: 0.3282\n",
      "Epoch 62/100 | Train Loss: 7170117.00 | Val Loss: 951702.16 | Val RMSE: 0.3305\n",
      "Epoch 63/100 | Train Loss: 7127945.19 | Val Loss: 946294.41 | Val RMSE: 0.5936\n",
      "Epoch 64/100 | Train Loss: 7089416.84 | Val Loss: 941042.81 | Val RMSE: 0.5317\n",
      "Epoch 65/100 | Train Loss: 7044723.81 | Val Loss: 935501.34 | Val RMSE: 0.5206\n",
      "Epoch 66/100 | Train Loss: 7003864.25 | Val Loss: 930042.59 | Val RMSE: 0.4167\n",
      "Epoch 67/100 | Train Loss: 6967862.72 | Val Loss: 924961.31 | Val RMSE: 0.3329\n",
      "Epoch 68/100 | Train Loss: 6928738.75 | Val Loss: 919907.81 | Val RMSE: 0.5335\n",
      "Epoch 69/100 | Train Loss: 6888334.97 | Val Loss: 914463.22 | Val RMSE: 0.3236\n",
      "Epoch 70/100 | Train Loss: 6849105.28 | Val Loss: 909788.72 | Val RMSE: 0.3770\n",
      "Epoch 71/100 | Train Loss: 6813117.59 | Val Loss: 905313.84 | Val RMSE: 0.3809\n",
      "Epoch 72/100 | Train Loss: 6780655.56 | Val Loss: 900413.97 | Val RMSE: 0.3340\n",
      "Epoch 73/100 | Train Loss: 6739491.00 | Val Loss: 896154.03 | Val RMSE: 0.3241\n",
      "Epoch 74/100 | Train Loss: 6712483.97 | Val Loss: 891051.50 | Val RMSE: 0.3240\n",
      "Epoch 75/100 | Train Loss: 6678382.94 | Val Loss: 887294.50 | Val RMSE: 0.3138\n",
      "Epoch 76/100 | Train Loss: 6647373.59 | Val Loss: 882517.50 | Val RMSE: 0.3254\n",
      "Epoch 77/100 | Train Loss: 6612212.28 | Val Loss: 878514.38 | Val RMSE: 0.3680\n",
      "Epoch 78/100 | Train Loss: 6590024.34 | Val Loss: 874946.94 | Val RMSE: 0.3165\n",
      "Epoch 79/100 | Train Loss: 6556983.28 | Val Loss: 871105.53 | Val RMSE: 0.4684\n",
      "Epoch 80/100 | Train Loss: 6528196.69 | Val Loss: 868188.56 | Val RMSE: 0.4847\n",
      "Epoch 81/100 | Train Loss: 6505316.75 | Val Loss: 864295.16 | Val RMSE: 0.3936\n",
      "Epoch 82/100 | Train Loss: 6482353.47 | Val Loss: 861623.41 | Val RMSE: 0.3529\n",
      "Epoch 83/100 | Train Loss: 6458482.34 | Val Loss: 859036.47 | Val RMSE: 0.3124\n",
      "Epoch 84/100 | Train Loss: 6435335.19 | Val Loss: 857154.34 | Val RMSE: 0.4378\n",
      "Epoch 85/100 | Train Loss: 6418872.84 | Val Loss: 854556.59 | Val RMSE: 0.3204\n",
      "Epoch 86/100 | Train Loss: 6404302.62 | Val Loss: 852714.12 | Val RMSE: 0.3602\n",
      "Epoch 87/100 | Train Loss: 6389570.88 | Val Loss: 851783.78 | Val RMSE: 0.4213\n",
      "Epoch 88/100 | Train Loss: 6374796.81 | Val Loss: 848908.84 | Val RMSE: 0.4477\n",
      "Epoch 89/100 | Train Loss: 6358494.56 | Val Loss: 846413.16 | Val RMSE: 0.4329\n",
      "Epoch 90/100 | Train Loss: 6350403.62 | Val Loss: 845361.09 | Val RMSE: 0.4360\n",
      "Epoch 91/100 | Train Loss: 6337079.72 | Val Loss: 843789.75 | Val RMSE: 0.3417\n",
      "Epoch 92/100 | Train Loss: 6325333.72 | Val Loss: 842128.84 | Val RMSE: 0.4368\n",
      "Epoch 93/100 | Train Loss: 6309850.94 | Val Loss: 840605.06 | Val RMSE: 0.3332\n",
      "Epoch 94/100 | Train Loss: 6296944.81 | Val Loss: 838464.34 | Val RMSE: 0.3635\n",
      "Epoch 95/100 | Train Loss: 6290253.75 | Val Loss: 837967.81 | Val RMSE: 0.3693\n",
      "Epoch 96/100 | Train Loss: 6273454.16 | Val Loss: 836310.91 | Val RMSE: 0.4051\n",
      "Epoch 97/100 | Train Loss: 6263317.31 | Val Loss: 834975.91 | Val RMSE: 0.3326\n",
      "Epoch 98/100 | Train Loss: 6254945.09 | Val Loss: 833610.88 | Val RMSE: 0.3301\n",
      "Epoch 99/100 | Train Loss: 6243242.06 | Val Loss: 832680.72 | Val RMSE: 0.3154\n",
      "Epoch 100/100 | Train Loss: 6237448.69 | Val Loss: 830958.53 | Val RMSE: 0.3749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 6237448.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 830958.53125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.37492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlaced-sweep-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/ah5b2lr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092024-ah5b2lr4/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fpjktnkk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092053-fpjktnkk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-sweep-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/fpjktnkk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33meffortless-sweep-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/fpjktnkk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092053-fpjktnkk/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092055-fpjktnkk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-sweep-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/fpjktnkk\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 10643045.94 | Val Loss: 1414634.00 | Val RMSE: 1.5240\n",
      "Epoch 2/100 | Train Loss: 10605636.31 | Val Loss: 1410729.88 | Val RMSE: 0.8830\n",
      "Epoch 3/100 | Train Loss: 10571840.56 | Val Loss: 1406578.31 | Val RMSE: 1.1969\n",
      "Epoch 4/100 | Train Loss: 10542664.44 | Val Loss: 1402323.50 | Val RMSE: 0.6243\n",
      "Epoch 5/100 | Train Loss: 10513726.25 | Val Loss: 1398818.00 | Val RMSE: 0.3973\n",
      "Epoch 6/100 | Train Loss: 10487126.50 | Val Loss: 1395122.31 | Val RMSE: 0.5429\n",
      "Epoch 7/100 | Train Loss: 10457505.00 | Val Loss: 1391813.56 | Val RMSE: 0.4888\n",
      "Epoch 8/100 | Train Loss: 10432113.94 | Val Loss: 1387839.50 | Val RMSE: 0.4543\n",
      "Epoch 9/100 | Train Loss: 10404223.38 | Val Loss: 1384294.06 | Val RMSE: 0.4085\n",
      "Epoch 10/100 | Train Loss: 10377631.69 | Val Loss: 1380919.62 | Val RMSE: 0.4009\n",
      "Epoch 11/100 | Train Loss: 10349829.19 | Val Loss: 1377832.62 | Val RMSE: 0.9088\n",
      "Epoch 12/100 | Train Loss: 10327578.31 | Val Loss: 1374166.75 | Val RMSE: 1.4074\n",
      "Epoch 13/100 | Train Loss: 10300893.56 | Val Loss: 1370849.06 | Val RMSE: 0.7484\n",
      "Epoch 14/100 | Train Loss: 10277277.38 | Val Loss: 1367699.00 | Val RMSE: 0.6510\n",
      "Epoch 15/100 | Train Loss: 10251093.94 | Val Loss: 1364106.31 | Val RMSE: 0.6469\n",
      "Epoch 16/100 | Train Loss: 10227932.75 | Val Loss: 1361017.88 | Val RMSE: 0.5017\n",
      "Epoch 17/100 | Train Loss: 10197756.38 | Val Loss: 1357842.56 | Val RMSE: 0.4122\n",
      "Epoch 18/100 | Train Loss: 10178193.25 | Val Loss: 1354721.50 | Val RMSE: 0.3924\n",
      "Epoch 19/100 | Train Loss: 10153224.62 | Val Loss: 1351352.00 | Val RMSE: 0.4932\n",
      "Epoch 20/100 | Train Loss: 10131164.81 | Val Loss: 1347797.00 | Val RMSE: 0.3730\n",
      "Epoch 21/100 | Train Loss: 10106710.69 | Val Loss: 1345198.75 | Val RMSE: 0.4975\n",
      "Epoch 22/100 | Train Loss: 10085621.62 | Val Loss: 1342200.06 | Val RMSE: 0.3773\n",
      "Epoch 23/100 | Train Loss: 10060866.38 | Val Loss: 1339503.81 | Val RMSE: 0.5829\n",
      "Epoch 24/100 | Train Loss: 10039306.62 | Val Loss: 1336533.44 | Val RMSE: 0.2990\n",
      "Epoch 25/100 | Train Loss: 10019881.00 | Val Loss: 1333526.31 | Val RMSE: 0.3962\n",
      "Epoch 26/100 | Train Loss: 9999794.75 | Val Loss: 1330592.69 | Val RMSE: 0.4468\n",
      "Epoch 27/100 | Train Loss: 9977753.62 | Val Loss: 1327876.19 | Val RMSE: 0.4533\n",
      "Epoch 28/100 | Train Loss: 9952809.44 | Val Loss: 1324435.06 | Val RMSE: 0.3875\n",
      "Epoch 29/100 | Train Loss: 9935137.50 | Val Loss: 1322322.50 | Val RMSE: 0.3279\n",
      "Epoch 30/100 | Train Loss: 9916124.50 | Val Loss: 1320103.00 | Val RMSE: 1.6916\n",
      "Epoch 31/100 | Train Loss: 9892859.06 | Val Loss: 1316895.69 | Val RMSE: 1.1174\n",
      "Epoch 32/100 | Train Loss: 9879035.88 | Val Loss: 1314693.12 | Val RMSE: 0.8391\n",
      "Epoch 33/100 | Train Loss: 9858578.19 | Val Loss: 1312273.12 | Val RMSE: 0.8352\n",
      "Epoch 34/100 | Train Loss: 9836366.19 | Val Loss: 1309721.12 | Val RMSE: 0.6659\n",
      "Epoch 35/100 | Train Loss: 9821533.94 | Val Loss: 1307110.75 | Val RMSE: 0.4680\n",
      "Epoch 36/100 | Train Loss: 9802456.19 | Val Loss: 1305188.44 | Val RMSE: 0.4065\n",
      "Epoch 37/100 | Train Loss: 9784342.62 | Val Loss: 1302711.75 | Val RMSE: 0.3787\n",
      "Epoch 38/100 | Train Loss: 9766263.69 | Val Loss: 1300749.62 | Val RMSE: 0.3505\n",
      "Epoch 39/100 | Train Loss: 9751569.81 | Val Loss: 1298317.81 | Val RMSE: 0.3807\n",
      "Epoch 40/100 | Train Loss: 9734023.19 | Val Loss: 1295944.75 | Val RMSE: 0.5038\n",
      "Epoch 41/100 | Train Loss: 9714409.19 | Val Loss: 1293741.75 | Val RMSE: 0.3436\n",
      "Epoch 42/100 | Train Loss: 9697119.81 | Val Loss: 1291768.69 | Val RMSE: 0.3659\n",
      "Epoch 43/100 | Train Loss: 9686642.69 | Val Loss: 1289652.62 | Val RMSE: 0.4112\n",
      "Epoch 44/100 | Train Loss: 9670200.06 | Val Loss: 1287383.44 | Val RMSE: 0.3472\n",
      "Epoch 45/100 | Train Loss: 9651524.44 | Val Loss: 1285363.69 | Val RMSE: 0.3689\n",
      "Epoch 46/100 | Train Loss: 9638360.62 | Val Loss: 1283195.56 | Val RMSE: 0.3397\n",
      "Epoch 47/100 | Train Loss: 9621522.50 | Val Loss: 1281302.56 | Val RMSE: 0.3769\n",
      "Epoch 48/100 | Train Loss: 9604409.00 | Val Loss: 1279220.62 | Val RMSE: 0.5086\n",
      "Epoch 49/100 | Train Loss: 9590757.12 | Val Loss: 1277087.94 | Val RMSE: 0.4011\n",
      "Epoch 50/100 | Train Loss: 9573911.06 | Val Loss: 1275498.56 | Val RMSE: 0.3530\n",
      "Epoch 51/100 | Train Loss: 9560210.50 | Val Loss: 1273226.50 | Val RMSE: 0.3555\n",
      "Epoch 52/100 | Train Loss: 9548768.06 | Val Loss: 1271248.69 | Val RMSE: 0.3333\n",
      "Epoch 53/100 | Train Loss: 9530803.19 | Val Loss: 1269220.75 | Val RMSE: 0.3518\n",
      "Epoch 54/100 | Train Loss: 9516086.38 | Val Loss: 1266721.88 | Val RMSE: 0.3338\n",
      "Epoch 55/100 | Train Loss: 9502155.00 | Val Loss: 1265253.38 | Val RMSE: 0.7553\n",
      "Epoch 56/100 | Train Loss: 9492040.19 | Val Loss: 1263647.19 | Val RMSE: 0.4397\n",
      "Epoch 57/100 | Train Loss: 9476102.75 | Val Loss: 1261943.56 | Val RMSE: 0.6782\n",
      "Epoch 58/100 | Train Loss: 9458473.25 | Val Loss: 1260214.12 | Val RMSE: 0.5644\n",
      "Epoch 59/100 | Train Loss: 9448161.88 | Val Loss: 1258517.06 | Val RMSE: 0.4393\n",
      "Epoch 60/100 | Train Loss: 9436713.69 | Val Loss: 1256339.50 | Val RMSE: 0.4719\n",
      "Epoch 61/100 | Train Loss: 9426189.31 | Val Loss: 1254881.94 | Val RMSE: 0.4611\n",
      "Epoch 62/100 | Train Loss: 9410668.50 | Val Loss: 1253076.50 | Val RMSE: 0.3403\n",
      "Epoch 63/100 | Train Loss: 9396700.06 | Val Loss: 1251452.75 | Val RMSE: 0.5675\n",
      "Epoch 64/100 | Train Loss: 9384308.31 | Val Loss: 1249895.88 | Val RMSE: 0.3246\n",
      "Epoch 65/100 | Train Loss: 9374292.31 | Val Loss: 1248563.69 | Val RMSE: 0.3732\n",
      "Epoch 66/100 | Train Loss: 9363793.81 | Val Loss: 1246928.06 | Val RMSE: 0.3758\n",
      "Epoch 67/100 | Train Loss: 9350447.19 | Val Loss: 1245977.62 | Val RMSE: 0.3236\n",
      "Epoch 68/100 | Train Loss: 9337471.69 | Val Loss: 1243773.75 | Val RMSE: 0.3815\n",
      "Epoch 69/100 | Train Loss: 9325693.44 | Val Loss: 1242683.56 | Val RMSE: 0.3891\n",
      "Epoch 70/100 | Train Loss: 9312218.62 | Val Loss: 1240640.88 | Val RMSE: 0.3184\n",
      "Epoch 71/100 | Train Loss: 9303000.44 | Val Loss: 1239644.69 | Val RMSE: 0.4153\n",
      "Epoch 72/100 | Train Loss: 9295349.38 | Val Loss: 1238163.25 | Val RMSE: 0.3567\n",
      "Epoch 73/100 | Train Loss: 9284730.19 | Val Loss: 1237314.69 | Val RMSE: 0.3822\n",
      "Epoch 74/100 | Train Loss: 9277004.69 | Val Loss: 1235733.00 | Val RMSE: 0.3286\n",
      "Epoch 75/100 | Train Loss: 9266708.06 | Val Loss: 1234547.31 | Val RMSE: 0.4564\n",
      "Epoch 76/100 | Train Loss: 9255709.12 | Val Loss: 1233185.62 | Val RMSE: 0.6086\n",
      "Epoch 77/100 | Train Loss: 9245691.56 | Val Loss: 1231832.12 | Val RMSE: 0.5611\n",
      "Epoch 78/100 | Train Loss: 9239013.62 | Val Loss: 1230739.75 | Val RMSE: 0.3523\n",
      "Epoch 79/100 | Train Loss: 9232239.06 | Val Loss: 1230191.19 | Val RMSE: 0.5424\n",
      "Epoch 80/100 | Train Loss: 9220700.19 | Val Loss: 1228228.00 | Val RMSE: 0.3351\n",
      "Epoch 81/100 | Train Loss: 9212082.94 | Val Loss: 1227598.44 | Val RMSE: 0.5902\n",
      "Epoch 82/100 | Train Loss: 9209907.12 | Val Loss: 1227431.62 | Val RMSE: 0.3493\n",
      "Epoch 83/100 | Train Loss: 9197775.31 | Val Loss: 1224920.31 | Val RMSE: 0.3547\n",
      "Epoch 84/100 | Train Loss: 9193112.81 | Val Loss: 1224764.62 | Val RMSE: 0.3239\n",
      "Epoch 85/100 | Train Loss: 9185316.06 | Val Loss: 1224084.62 | Val RMSE: 0.4006\n",
      "Epoch 86/100 | Train Loss: 9178927.81 | Val Loss: 1223702.25 | Val RMSE: 0.3669\n",
      "Epoch 87/100 | Train Loss: 9175157.31 | Val Loss: 1222852.19 | Val RMSE: 0.5857\n",
      "Epoch 88/100 | Train Loss: 9165555.00 | Val Loss: 1221876.94 | Val RMSE: 0.3632\n",
      "Epoch 89/100 | Train Loss: 9167179.50 | Val Loss: 1221719.88 | Val RMSE: 0.3331\n",
      "Epoch 90/100 | Train Loss: 9160107.25 | Val Loss: 1221206.75 | Val RMSE: 0.4365\n",
      "Epoch 91/100 | Train Loss: 9161345.69 | Val Loss: 1220581.38 | Val RMSE: 0.5246\n",
      "Epoch 92/100 | Train Loss: 9151746.44 | Val Loss: 1220389.12 | Val RMSE: 0.3311\n",
      "Epoch 93/100 | Train Loss: 9149831.06 | Val Loss: 1219833.62 | Val RMSE: 0.3940\n",
      "Epoch 94/100 | Train Loss: 9152859.31 | Val Loss: 1219999.00 | Val RMSE: 0.5300\n",
      "Epoch 95/100 | Train Loss: 9148420.00 | Val Loss: 1219677.69 | Val RMSE: 0.3841\n",
      "Epoch 96/100 | Train Loss: 9145163.12 | Val Loss: 1219046.00 | Val RMSE: 0.3166\n",
      "Epoch 97/100 | Train Loss: 9143348.25 | Val Loss: 1218987.88 | Val RMSE: 0.3281\n",
      "Epoch 98/100 | Train Loss: 9140826.12 | Val Loss: 1218244.88 | Val RMSE: 0.3328\n",
      "Epoch 99/100 | Train Loss: 9138813.50 | Val Loss: 1218134.62 | Val RMSE: 0.3722\n",
      "Epoch 100/100 | Train Loss: 9134506.88 | Val Loss: 1217865.25 | Val RMSE: 0.4576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml; uploading history steps 0-0, summary, console lines 2-99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 9134506.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1217865.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.45765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33meffortless-sweep-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/fpjktnkk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092055-fpjktnkk/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8vtqu69s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092115-8vtqu69s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-sweep-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/8vtqu69s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwoven-sweep-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/8vtqu69s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092115-8vtqu69s/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092116-8vtqu69s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-sweep-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/8vtqu69s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 14952478.31 | Val Loss: 1993021.25 | Val RMSE: 0.7464\n",
      "Epoch 2/100 | Train Loss: 14944954.44 | Val Loss: 1992192.00 | Val RMSE: 0.5269\n",
      "Epoch 3/100 | Train Loss: 14937258.12 | Val Loss: 1991242.25 | Val RMSE: 0.5422\n",
      "Epoch 4/100 | Train Loss: 14932993.56 | Val Loss: 1990367.56 | Val RMSE: 0.5685\n",
      "Epoch 5/100 | Train Loss: 14926937.81 | Val Loss: 1989589.19 | Val RMSE: 0.4388\n",
      "Epoch 6/100 | Train Loss: 14919986.75 | Val Loss: 1988417.44 | Val RMSE: 0.6649\n",
      "Epoch 7/100 | Train Loss: 14914304.06 | Val Loss: 1988283.50 | Val RMSE: 0.4600\n",
      "Epoch 8/100 | Train Loss: 14910555.12 | Val Loss: 1987613.56 | Val RMSE: 0.5465\n",
      "Epoch 9/100 | Train Loss: 14903524.94 | Val Loss: 1986645.06 | Val RMSE: 0.2683\n",
      "Epoch 10/100 | Train Loss: 14897285.75 | Val Loss: 1985890.12 | Val RMSE: 0.5122\n",
      "Epoch 11/100 | Train Loss: 14889984.75 | Val Loss: 1985019.25 | Val RMSE: 0.2555\n",
      "Epoch 12/100 | Train Loss: 14886618.44 | Val Loss: 1984537.56 | Val RMSE: 0.4004\n",
      "Epoch 13/100 | Train Loss: 14882457.19 | Val Loss: 1983323.62 | Val RMSE: 0.7490\n",
      "Epoch 14/100 | Train Loss: 14875973.44 | Val Loss: 1982070.62 | Val RMSE: 0.4086\n",
      "Epoch 15/100 | Train Loss: 14874137.94 | Val Loss: 1982388.50 | Val RMSE: 0.2555\n",
      "Epoch 16/100 | Train Loss: 14866148.19 | Val Loss: 1981490.75 | Val RMSE: 0.5283\n",
      "Epoch 17/100 | Train Loss: 14860240.75 | Val Loss: 1980741.56 | Val RMSE: 0.4556\n",
      "Epoch 18/100 | Train Loss: 14855348.25 | Val Loss: 1979974.25 | Val RMSE: 0.3600\n",
      "Epoch 19/100 | Train Loss: 14849270.62 | Val Loss: 1978980.25 | Val RMSE: 0.4177\n",
      "Epoch 20/100 | Train Loss: 14841396.12 | Val Loss: 1978694.00 | Val RMSE: 0.3123\n",
      "Epoch 21/100 | Train Loss: 14838533.25 | Val Loss: 1977655.19 | Val RMSE: 0.3185\n",
      "Epoch 22/100 | Train Loss: 14829494.56 | Val Loss: 1976713.69 | Val RMSE: 0.3880\n",
      "Epoch 23/100 | Train Loss: 14824214.06 | Val Loss: 1975654.12 | Val RMSE: 0.4252\n",
      "Epoch 24/100 | Train Loss: 14818306.75 | Val Loss: 1975160.31 | Val RMSE: 0.5798\n",
      "Epoch 25/100 | Train Loss: 14812142.25 | Val Loss: 1974691.31 | Val RMSE: 0.6594\n",
      "Epoch 26/100 | Train Loss: 14807847.00 | Val Loss: 1973604.75 | Val RMSE: 0.3267\n",
      "Epoch 27/100 | Train Loss: 14802971.31 | Val Loss: 1972299.56 | Val RMSE: 0.3563\n",
      "Epoch 28/100 | Train Loss: 14794825.69 | Val Loss: 1972259.56 | Val RMSE: 0.4312\n",
      "Epoch 29/100 | Train Loss: 14784433.75 | Val Loss: 1971451.81 | Val RMSE: 0.3973\n",
      "Epoch 30/100 | Train Loss: 14785497.06 | Val Loss: 1970466.12 | Val RMSE: 0.3014\n",
      "Epoch 31/100 | Train Loss: 14775055.44 | Val Loss: 1969791.00 | Val RMSE: 0.3047\n",
      "Epoch 32/100 | Train Loss: 14771705.00 | Val Loss: 1969292.00 | Val RMSE: 0.2856\n",
      "Epoch 33/100 | Train Loss: 14767541.12 | Val Loss: 1968453.31 | Val RMSE: 0.2741\n",
      "Epoch 34/100 | Train Loss: 14759654.69 | Val Loss: 1967483.94 | Val RMSE: 0.3860\n",
      "Epoch 35/100 | Train Loss: 14754825.62 | Val Loss: 1966792.50 | Val RMSE: 0.3081\n",
      "Epoch 36/100 | Train Loss: 14746524.62 | Val Loss: 1965853.94 | Val RMSE: 0.3665\n",
      "Epoch 37/100 | Train Loss: 14742599.19 | Val Loss: 1965014.94 | Val RMSE: 0.2717\n",
      "Epoch 38/100 | Train Loss: 14739911.56 | Val Loss: 1964296.56 | Val RMSE: 0.6528\n",
      "Epoch 39/100 | Train Loss: 14731200.12 | Val Loss: 1963379.69 | Val RMSE: 0.3147\n",
      "Epoch 40/100 | Train Loss: 14726368.69 | Val Loss: 1962343.19 | Val RMSE: 0.3422\n",
      "Epoch 41/100 | Train Loss: 14716099.50 | Val Loss: 1961584.00 | Val RMSE: 0.8534\n",
      "Epoch 42/100 | Train Loss: 14715501.06 | Val Loss: 1961430.06 | Val RMSE: 0.6881\n",
      "Epoch 43/100 | Train Loss: 14703985.38 | Val Loss: 1960493.06 | Val RMSE: 0.2885\n",
      "Epoch 44/100 | Train Loss: 14703073.44 | Val Loss: 1959516.56 | Val RMSE: 0.4734\n",
      "Epoch 45/100 | Train Loss: 14691801.81 | Val Loss: 1958809.81 | Val RMSE: 0.3542\n",
      "Epoch 46/100 | Train Loss: 14691690.56 | Val Loss: 1957971.38 | Val RMSE: 0.4279\n",
      "Epoch 47/100 | Train Loss: 14683559.25 | Val Loss: 1957781.12 | Val RMSE: 0.3264\n",
      "Epoch 48/100 | Train Loss: 14677195.81 | Val Loss: 1956128.12 | Val RMSE: 0.2712\n",
      "Epoch 49/100 | Train Loss: 14671701.38 | Val Loss: 1955096.81 | Val RMSE: 0.8509\n",
      "Epoch 50/100 | Train Loss: 14665804.94 | Val Loss: 1954789.50 | Val RMSE: 0.2887\n",
      "Epoch 51/100 | Train Loss: 14660334.38 | Val Loss: 1954058.62 | Val RMSE: 0.2879\n",
      "Epoch 52/100 | Train Loss: 14656569.31 | Val Loss: 1953175.44 | Val RMSE: 0.3598\n",
      "Epoch 53/100 | Train Loss: 14653416.19 | Val Loss: 1952456.12 | Val RMSE: 0.3187\n",
      "Epoch 54/100 | Train Loss: 14642289.75 | Val Loss: 1951292.94 | Val RMSE: 0.3123\n",
      "Epoch 55/100 | Train Loss: 14634194.25 | Val Loss: 1951005.44 | Val RMSE: 0.4500\n",
      "Epoch 56/100 | Train Loss: 14630510.75 | Val Loss: 1949940.69 | Val RMSE: 0.2518\n",
      "Epoch 57/100 | Train Loss: 14623998.00 | Val Loss: 1949554.94 | Val RMSE: 0.3843\n",
      "Epoch 58/100 | Train Loss: 14619590.31 | Val Loss: 1948657.31 | Val RMSE: 0.3800\n",
      "Epoch 59/100 | Train Loss: 14611503.94 | Val Loss: 1947831.62 | Val RMSE: 0.2585\n",
      "Epoch 60/100 | Train Loss: 14606574.44 | Val Loss: 1947171.44 | Val RMSE: 0.2353\n",
      "Epoch 61/100 | Train Loss: 14602831.81 | Val Loss: 1946419.12 | Val RMSE: 0.3532\n",
      "Epoch 62/100 | Train Loss: 14595620.94 | Val Loss: 1945591.69 | Val RMSE: 0.3430\n",
      "Epoch 63/100 | Train Loss: 14592080.12 | Val Loss: 1944992.88 | Val RMSE: 0.3499\n",
      "Epoch 64/100 | Train Loss: 14582937.56 | Val Loss: 1943910.00 | Val RMSE: 0.3130\n",
      "Epoch 65/100 | Train Loss: 14577873.88 | Val Loss: 1943656.62 | Val RMSE: 0.7790\n",
      "Epoch 66/100 | Train Loss: 14571371.50 | Val Loss: 1942210.38 | Val RMSE: 0.3995\n",
      "Epoch 67/100 | Train Loss: 14566162.56 | Val Loss: 1941563.50 | Val RMSE: 0.6081\n",
      "Epoch 68/100 | Train Loss: 14559118.62 | Val Loss: 1940730.12 | Val RMSE: 0.3545\n",
      "Epoch 69/100 | Train Loss: 14551101.00 | Val Loss: 1939989.06 | Val RMSE: 0.3413\n",
      "Epoch 70/100 | Train Loss: 14549152.75 | Val Loss: 1939110.38 | Val RMSE: 0.3626\n",
      "Epoch 71/100 | Train Loss: 14543538.00 | Val Loss: 1938050.19 | Val RMSE: 0.2836\n",
      "Epoch 72/100 | Train Loss: 14537406.75 | Val Loss: 1937410.75 | Val RMSE: 0.3367\n",
      "Epoch 73/100 | Train Loss: 14530431.38 | Val Loss: 1936481.25 | Val RMSE: 0.4066\n",
      "Epoch 74/100 | Train Loss: 14526588.31 | Val Loss: 1935956.19 | Val RMSE: 0.3070\n",
      "Epoch 75/100 | Train Loss: 14518765.56 | Val Loss: 1934664.06 | Val RMSE: 0.3522\n",
      "Epoch 76/100 | Train Loss: 14512118.19 | Val Loss: 1934427.81 | Val RMSE: 0.3934\n",
      "Epoch 77/100 | Train Loss: 14504388.12 | Val Loss: 1933200.00 | Val RMSE: 0.4507\n",
      "Epoch 78/100 | Train Loss: 14499710.94 | Val Loss: 1932636.88 | Val RMSE: 0.3553\n",
      "Epoch 79/100 | Train Loss: 14492130.56 | Val Loss: 1932187.00 | Val RMSE: 0.7863\n",
      "Epoch 80/100 | Train Loss: 14486827.56 | Val Loss: 1931185.81 | Val RMSE: 0.2323\n",
      "Epoch 81/100 | Train Loss: 14483350.88 | Val Loss: 1930293.75 | Val RMSE: 0.2639\n",
      "Epoch 82/100 | Train Loss: 14474938.75 | Val Loss: 1929602.94 | Val RMSE: 0.2277\n",
      "Epoch 83/100 | Train Loss: 14470259.56 | Val Loss: 1928824.75 | Val RMSE: 0.5690\n",
      "Epoch 84/100 | Train Loss: 14467087.69 | Val Loss: 1927871.31 | Val RMSE: 0.4903\n",
      "Epoch 85/100 | Train Loss: 14458653.00 | Val Loss: 1927482.44 | Val RMSE: 0.3216\n",
      "Epoch 86/100 | Train Loss: 14451842.69 | Val Loss: 1926375.25 | Val RMSE: 0.3125\n",
      "Epoch 87/100 | Train Loss: 14448532.19 | Val Loss: 1925893.00 | Val RMSE: 0.3290\n",
      "Epoch 88/100 | Train Loss: 14440830.38 | Val Loss: 1924688.44 | Val RMSE: 0.5776\n",
      "Epoch 89/100 | Train Loss: 14434253.44 | Val Loss: 1923703.62 | Val RMSE: 0.2745\n",
      "Epoch 90/100 | Train Loss: 14430390.50 | Val Loss: 1922581.75 | Val RMSE: 0.3075\n",
      "Epoch 91/100 | Train Loss: 14424844.75 | Val Loss: 1922439.31 | Val RMSE: 0.5460\n",
      "Epoch 92/100 | Train Loss: 14414602.62 | Val Loss: 1921797.62 | Val RMSE: 0.5894\n",
      "Epoch 93/100 | Train Loss: 14410450.56 | Val Loss: 1920618.44 | Val RMSE: 0.2435\n",
      "Epoch 94/100 | Train Loss: 14403531.44 | Val Loss: 1919804.19 | Val RMSE: 0.3749\n",
      "Epoch 95/100 | Train Loss: 14396478.06 | Val Loss: 1918639.56 | Val RMSE: 0.3624\n",
      "Epoch 96/100 | Train Loss: 14392324.75 | Val Loss: 1917989.75 | Val RMSE: 0.3631\n",
      "Epoch 97/100 | Train Loss: 14385800.00 | Val Loss: 1917272.69 | Val RMSE: 0.4352\n",
      "Epoch 98/100 | Train Loss: 14379478.38 | Val Loss: 1916465.31 | Val RMSE: 0.5165\n",
      "Epoch 99/100 | Train Loss: 14372539.06 | Val Loss: 1915398.62 | Val RMSE: 0.2993\n",
      "Epoch 100/100 | Train Loss: 14364727.00 | Val Loss: 1915134.81 | Val RMSE: 0.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 14364727.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1915134.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.51385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwoven-sweep-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/8vtqu69s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092116-8vtqu69s/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lcamfw17 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092141-lcamfw17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mspring-sweep-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/lcamfw17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mspring-sweep-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/lcamfw17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092141-lcamfw17/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092142-lcamfw17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mspring-sweep-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/lcamfw17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 15322388.62 | Val Loss: 2042260.06 | Val RMSE: 1.0917\n",
      "Epoch 2/100 | Train Loss: 15317974.31 | Val Loss: 2042000.31 | Val RMSE: 0.5683\n",
      "Epoch 3/100 | Train Loss: 15313865.25 | Val Loss: 2041620.19 | Val RMSE: 0.4379\n",
      "Epoch 4/100 | Train Loss: 15311333.75 | Val Loss: 2041485.06 | Val RMSE: 0.7264\n",
      "Epoch 5/100 | Train Loss: 15310544.00 | Val Loss: 2040903.69 | Val RMSE: 0.8958\n",
      "Epoch 6/100 | Train Loss: 15306755.62 | Val Loss: 2040888.25 | Val RMSE: 0.6774\n",
      "Epoch 7/100 | Train Loss: 15305823.75 | Val Loss: 2040441.00 | Val RMSE: 0.3338\n",
      "Epoch 8/100 | Train Loss: 15300568.50 | Val Loss: 2040241.62 | Val RMSE: 0.4811\n",
      "Epoch 9/100 | Train Loss: 15300955.94 | Val Loss: 2040341.25 | Val RMSE: 0.4948\n",
      "Epoch 10/100 | Train Loss: 15298804.19 | Val Loss: 2039469.19 | Val RMSE: 0.4114\n",
      "Epoch 11/100 | Train Loss: 15298839.19 | Val Loss: 2039184.75 | Val RMSE: 0.3753\n",
      "Epoch 12/100 | Train Loss: 15295362.12 | Val Loss: 2039171.50 | Val RMSE: 0.3906\n",
      "Epoch 13/100 | Train Loss: 15294457.88 | Val Loss: 2038862.25 | Val RMSE: 0.3311\n",
      "Epoch 14/100 | Train Loss: 15290910.88 | Val Loss: 2039161.88 | Val RMSE: 0.4877\n",
      "Epoch 15/100 | Train Loss: 15292058.75 | Val Loss: 2038484.56 | Val RMSE: 0.2955\n",
      "Epoch 16/100 | Train Loss: 15288452.94 | Val Loss: 2038271.44 | Val RMSE: 0.5425\n",
      "Epoch 17/100 | Train Loss: 15289169.75 | Val Loss: 2038248.75 | Val RMSE: 0.6167\n",
      "Epoch 18/100 | Train Loss: 15286889.31 | Val Loss: 2038173.94 | Val RMSE: 0.5614\n",
      "Epoch 19/100 | Train Loss: 15282763.19 | Val Loss: 2037691.31 | Val RMSE: 0.3395\n",
      "Epoch 20/100 | Train Loss: 15280616.00 | Val Loss: 2037299.75 | Val RMSE: 0.3170\n",
      "Epoch 21/100 | Train Loss: 15280018.19 | Val Loss: 2036906.19 | Val RMSE: 0.3888\n",
      "Epoch 22/100 | Train Loss: 15277121.12 | Val Loss: 2037184.31 | Val RMSE: 0.2699\n",
      "Epoch 23/100 | Train Loss: 15274847.19 | Val Loss: 2036933.12 | Val RMSE: 0.3714\n",
      "Epoch 24/100 | Train Loss: 15274202.31 | Val Loss: 2036325.25 | Val RMSE: 0.3804\n",
      "Epoch 25/100 | Train Loss: 15272268.94 | Val Loss: 2036339.75 | Val RMSE: 0.4592\n",
      "Epoch 26/100 | Train Loss: 15270413.50 | Val Loss: 2035758.50 | Val RMSE: 0.2576\n",
      "Epoch 27/100 | Train Loss: 15269033.81 | Val Loss: 2035711.50 | Val RMSE: 0.6269\n",
      "Epoch 28/100 | Train Loss: 15265105.62 | Val Loss: 2035350.31 | Val RMSE: 0.4620\n",
      "Epoch 29/100 | Train Loss: 15265227.81 | Val Loss: 2035283.81 | Val RMSE: 0.3446\n",
      "Epoch 30/100 | Train Loss: 15264039.25 | Val Loss: 2034728.19 | Val RMSE: 0.4206\n",
      "Epoch 31/100 | Train Loss: 15263601.62 | Val Loss: 2034810.94 | Val RMSE: 0.3591\n",
      "Epoch 32/100 | Train Loss: 15263267.88 | Val Loss: 2034158.12 | Val RMSE: 0.2454\n",
      "Epoch 33/100 | Train Loss: 15259448.31 | Val Loss: 2034337.31 | Val RMSE: 0.4110\n",
      "Epoch 34/100 | Train Loss: 15254881.12 | Val Loss: 2034116.81 | Val RMSE: 0.3028\n",
      "Epoch 35/100 | Train Loss: 15255288.88 | Val Loss: 2034222.31 | Val RMSE: 0.3735\n",
      "Epoch 36/100 | Train Loss: 15254679.44 | Val Loss: 2033486.00 | Val RMSE: 0.2437\n",
      "Epoch 37/100 | Train Loss: 15251591.44 | Val Loss: 2033495.94 | Val RMSE: 0.4168\n",
      "Epoch 38/100 | Train Loss: 15250236.38 | Val Loss: 2032938.88 | Val RMSE: 0.3437\n",
      "Epoch 39/100 | Train Loss: 15248108.81 | Val Loss: 2032693.31 | Val RMSE: 0.5616\n",
      "Epoch 40/100 | Train Loss: 15249372.12 | Val Loss: 2032616.94 | Val RMSE: 0.5610\n",
      "Epoch 41/100 | Train Loss: 15245411.12 | Val Loss: 2032558.75 | Val RMSE: 0.3012\n",
      "Epoch 42/100 | Train Loss: 15242386.31 | Val Loss: 2032358.56 | Val RMSE: 0.4885\n",
      "Epoch 43/100 | Train Loss: 15238112.44 | Val Loss: 2031963.56 | Val RMSE: 0.8863\n",
      "Epoch 44/100 | Train Loss: 15239382.12 | Val Loss: 2031683.81 | Val RMSE: 0.2354\n",
      "Epoch 45/100 | Train Loss: 15238706.69 | Val Loss: 2031318.69 | Val RMSE: 0.8333\n",
      "Epoch 46/100 | Train Loss: 15236205.94 | Val Loss: 2030870.44 | Val RMSE: 0.3745\n",
      "Epoch 47/100 | Train Loss: 15234857.38 | Val Loss: 2031204.38 | Val RMSE: 0.4827\n",
      "Epoch 48/100 | Train Loss: 15232195.81 | Val Loss: 2030801.94 | Val RMSE: 0.4949\n",
      "Epoch 49/100 | Train Loss: 15230976.88 | Val Loss: 2030485.88 | Val RMSE: 0.3179\n",
      "Epoch 50/100 | Train Loss: 15231818.69 | Val Loss: 2030178.31 | Val RMSE: 0.3995\n",
      "Epoch 51/100 | Train Loss: 15226327.88 | Val Loss: 2030094.88 | Val RMSE: 0.4066\n",
      "Epoch 52/100 | Train Loss: 15226876.50 | Val Loss: 2029740.81 | Val RMSE: 0.5874\n",
      "Epoch 53/100 | Train Loss: 15221139.44 | Val Loss: 2029346.12 | Val RMSE: 0.5179\n",
      "Epoch 54/100 | Train Loss: 15222747.00 | Val Loss: 2029455.88 | Val RMSE: 0.4132\n",
      "Epoch 55/100 | Train Loss: 15219706.06 | Val Loss: 2029368.75 | Val RMSE: 0.2935\n",
      "Epoch 56/100 | Train Loss: 15219132.88 | Val Loss: 2028913.38 | Val RMSE: 0.3973\n",
      "Epoch 57/100 | Train Loss: 15214622.62 | Val Loss: 2028616.00 | Val RMSE: 0.9111\n",
      "Epoch 58/100 | Train Loss: 15212467.94 | Val Loss: 2028566.19 | Val RMSE: 0.5331\n",
      "Epoch 59/100 | Train Loss: 15212486.88 | Val Loss: 2028258.12 | Val RMSE: 0.4695\n",
      "Epoch 60/100 | Train Loss: 15209923.88 | Val Loss: 2028226.06 | Val RMSE: 0.6541\n",
      "Epoch 61/100 | Train Loss: 15210635.38 | Val Loss: 2027770.38 | Val RMSE: 0.9603\n",
      "Epoch 62/100 | Train Loss: 15206543.94 | Val Loss: 2027691.00 | Val RMSE: 0.2934\n",
      "Epoch 63/100 | Train Loss: 15207940.25 | Val Loss: 2027176.75 | Val RMSE: 0.6798\n",
      "Epoch 64/100 | Train Loss: 15204221.00 | Val Loss: 2027202.00 | Val RMSE: 0.2624\n",
      "Epoch 65/100 | Train Loss: 15203570.06 | Val Loss: 2026803.94 | Val RMSE: 0.2733\n",
      "Epoch 66/100 | Train Loss: 15200241.44 | Val Loss: 2026526.75 | Val RMSE: 0.4898\n",
      "Epoch 67/100 | Train Loss: 15199923.00 | Val Loss: 2026791.94 | Val RMSE: 0.3360\n",
      "Epoch 68/100 | Train Loss: 15197476.44 | Val Loss: 2026021.25 | Val RMSE: 0.2902\n",
      "Epoch 69/100 | Train Loss: 15195298.75 | Val Loss: 2025791.25 | Val RMSE: 0.3205\n",
      "Epoch 70/100 | Train Loss: 15192957.38 | Val Loss: 2025587.19 | Val RMSE: 0.4469\n",
      "Epoch 71/100 | Train Loss: 15189576.12 | Val Loss: 2025540.12 | Val RMSE: 0.4818\n",
      "Epoch 72/100 | Train Loss: 15189259.69 | Val Loss: 2025003.12 | Val RMSE: 0.4096\n",
      "Epoch 73/100 | Train Loss: 15185843.12 | Val Loss: 2024935.94 | Val RMSE: 0.5450\n",
      "Epoch 74/100 | Train Loss: 15186033.25 | Val Loss: 2024816.88 | Val RMSE: 0.4907\n",
      "Epoch 75/100 | Train Loss: 15186618.19 | Val Loss: 2024561.38 | Val RMSE: 0.4156\n",
      "Epoch 76/100 | Train Loss: 15182550.81 | Val Loss: 2023876.62 | Val RMSE: 0.3308\n",
      "Epoch 77/100 | Train Loss: 15180058.06 | Val Loss: 2023910.50 | Val RMSE: 0.4235\n",
      "Epoch 78/100 | Train Loss: 15179450.31 | Val Loss: 2023740.44 | Val RMSE: 0.3077\n",
      "Epoch 79/100 | Train Loss: 15175723.94 | Val Loss: 2023376.88 | Val RMSE: 0.3143\n",
      "Epoch 80/100 | Train Loss: 15177926.88 | Val Loss: 2022648.94 | Val RMSE: 0.2809\n",
      "Epoch 81/100 | Train Loss: 15169931.19 | Val Loss: 2022892.56 | Val RMSE: 0.3760\n",
      "Epoch 82/100 | Train Loss: 15170059.12 | Val Loss: 2023025.38 | Val RMSE: 0.7833\n",
      "Epoch 83/100 | Train Loss: 15168603.31 | Val Loss: 2022587.62 | Val RMSE: 0.3229\n",
      "Epoch 84/100 | Train Loss: 15164940.12 | Val Loss: 2021995.81 | Val RMSE: 0.4111\n",
      "Epoch 85/100 | Train Loss: 15164184.31 | Val Loss: 2021765.75 | Val RMSE: 0.4186\n",
      "Epoch 86/100 | Train Loss: 15162693.75 | Val Loss: 2021445.38 | Val RMSE: 0.3844\n",
      "Epoch 87/100 | Train Loss: 15160397.81 | Val Loss: 2021508.88 | Val RMSE: 0.6803\n",
      "Epoch 88/100 | Train Loss: 15159199.62 | Val Loss: 2021219.75 | Val RMSE: 0.3680\n",
      "Epoch 89/100 | Train Loss: 15157496.81 | Val Loss: 2020639.94 | Val RMSE: 0.3896\n",
      "Epoch 90/100 | Train Loss: 15155135.56 | Val Loss: 2020028.44 | Val RMSE: 0.5551\n",
      "Epoch 91/100 | Train Loss: 15155184.38 | Val Loss: 2020716.00 | Val RMSE: 0.6102\n",
      "Epoch 92/100 | Train Loss: 15154198.38 | Val Loss: 2020319.06 | Val RMSE: 0.2991\n",
      "Epoch 93/100 | Train Loss: 15152248.19 | Val Loss: 2020157.62 | Val RMSE: 0.2654\n",
      "Epoch 94/100 | Train Loss: 15148425.06 | Val Loss: 2020030.44 | Val RMSE: 0.4137\n",
      "Epoch 95/100 | Train Loss: 15147822.81 | Val Loss: 2019378.88 | Val RMSE: 0.4695\n",
      "Epoch 96/100 | Train Loss: 15143638.50 | Val Loss: 2019555.06 | Val RMSE: 0.4015\n",
      "Epoch 97/100 | Train Loss: 15140777.81 | Val Loss: 2018588.25 | Val RMSE: 0.3535\n",
      "Epoch 98/100 | Train Loss: 15141697.31 | Val Loss: 2018609.44 | Val RMSE: 0.6705\n",
      "Epoch 99/100 | Train Loss: 15138332.75 | Val Loss: 2018545.12 | Val RMSE: 0.3095\n",
      "Epoch 100/100 | Train Loss: 15135942.00 | Val Loss: 2017955.94 | Val RMSE: 0.3826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 15135942.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 2017955.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.38265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mspring-sweep-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/lcamfw17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092142-lcamfw17/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: njb04kws with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092207-njb04kws\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-sweep-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/njb04kws\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstoic-sweep-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/njb04kws\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092207-njb04kws/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092208-njb04kws\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-sweep-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/njb04kws\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 12319054.00 | Val Loss: 1641574.75 | Val RMSE: 0.4461\n",
      "Epoch 2/100 | Train Loss: 12310092.50 | Val Loss: 1640492.75 | Val RMSE: 0.7074\n",
      "Epoch 3/100 | Train Loss: 12306819.62 | Val Loss: 1639970.19 | Val RMSE: 0.8985\n",
      "Epoch 4/100 | Train Loss: 12299340.12 | Val Loss: 1639521.19 | Val RMSE: 0.6216\n",
      "Epoch 5/100 | Train Loss: 12293777.25 | Val Loss: 1638386.50 | Val RMSE: 0.6439\n",
      "Epoch 6/100 | Train Loss: 12289132.38 | Val Loss: 1638335.88 | Val RMSE: 0.4303\n",
      "Epoch 7/100 | Train Loss: 12281555.75 | Val Loss: 1637183.31 | Val RMSE: 0.4346\n",
      "Epoch 8/100 | Train Loss: 12276285.69 | Val Loss: 1636327.50 | Val RMSE: 0.3287\n",
      "Epoch 9/100 | Train Loss: 12272799.81 | Val Loss: 1635606.25 | Val RMSE: 0.3171\n",
      "Epoch 10/100 | Train Loss: 12267486.81 | Val Loss: 1635012.00 | Val RMSE: 0.3121\n",
      "Epoch 11/100 | Train Loss: 12261629.12 | Val Loss: 1634649.50 | Val RMSE: 0.5200\n",
      "Epoch 12/100 | Train Loss: 12257217.44 | Val Loss: 1633631.12 | Val RMSE: 0.3570\n",
      "Epoch 13/100 | Train Loss: 12248408.19 | Val Loss: 1633034.00 | Val RMSE: 0.3201\n",
      "Epoch 14/100 | Train Loss: 12245466.12 | Val Loss: 1632121.25 | Val RMSE: 0.3131\n",
      "Epoch 15/100 | Train Loss: 12241386.19 | Val Loss: 1631508.06 | Val RMSE: 0.6161\n",
      "Epoch 16/100 | Train Loss: 12233790.19 | Val Loss: 1631311.00 | Val RMSE: 0.3641\n",
      "Epoch 17/100 | Train Loss: 12229795.25 | Val Loss: 1630261.81 | Val RMSE: 0.3378\n",
      "Epoch 18/100 | Train Loss: 12226341.38 | Val Loss: 1629781.00 | Val RMSE: 0.3746\n",
      "Epoch 19/100 | Train Loss: 12221892.88 | Val Loss: 1628655.44 | Val RMSE: 0.5669\n",
      "Epoch 20/100 | Train Loss: 12212220.50 | Val Loss: 1627750.19 | Val RMSE: 0.5226\n",
      "Epoch 21/100 | Train Loss: 12209341.88 | Val Loss: 1627703.94 | Val RMSE: 0.4046\n",
      "Epoch 22/100 | Train Loss: 12201362.94 | Val Loss: 1626885.12 | Val RMSE: 0.5204\n",
      "Epoch 23/100 | Train Loss: 12198267.19 | Val Loss: 1625700.06 | Val RMSE: 0.3797\n",
      "Epoch 24/100 | Train Loss: 12193996.94 | Val Loss: 1625141.19 | Val RMSE: 0.6694\n",
      "Epoch 25/100 | Train Loss: 12191248.88 | Val Loss: 1624217.44 | Val RMSE: 0.5089\n",
      "Epoch 26/100 | Train Loss: 12183145.19 | Val Loss: 1624089.19 | Val RMSE: 0.5325\n",
      "Epoch 27/100 | Train Loss: 12176244.25 | Val Loss: 1622831.69 | Val RMSE: 0.3657\n",
      "Epoch 28/100 | Train Loss: 12175470.25 | Val Loss: 1622403.31 | Val RMSE: 0.4350\n",
      "Epoch 29/100 | Train Loss: 12166786.31 | Val Loss: 1621697.81 | Val RMSE: 0.4127\n",
      "Epoch 30/100 | Train Loss: 12164488.44 | Val Loss: 1620846.19 | Val RMSE: 0.5709\n",
      "Epoch 31/100 | Train Loss: 12157876.88 | Val Loss: 1620179.31 | Val RMSE: 0.3954\n",
      "Epoch 32/100 | Train Loss: 12152844.69 | Val Loss: 1619731.44 | Val RMSE: 0.3434\n",
      "Epoch 33/100 | Train Loss: 12146892.38 | Val Loss: 1619478.75 | Val RMSE: 0.4099\n",
      "Epoch 34/100 | Train Loss: 12143007.00 | Val Loss: 1618422.62 | Val RMSE: 0.4109\n",
      "Epoch 35/100 | Train Loss: 12135373.75 | Val Loss: 1617695.19 | Val RMSE: 0.3096\n",
      "Epoch 36/100 | Train Loss: 12132649.88 | Val Loss: 1616664.50 | Val RMSE: 0.5790\n",
      "Epoch 37/100 | Train Loss: 12126808.81 | Val Loss: 1616279.50 | Val RMSE: 0.3971\n",
      "Epoch 38/100 | Train Loss: 12120675.25 | Val Loss: 1615499.12 | Val RMSE: 0.6889\n",
      "Epoch 39/100 | Train Loss: 12113970.44 | Val Loss: 1613998.06 | Val RMSE: 0.4135\n",
      "Epoch 40/100 | Train Loss: 12110263.62 | Val Loss: 1613811.12 | Val RMSE: 0.3122\n",
      "Epoch 41/100 | Train Loss: 12102149.69 | Val Loss: 1614000.06 | Val RMSE: 0.5452\n",
      "Epoch 42/100 | Train Loss: 12101520.50 | Val Loss: 1612588.44 | Val RMSE: 0.4721\n",
      "Epoch 43/100 | Train Loss: 12089421.69 | Val Loss: 1611784.75 | Val RMSE: 0.3899\n",
      "Epoch 44/100 | Train Loss: 12089003.56 | Val Loss: 1611065.81 | Val RMSE: 0.3609\n",
      "Epoch 45/100 | Train Loss: 12084174.88 | Val Loss: 1610651.12 | Val RMSE: 0.3723\n",
      "Epoch 46/100 | Train Loss: 12080117.25 | Val Loss: 1609706.12 | Val RMSE: 0.4309\n",
      "Epoch 47/100 | Train Loss: 12075223.00 | Val Loss: 1609649.50 | Val RMSE: 0.3107\n",
      "Epoch 48/100 | Train Loss: 12063706.19 | Val Loss: 1608377.25 | Val RMSE: 0.3862\n",
      "Epoch 49/100 | Train Loss: 12057346.50 | Val Loss: 1607597.69 | Val RMSE: 0.3937\n",
      "Epoch 50/100 | Train Loss: 12057177.00 | Val Loss: 1606854.50 | Val RMSE: 0.3734\n",
      "Epoch 51/100 | Train Loss: 12051196.88 | Val Loss: 1606140.62 | Val RMSE: 0.4251\n",
      "Epoch 52/100 | Train Loss: 12043750.50 | Val Loss: 1605495.69 | Val RMSE: 0.3206\n",
      "Epoch 53/100 | Train Loss: 12041132.25 | Val Loss: 1604944.31 | Val RMSE: 0.4072\n",
      "Epoch 54/100 | Train Loss: 12034280.19 | Val Loss: 1604203.44 | Val RMSE: 0.3973\n",
      "Epoch 55/100 | Train Loss: 12028107.69 | Val Loss: 1603104.06 | Val RMSE: 0.5147\n",
      "Epoch 56/100 | Train Loss: 12024323.25 | Val Loss: 1602561.06 | Val RMSE: 0.3567\n",
      "Epoch 57/100 | Train Loss: 12019938.69 | Val Loss: 1602070.19 | Val RMSE: 0.3984\n",
      "Epoch 58/100 | Train Loss: 12012436.56 | Val Loss: 1600724.50 | Val RMSE: 0.4434\n",
      "Epoch 59/100 | Train Loss: 12008517.62 | Val Loss: 1600211.94 | Val RMSE: 0.3566\n",
      "Epoch 60/100 | Train Loss: 12001953.19 | Val Loss: 1599667.06 | Val RMSE: 0.3904\n",
      "Epoch 61/100 | Train Loss: 11993490.00 | Val Loss: 1599056.75 | Val RMSE: 0.3724\n",
      "Epoch 62/100 | Train Loss: 11991402.75 | Val Loss: 1597892.88 | Val RMSE: 0.2897\n",
      "Epoch 63/100 | Train Loss: 11985017.50 | Val Loss: 1597315.69 | Val RMSE: 0.3411\n",
      "Epoch 64/100 | Train Loss: 11974770.69 | Val Loss: 1596640.69 | Val RMSE: 0.4420\n",
      "Epoch 65/100 | Train Loss: 11972566.00 | Val Loss: 1596126.94 | Val RMSE: 0.4366\n",
      "Epoch 66/100 | Train Loss: 11968767.75 | Val Loss: 1594825.25 | Val RMSE: 0.3842\n",
      "Epoch 67/100 | Train Loss: 11965693.25 | Val Loss: 1594296.38 | Val RMSE: 0.3697\n",
      "Epoch 68/100 | Train Loss: 11955788.75 | Val Loss: 1593718.56 | Val RMSE: 0.3810\n",
      "Epoch 69/100 | Train Loss: 11953579.69 | Val Loss: 1593001.06 | Val RMSE: 0.4324\n",
      "Epoch 70/100 | Train Loss: 11949142.56 | Val Loss: 1592470.44 | Val RMSE: 0.3248\n",
      "Epoch 71/100 | Train Loss: 11943364.44 | Val Loss: 1591451.38 | Val RMSE: 0.3208\n",
      "Epoch 72/100 | Train Loss: 11937495.12 | Val Loss: 1590528.88 | Val RMSE: 0.3435\n",
      "Epoch 73/100 | Train Loss: 11928477.44 | Val Loss: 1590342.25 | Val RMSE: 0.3638\n",
      "Epoch 74/100 | Train Loss: 11923968.81 | Val Loss: 1589436.12 | Val RMSE: 0.3509\n",
      "Epoch 75/100 | Train Loss: 11917916.69 | Val Loss: 1588620.31 | Val RMSE: 0.6044\n",
      "Epoch 76/100 | Train Loss: 11912577.56 | Val Loss: 1588082.25 | Val RMSE: 0.4433\n",
      "Epoch 77/100 | Train Loss: 11907611.56 | Val Loss: 1587250.88 | Val RMSE: 0.3209\n",
      "Epoch 78/100 | Train Loss: 11902946.25 | Val Loss: 1586219.31 | Val RMSE: 0.3387\n",
      "Epoch 79/100 | Train Loss: 11895493.94 | Val Loss: 1585664.56 | Val RMSE: 1.0111\n",
      "Epoch 80/100 | Train Loss: 11892574.62 | Val Loss: 1585056.25 | Val RMSE: 0.3135\n",
      "Epoch 81/100 | Train Loss: 11884810.38 | Val Loss: 1584029.06 | Val RMSE: 0.3819\n",
      "Epoch 82/100 | Train Loss: 11878785.25 | Val Loss: 1583210.12 | Val RMSE: 0.4835\n",
      "Epoch 83/100 | Train Loss: 11875047.56 | Val Loss: 1582600.62 | Val RMSE: 0.5229\n",
      "Epoch 84/100 | Train Loss: 11867747.81 | Val Loss: 1581648.88 | Val RMSE: 0.3303\n",
      "Epoch 85/100 | Train Loss: 11860360.62 | Val Loss: 1580772.06 | Val RMSE: 0.4253\n",
      "Epoch 86/100 | Train Loss: 11856993.38 | Val Loss: 1580524.00 | Val RMSE: 0.3093\n",
      "Epoch 87/100 | Train Loss: 11851110.12 | Val Loss: 1579691.19 | Val RMSE: 0.3316\n",
      "Epoch 88/100 | Train Loss: 11847725.62 | Val Loss: 1578737.69 | Val RMSE: 0.2905\n",
      "Epoch 89/100 | Train Loss: 11837902.06 | Val Loss: 1577972.75 | Val RMSE: 0.3016\n",
      "Epoch 90/100 | Train Loss: 11832671.19 | Val Loss: 1577121.38 | Val RMSE: 0.2894\n",
      "Epoch 91/100 | Train Loss: 11828539.00 | Val Loss: 1576326.38 | Val RMSE: 0.4139\n",
      "Epoch 92/100 | Train Loss: 11820651.38 | Val Loss: 1575807.12 | Val RMSE: 0.4896\n",
      "Epoch 93/100 | Train Loss: 11815444.19 | Val Loss: 1575241.38 | Val RMSE: 0.3471\n",
      "Epoch 94/100 | Train Loss: 11810827.94 | Val Loss: 1573754.75 | Val RMSE: 0.4910\n",
      "Epoch 95/100 | Train Loss: 11803095.44 | Val Loss: 1573382.88 | Val RMSE: 0.2940\n",
      "Epoch 96/100 | Train Loss: 11795117.00 | Val Loss: 1572189.88 | Val RMSE: 0.5385\n",
      "Epoch 97/100 | Train Loss: 11789489.75 | Val Loss: 1571803.75 | Val RMSE: 0.6691\n",
      "Epoch 98/100 | Train Loss: 11784022.50 | Val Loss: 1570370.12 | Val RMSE: 0.4110\n",
      "Epoch 99/100 | Train Loss: 11776616.56 | Val Loss: 1569767.94 | Val RMSE: 0.3122\n",
      "Epoch 100/100 | Train Loss: 11775763.44 | Val Loss: 1569421.94 | Val RMSE: 0.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 11775763.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1569421.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.40845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstoic-sweep-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/njb04kws\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092208-njb04kws/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 92ooep1p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092234-92ooep1p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-sweep-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/92ooep1p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-sweep-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/92ooep1p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092234-92ooep1p/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092235-92ooep1p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-sweep-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/92ooep1p\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 12608367.88 | Val Loss: 1680170.06 | Val RMSE: 0.6513\n",
      "Epoch 2/100 | Train Loss: 12599630.31 | Val Loss: 1679525.38 | Val RMSE: 0.7724\n",
      "Epoch 3/100 | Train Loss: 12596171.38 | Val Loss: 1679545.25 | Val RMSE: 1.0312\n",
      "Epoch 4/100 | Train Loss: 12592121.44 | Val Loss: 1678686.75 | Val RMSE: 1.0773\n",
      "Epoch 5/100 | Train Loss: 12592009.88 | Val Loss: 1678878.06 | Val RMSE: 1.1458\n",
      "Epoch 6/100 | Train Loss: 12590134.62 | Val Loss: 1678560.81 | Val RMSE: 0.7889\n",
      "Epoch 7/100 | Train Loss: 12586796.50 | Val Loss: 1678160.88 | Val RMSE: 0.5101\n",
      "Epoch 8/100 | Train Loss: 12585100.06 | Val Loss: 1677617.19 | Val RMSE: 0.4395\n",
      "Epoch 9/100 | Train Loss: 12582286.62 | Val Loss: 1677255.19 | Val RMSE: 0.4591\n",
      "Epoch 10/100 | Train Loss: 12580052.75 | Val Loss: 1677427.69 | Val RMSE: 0.3637\n",
      "Epoch 11/100 | Train Loss: 12577781.06 | Val Loss: 1676786.06 | Val RMSE: 0.3864\n",
      "Epoch 12/100 | Train Loss: 12575414.19 | Val Loss: 1676759.25 | Val RMSE: 0.3712\n",
      "Epoch 13/100 | Train Loss: 12573976.06 | Val Loss: 1676398.50 | Val RMSE: 0.3819\n",
      "Epoch 14/100 | Train Loss: 12572311.75 | Val Loss: 1675852.44 | Val RMSE: 0.5056\n",
      "Epoch 15/100 | Train Loss: 12570632.81 | Val Loss: 1676000.81 | Val RMSE: 0.6679\n",
      "Epoch 16/100 | Train Loss: 12567739.69 | Val Loss: 1675324.00 | Val RMSE: 0.4098\n",
      "Epoch 17/100 | Train Loss: 12564786.19 | Val Loss: 1674938.12 | Val RMSE: 0.3421\n",
      "Epoch 18/100 | Train Loss: 12562214.69 | Val Loss: 1674589.00 | Val RMSE: 0.3968\n",
      "Epoch 19/100 | Train Loss: 12559336.81 | Val Loss: 1674484.12 | Val RMSE: 0.3823\n",
      "Epoch 20/100 | Train Loss: 12559263.88 | Val Loss: 1674638.69 | Val RMSE: 0.5158\n",
      "Epoch 21/100 | Train Loss: 12555763.25 | Val Loss: 1673948.38 | Val RMSE: 0.4150\n",
      "Epoch 22/100 | Train Loss: 12554239.12 | Val Loss: 1673528.38 | Val RMSE: 0.3815\n",
      "Epoch 23/100 | Train Loss: 12551491.75 | Val Loss: 1673363.75 | Val RMSE: 0.4410\n",
      "Epoch 24/100 | Train Loss: 12552417.00 | Val Loss: 1673158.88 | Val RMSE: 0.3896\n",
      "Epoch 25/100 | Train Loss: 12547631.62 | Val Loss: 1672749.44 | Val RMSE: 0.3889\n",
      "Epoch 26/100 | Train Loss: 12544996.88 | Val Loss: 1672342.25 | Val RMSE: 0.3624\n",
      "Epoch 27/100 | Train Loss: 12542774.62 | Val Loss: 1672533.62 | Val RMSE: 0.3820\n",
      "Epoch 28/100 | Train Loss: 12541845.69 | Val Loss: 1671685.88 | Val RMSE: 0.3820\n",
      "Epoch 29/100 | Train Loss: 12539390.38 | Val Loss: 1671929.12 | Val RMSE: 0.5896\n",
      "Epoch 30/100 | Train Loss: 12536462.88 | Val Loss: 1671180.75 | Val RMSE: 0.3223\n",
      "Epoch 31/100 | Train Loss: 12533131.12 | Val Loss: 1671163.44 | Val RMSE: 0.4544\n",
      "Epoch 32/100 | Train Loss: 12532424.75 | Val Loss: 1670963.94 | Val RMSE: 0.3591\n",
      "Epoch 33/100 | Train Loss: 12528781.56 | Val Loss: 1670707.94 | Val RMSE: 0.4634\n",
      "Epoch 34/100 | Train Loss: 12527777.06 | Val Loss: 1670012.25 | Val RMSE: 0.3350\n",
      "Epoch 35/100 | Train Loss: 12525152.56 | Val Loss: 1669865.56 | Val RMSE: 0.4651\n",
      "Epoch 36/100 | Train Loss: 12522164.88 | Val Loss: 1669481.00 | Val RMSE: 0.3644\n",
      "Epoch 37/100 | Train Loss: 12522307.56 | Val Loss: 1669680.56 | Val RMSE: 0.3860\n",
      "Epoch 38/100 | Train Loss: 12521775.38 | Val Loss: 1668869.75 | Val RMSE: 0.3259\n",
      "Epoch 39/100 | Train Loss: 12520023.81 | Val Loss: 1668806.12 | Val RMSE: 0.4416\n",
      "Epoch 40/100 | Train Loss: 12515769.50 | Val Loss: 1668450.31 | Val RMSE: 0.3181\n",
      "Epoch 41/100 | Train Loss: 12515463.75 | Val Loss: 1667835.06 | Val RMSE: 0.6928\n",
      "Epoch 42/100 | Train Loss: 12513033.62 | Val Loss: 1668012.75 | Val RMSE: 0.4341\n",
      "Epoch 43/100 | Train Loss: 12508568.75 | Val Loss: 1667593.31 | Val RMSE: 0.4335\n",
      "Epoch 44/100 | Train Loss: 12506462.25 | Val Loss: 1667657.38 | Val RMSE: 0.3435\n",
      "Epoch 45/100 | Train Loss: 12503387.62 | Val Loss: 1667219.81 | Val RMSE: 0.4687\n",
      "Epoch 46/100 | Train Loss: 12502537.75 | Val Loss: 1667064.19 | Val RMSE: 0.5161\n",
      "Epoch 47/100 | Train Loss: 12500146.88 | Val Loss: 1666465.44 | Val RMSE: 0.2970\n",
      "Epoch 48/100 | Train Loss: 12498647.94 | Val Loss: 1665939.69 | Val RMSE: 0.3345\n",
      "Epoch 49/100 | Train Loss: 12495688.44 | Val Loss: 1666026.81 | Val RMSE: 0.4527\n",
      "Epoch 50/100 | Train Loss: 12495973.19 | Val Loss: 1665743.44 | Val RMSE: 0.3894\n",
      "Epoch 51/100 | Train Loss: 12495698.81 | Val Loss: 1665321.44 | Val RMSE: 0.3617\n",
      "Epoch 52/100 | Train Loss: 12491380.56 | Val Loss: 1665284.12 | Val RMSE: 0.3049\n",
      "Epoch 53/100 | Train Loss: 12490643.94 | Val Loss: 1665379.50 | Val RMSE: 0.3595\n",
      "Epoch 54/100 | Train Loss: 12485269.19 | Val Loss: 1664800.56 | Val RMSE: 0.3368\n",
      "Epoch 55/100 | Train Loss: 12486097.06 | Val Loss: 1664035.25 | Val RMSE: 0.4385\n",
      "Epoch 56/100 | Train Loss: 12484773.00 | Val Loss: 1664530.38 | Val RMSE: 0.2903\n",
      "Epoch 57/100 | Train Loss: 12479598.19 | Val Loss: 1663271.38 | Val RMSE: 0.4738\n",
      "Epoch 58/100 | Train Loss: 12478551.56 | Val Loss: 1664078.69 | Val RMSE: 0.3541\n",
      "Epoch 59/100 | Train Loss: 12473275.88 | Val Loss: 1663035.56 | Val RMSE: 0.3150\n",
      "Epoch 60/100 | Train Loss: 12475423.94 | Val Loss: 1662971.31 | Val RMSE: 0.3484\n",
      "Epoch 61/100 | Train Loss: 12471388.50 | Val Loss: 1662560.38 | Val RMSE: 0.3757\n",
      "Epoch 62/100 | Train Loss: 12467568.44 | Val Loss: 1662378.19 | Val RMSE: 0.2967\n",
      "Epoch 63/100 | Train Loss: 12469080.12 | Val Loss: 1661997.12 | Val RMSE: 0.3663\n",
      "Epoch 64/100 | Train Loss: 12463380.00 | Val Loss: 1661855.69 | Val RMSE: 0.3583\n",
      "Epoch 65/100 | Train Loss: 12464212.38 | Val Loss: 1661791.06 | Val RMSE: 0.3783\n",
      "Epoch 66/100 | Train Loss: 12461866.81 | Val Loss: 1661304.00 | Val RMSE: 0.4357\n",
      "Epoch 67/100 | Train Loss: 12458322.44 | Val Loss: 1660668.62 | Val RMSE: 0.2788\n",
      "Epoch 68/100 | Train Loss: 12458314.44 | Val Loss: 1660493.56 | Val RMSE: 0.3170\n",
      "Epoch 69/100 | Train Loss: 12455840.56 | Val Loss: 1660577.25 | Val RMSE: 0.2846\n",
      "Epoch 70/100 | Train Loss: 12451153.12 | Val Loss: 1660013.44 | Val RMSE: 0.3492\n",
      "Epoch 71/100 | Train Loss: 12450154.00 | Val Loss: 1659955.94 | Val RMSE: 0.4658\n",
      "Epoch 72/100 | Train Loss: 12450161.62 | Val Loss: 1659472.00 | Val RMSE: 0.3704\n",
      "Epoch 73/100 | Train Loss: 12447041.06 | Val Loss: 1659020.12 | Val RMSE: 0.3544\n",
      "Epoch 74/100 | Train Loss: 12445373.00 | Val Loss: 1658665.38 | Val RMSE: 0.3624\n",
      "Epoch 75/100 | Train Loss: 12440937.12 | Val Loss: 1658596.56 | Val RMSE: 0.3674\n",
      "Epoch 76/100 | Train Loss: 12440781.06 | Val Loss: 1658962.00 | Val RMSE: 0.3078\n",
      "Epoch 77/100 | Train Loss: 12437315.25 | Val Loss: 1658354.88 | Val RMSE: 0.6869\n",
      "Epoch 78/100 | Train Loss: 12434487.06 | Val Loss: 1657992.75 | Val RMSE: 0.3486\n",
      "Epoch 79/100 | Train Loss: 12434592.06 | Val Loss: 1657157.56 | Val RMSE: 0.5370\n",
      "Epoch 80/100 | Train Loss: 12433755.38 | Val Loss: 1657542.62 | Val RMSE: 0.3410\n",
      "Epoch 81/100 | Train Loss: 12427025.81 | Val Loss: 1656544.50 | Val RMSE: 0.3216\n",
      "Epoch 82/100 | Train Loss: 12428347.69 | Val Loss: 1656678.00 | Val RMSE: 0.3205\n",
      "Epoch 83/100 | Train Loss: 12426683.75 | Val Loss: 1656544.25 | Val RMSE: 0.4477\n",
      "Epoch 84/100 | Train Loss: 12422170.62 | Val Loss: 1656328.81 | Val RMSE: 0.3045\n",
      "Epoch 85/100 | Train Loss: 12419458.19 | Val Loss: 1655649.25 | Val RMSE: 0.2807\n",
      "Epoch 86/100 | Train Loss: 12418639.25 | Val Loss: 1655790.44 | Val RMSE: 0.4085\n",
      "Epoch 87/100 | Train Loss: 12418811.31 | Val Loss: 1655022.50 | Val RMSE: 0.4425\n",
      "Epoch 88/100 | Train Loss: 12415354.25 | Val Loss: 1655270.38 | Val RMSE: 0.3150\n",
      "Epoch 89/100 | Train Loss: 12413874.62 | Val Loss: 1654810.25 | Val RMSE: 0.3390\n",
      "Epoch 90/100 | Train Loss: 12409577.62 | Val Loss: 1654562.50 | Val RMSE: 0.4398\n",
      "Epoch 91/100 | Train Loss: 12409866.50 | Val Loss: 1654652.12 | Val RMSE: 0.3203\n",
      "Epoch 92/100 | Train Loss: 12408950.06 | Val Loss: 1654105.25 | Val RMSE: 0.5565\n",
      "Epoch 93/100 | Train Loss: 12402033.50 | Val Loss: 1654402.50 | Val RMSE: 0.5022\n",
      "Epoch 94/100 | Train Loss: 12401591.69 | Val Loss: 1653180.31 | Val RMSE: 0.3485\n",
      "Epoch 95/100 | Train Loss: 12400497.38 | Val Loss: 1653045.56 | Val RMSE: 0.3280\n",
      "Epoch 96/100 | Train Loss: 12399149.44 | Val Loss: 1653127.19 | Val RMSE: 0.3505\n",
      "Epoch 97/100 | Train Loss: 12394983.69 | Val Loss: 1652409.50 | Val RMSE: 0.2872\n",
      "Epoch 98/100 | Train Loss: 12391768.81 | Val Loss: 1652521.94 | Val RMSE: 0.2985\n",
      "Epoch 99/100 | Train Loss: 12393395.56 | Val Loss: 1652060.50 | Val RMSE: 0.3162\n",
      "Epoch 100/100 | Train Loss: 12391658.00 | Val Loss: 1651727.88 | Val RMSE: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 12391658.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1651727.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.33244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-sweep-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/92ooep1p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092235-92ooep1p/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h8c7z9qa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092259-h8c7z9qa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melectric-sweep-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/h8c7z9qa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33melectric-sweep-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/h8c7z9qa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092259-h8c7z9qa/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092301-h8c7z9qa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melectric-sweep-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/h8c7z9qa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 13091945.06 | Val Loss: 1745220.00 | Val RMSE: 0.4769\n",
      "Epoch 2/100 | Train Loss: 13087247.62 | Val Loss: 1744342.69 | Val RMSE: 0.5122\n",
      "Epoch 3/100 | Train Loss: 13081602.94 | Val Loss: 1743472.06 | Val RMSE: 0.6145\n",
      "Epoch 4/100 | Train Loss: 13078358.12 | Val Loss: 1743160.31 | Val RMSE: 0.5849\n",
      "Epoch 5/100 | Train Loss: 13073024.38 | Val Loss: 1742653.12 | Val RMSE: 0.4950\n",
      "Epoch 6/100 | Train Loss: 13068257.88 | Val Loss: 1741909.56 | Val RMSE: 0.3719\n",
      "Epoch 7/100 | Train Loss: 13064117.50 | Val Loss: 1741246.50 | Val RMSE: 0.3728\n",
      "Epoch 8/100 | Train Loss: 13058059.00 | Val Loss: 1740918.50 | Val RMSE: 0.4929\n",
      "Epoch 9/100 | Train Loss: 13055648.19 | Val Loss: 1740177.19 | Val RMSE: 0.4705\n",
      "Epoch 10/100 | Train Loss: 13050942.75 | Val Loss: 1739756.44 | Val RMSE: 0.5518\n",
      "Epoch 11/100 | Train Loss: 13044526.38 | Val Loss: 1738799.94 | Val RMSE: 0.3109\n",
      "Epoch 12/100 | Train Loss: 13043020.75 | Val Loss: 1738130.31 | Val RMSE: 0.7602\n",
      "Epoch 13/100 | Train Loss: 13035659.19 | Val Loss: 1737505.19 | Val RMSE: 0.3187\n",
      "Epoch 14/100 | Train Loss: 13030102.69 | Val Loss: 1736977.56 | Val RMSE: 0.6082\n",
      "Epoch 15/100 | Train Loss: 13028639.88 | Val Loss: 1736694.62 | Val RMSE: 0.4669\n",
      "Epoch 16/100 | Train Loss: 13023877.94 | Val Loss: 1735911.88 | Val RMSE: 0.3948\n",
      "Epoch 17/100 | Train Loss: 13016266.12 | Val Loss: 1734607.25 | Val RMSE: 0.3832\n",
      "Epoch 18/100 | Train Loss: 13016157.69 | Val Loss: 1734773.56 | Val RMSE: 0.4025\n",
      "Epoch 19/100 | Train Loss: 13008488.19 | Val Loss: 1733905.50 | Val RMSE: 0.5198\n",
      "Epoch 20/100 | Train Loss: 13003470.69 | Val Loss: 1733430.06 | Val RMSE: 0.5412\n",
      "Epoch 21/100 | Train Loss: 13002616.06 | Val Loss: 1732986.75 | Val RMSE: 0.3983\n",
      "Epoch 22/100 | Train Loss: 12997837.00 | Val Loss: 1732639.56 | Val RMSE: 0.3136\n",
      "Epoch 23/100 | Train Loss: 12992591.81 | Val Loss: 1731705.94 | Val RMSE: 0.3967\n",
      "Epoch 24/100 | Train Loss: 12986915.88 | Val Loss: 1731501.94 | Val RMSE: 0.3846\n",
      "Epoch 25/100 | Train Loss: 12983160.31 | Val Loss: 1731077.12 | Val RMSE: 0.2905\n",
      "Epoch 26/100 | Train Loss: 12976538.06 | Val Loss: 1729723.81 | Val RMSE: 0.4524\n",
      "Epoch 27/100 | Train Loss: 12971034.25 | Val Loss: 1729152.75 | Val RMSE: 0.6886\n",
      "Epoch 28/100 | Train Loss: 12969757.88 | Val Loss: 1729153.62 | Val RMSE: 0.5617\n",
      "Epoch 29/100 | Train Loss: 12966612.19 | Val Loss: 1728387.25 | Val RMSE: 0.6834\n",
      "Epoch 30/100 | Train Loss: 12959660.06 | Val Loss: 1727438.38 | Val RMSE: 0.3507\n",
      "Epoch 31/100 | Train Loss: 12957187.44 | Val Loss: 1727262.69 | Val RMSE: 0.3541\n",
      "Epoch 32/100 | Train Loss: 12949953.81 | Val Loss: 1726395.69 | Val RMSE: 0.5030\n",
      "Epoch 33/100 | Train Loss: 12947797.25 | Val Loss: 1725741.25 | Val RMSE: 0.6753\n",
      "Epoch 34/100 | Train Loss: 12940946.38 | Val Loss: 1724735.94 | Val RMSE: 0.3839\n",
      "Epoch 35/100 | Train Loss: 12938268.88 | Val Loss: 1724516.25 | Val RMSE: 0.3289\n",
      "Epoch 36/100 | Train Loss: 12933860.25 | Val Loss: 1724105.00 | Val RMSE: 0.5824\n",
      "Epoch 37/100 | Train Loss: 12927847.44 | Val Loss: 1723214.50 | Val RMSE: 0.4448\n",
      "Epoch 38/100 | Train Loss: 12920821.00 | Val Loss: 1722629.69 | Val RMSE: 0.5500\n",
      "Epoch 39/100 | Train Loss: 12921554.12 | Val Loss: 1721975.38 | Val RMSE: 0.3792\n",
      "Epoch 40/100 | Train Loss: 12916450.62 | Val Loss: 1721437.88 | Val RMSE: 0.4628\n",
      "Epoch 41/100 | Train Loss: 12908325.69 | Val Loss: 1720875.12 | Val RMSE: 0.3505\n",
      "Epoch 42/100 | Train Loss: 12906386.81 | Val Loss: 1720177.56 | Val RMSE: 0.4750\n",
      "Epoch 43/100 | Train Loss: 12899228.75 | Val Loss: 1720072.44 | Val RMSE: 0.5823\n",
      "Epoch 44/100 | Train Loss: 12896567.12 | Val Loss: 1718818.00 | Val RMSE: 0.4297\n",
      "Epoch 45/100 | Train Loss: 12894109.56 | Val Loss: 1718748.81 | Val RMSE: 0.2839\n",
      "Epoch 46/100 | Train Loss: 12883356.38 | Val Loss: 1717137.62 | Val RMSE: 0.3142\n",
      "Epoch 47/100 | Train Loss: 12880940.06 | Val Loss: 1717094.38 | Val RMSE: 0.3333\n",
      "Epoch 48/100 | Train Loss: 12875078.31 | Val Loss: 1716446.25 | Val RMSE: 0.3624\n",
      "Epoch 49/100 | Train Loss: 12873377.12 | Val Loss: 1715698.94 | Val RMSE: 0.3740\n",
      "Epoch 50/100 | Train Loss: 12866584.12 | Val Loss: 1715168.19 | Val RMSE: 0.3246\n",
      "Epoch 51/100 | Train Loss: 12862769.62 | Val Loss: 1714307.19 | Val RMSE: 0.4546\n",
      "Epoch 52/100 | Train Loss: 12858289.75 | Val Loss: 1713911.62 | Val RMSE: 0.4965\n",
      "Epoch 53/100 | Train Loss: 12855202.19 | Val Loss: 1713233.94 | Val RMSE: 0.3886\n",
      "Epoch 54/100 | Train Loss: 12848682.25 | Val Loss: 1713106.38 | Val RMSE: 0.4599\n",
      "Epoch 55/100 | Train Loss: 12844375.81 | Val Loss: 1712209.06 | Val RMSE: 0.6639\n",
      "Epoch 56/100 | Train Loss: 12838277.88 | Val Loss: 1711539.19 | Val RMSE: 0.5364\n",
      "Epoch 57/100 | Train Loss: 12837012.12 | Val Loss: 1710881.94 | Val RMSE: 0.4105\n",
      "Epoch 58/100 | Train Loss: 12829734.75 | Val Loss: 1710265.88 | Val RMSE: 0.6769\n",
      "Epoch 59/100 | Train Loss: 12826119.06 | Val Loss: 1709768.88 | Val RMSE: 0.2843\n",
      "Epoch 60/100 | Train Loss: 12820210.19 | Val Loss: 1708924.62 | Val RMSE: 0.2790\n",
      "Epoch 61/100 | Train Loss: 12815955.62 | Val Loss: 1708628.94 | Val RMSE: 0.4451\n",
      "Epoch 62/100 | Train Loss: 12814014.69 | Val Loss: 1707740.12 | Val RMSE: 0.4180\n",
      "Epoch 63/100 | Train Loss: 12807478.88 | Val Loss: 1707247.56 | Val RMSE: 0.3905\n",
      "Epoch 64/100 | Train Loss: 12804895.88 | Val Loss: 1706630.06 | Val RMSE: 0.6686\n",
      "Epoch 65/100 | Train Loss: 12797476.56 | Val Loss: 1706291.75 | Val RMSE: 0.5632\n",
      "Epoch 66/100 | Train Loss: 12791957.81 | Val Loss: 1705030.25 | Val RMSE: 0.3418\n",
      "Epoch 67/100 | Train Loss: 12786659.12 | Val Loss: 1704396.00 | Val RMSE: 0.3417\n",
      "Epoch 68/100 | Train Loss: 12784629.38 | Val Loss: 1704445.69 | Val RMSE: 0.3035\n",
      "Epoch 69/100 | Train Loss: 12779399.12 | Val Loss: 1703303.31 | Val RMSE: 0.2887\n",
      "Epoch 70/100 | Train Loss: 12774420.38 | Val Loss: 1702456.62 | Val RMSE: 0.3099\n",
      "Epoch 71/100 | Train Loss: 12769029.69 | Val Loss: 1702254.62 | Val RMSE: 0.7316\n",
      "Epoch 72/100 | Train Loss: 12767451.19 | Val Loss: 1701467.06 | Val RMSE: 0.9738\n",
      "Epoch 73/100 | Train Loss: 12761249.69 | Val Loss: 1700806.81 | Val RMSE: 0.3731\n",
      "Epoch 74/100 | Train Loss: 12755562.44 | Val Loss: 1700875.12 | Val RMSE: 0.3998\n",
      "Epoch 75/100 | Train Loss: 12750365.94 | Val Loss: 1699931.25 | Val RMSE: 0.4215\n",
      "Epoch 76/100 | Train Loss: 12745598.94 | Val Loss: 1699338.06 | Val RMSE: 0.4334\n",
      "Epoch 77/100 | Train Loss: 12743010.50 | Val Loss: 1698249.00 | Val RMSE: 0.4485\n",
      "Epoch 78/100 | Train Loss: 12734791.69 | Val Loss: 1697547.06 | Val RMSE: 0.6153\n",
      "Epoch 79/100 | Train Loss: 12731378.31 | Val Loss: 1697057.25 | Val RMSE: 0.3284\n",
      "Epoch 80/100 | Train Loss: 12729154.62 | Val Loss: 1696562.25 | Val RMSE: 0.2968\n",
      "Epoch 81/100 | Train Loss: 12723297.12 | Val Loss: 1696099.50 | Val RMSE: 0.4827\n",
      "Epoch 82/100 | Train Loss: 12718589.31 | Val Loss: 1695393.06 | Val RMSE: 0.4185\n",
      "Epoch 83/100 | Train Loss: 12712346.38 | Val Loss: 1694754.81 | Val RMSE: 0.4260\n",
      "Epoch 84/100 | Train Loss: 12707022.12 | Val Loss: 1693729.44 | Val RMSE: 0.2971\n",
      "Epoch 85/100 | Train Loss: 12702465.31 | Val Loss: 1693110.44 | Val RMSE: 0.3125\n",
      "Epoch 86/100 | Train Loss: 12698906.50 | Val Loss: 1692683.94 | Val RMSE: 0.7718\n",
      "Epoch 87/100 | Train Loss: 12694072.12 | Val Loss: 1691971.12 | Val RMSE: 0.3199\n",
      "Epoch 88/100 | Train Loss: 12688027.69 | Val Loss: 1691619.25 | Val RMSE: 0.3387\n",
      "Epoch 89/100 | Train Loss: 12684267.56 | Val Loss: 1691078.19 | Val RMSE: 0.3759\n",
      "Epoch 90/100 | Train Loss: 12678206.56 | Val Loss: 1690335.69 | Val RMSE: 0.5858\n",
      "Epoch 91/100 | Train Loss: 12675061.00 | Val Loss: 1689467.06 | Val RMSE: 0.4162\n",
      "Epoch 92/100 | Train Loss: 12667991.31 | Val Loss: 1688637.06 | Val RMSE: 0.6080\n",
      "Epoch 93/100 | Train Loss: 12663896.75 | Val Loss: 1688211.19 | Val RMSE: 0.4688\n",
      "Epoch 94/100 | Train Loss: 12660429.94 | Val Loss: 1687996.62 | Val RMSE: 0.4215\n",
      "Epoch 95/100 | Train Loss: 12657424.25 | Val Loss: 1687382.56 | Val RMSE: 0.5497\n",
      "Epoch 96/100 | Train Loss: 12651073.69 | Val Loss: 1686238.44 | Val RMSE: 0.5159\n",
      "Epoch 97/100 | Train Loss: 12647334.62 | Val Loss: 1685589.94 | Val RMSE: 0.4187\n",
      "Epoch 98/100 | Train Loss: 12639394.88 | Val Loss: 1685338.00 | Val RMSE: 0.4458\n",
      "Epoch 99/100 | Train Loss: 12635644.56 | Val Loss: 1684111.56 | Val RMSE: 0.5022\n",
      "Epoch 100/100 | Train Loss: 12633450.25 | Val Loss: 1683495.50 | Val RMSE: 0.4822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 12633450.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1683495.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.48224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33melectric-sweep-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/h8c7z9qa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092301-h8c7z9qa/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xrzogen3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092326-xrzogen3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-sweep-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/xrzogen3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstilted-sweep-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/xrzogen3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092326-xrzogen3/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092327-xrzogen3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-sweep-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/xrzogen3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 13372244.00 | Val Loss: 1782428.00 | Val RMSE: 0.5918\n",
      "Epoch 2/100 | Train Loss: 13369156.56 | Val Loss: 1782732.19 | Val RMSE: 0.7394\n",
      "Epoch 3/100 | Train Loss: 13368101.12 | Val Loss: 1782178.19 | Val RMSE: 0.5135\n",
      "Epoch 4/100 | Train Loss: 13365154.00 | Val Loss: 1781832.12 | Val RMSE: 0.5265\n",
      "Epoch 5/100 | Train Loss: 13362344.31 | Val Loss: 1781718.56 | Val RMSE: 0.5046\n",
      "Epoch 6/100 | Train Loss: 13363266.81 | Val Loss: 1781112.38 | Val RMSE: 0.6276\n",
      "Epoch 7/100 | Train Loss: 13358434.31 | Val Loss: 1781036.00 | Val RMSE: 0.5637\n",
      "Epoch 8/100 | Train Loss: 13359871.81 | Val Loss: 1780744.12 | Val RMSE: 0.3607\n",
      "Epoch 9/100 | Train Loss: 13356798.75 | Val Loss: 1780858.19 | Val RMSE: 0.3639\n",
      "Epoch 10/100 | Train Loss: 13357029.19 | Val Loss: 1780424.69 | Val RMSE: 0.3566\n",
      "Epoch 11/100 | Train Loss: 13353363.88 | Val Loss: 1780617.81 | Val RMSE: 0.6422\n",
      "Epoch 12/100 | Train Loss: 13349687.19 | Val Loss: 1779984.69 | Val RMSE: 0.5422\n",
      "Epoch 13/100 | Train Loss: 13349336.31 | Val Loss: 1779727.81 | Val RMSE: 0.4451\n",
      "Epoch 14/100 | Train Loss: 13345676.31 | Val Loss: 1779352.56 | Val RMSE: 0.3509\n",
      "Epoch 15/100 | Train Loss: 13346462.94 | Val Loss: 1779582.25 | Val RMSE: 0.6789\n",
      "Epoch 16/100 | Train Loss: 13344568.31 | Val Loss: 1779240.25 | Val RMSE: 0.3416\n",
      "Epoch 17/100 | Train Loss: 13343753.81 | Val Loss: 1779110.50 | Val RMSE: 0.3951\n",
      "Epoch 18/100 | Train Loss: 13341599.50 | Val Loss: 1778834.88 | Val RMSE: 0.5154\n",
      "Epoch 19/100 | Train Loss: 13341262.62 | Val Loss: 1778211.50 | Val RMSE: 0.3441\n",
      "Epoch 20/100 | Train Loss: 13338871.00 | Val Loss: 1778267.50 | Val RMSE: 0.4231\n",
      "Epoch 21/100 | Train Loss: 13338517.19 | Val Loss: 1778123.81 | Val RMSE: 0.3091\n",
      "Epoch 22/100 | Train Loss: 13335075.31 | Val Loss: 1777497.06 | Val RMSE: 0.3506\n",
      "Epoch 23/100 | Train Loss: 13336300.12 | Val Loss: 1777522.81 | Val RMSE: 0.6554\n",
      "Epoch 24/100 | Train Loss: 13333529.62 | Val Loss: 1777563.81 | Val RMSE: 0.4812\n",
      "Epoch 25/100 | Train Loss: 13331884.00 | Val Loss: 1777043.19 | Val RMSE: 0.3639\n",
      "Epoch 26/100 | Train Loss: 13331926.44 | Val Loss: 1777548.50 | Val RMSE: 0.4939\n",
      "Epoch 27/100 | Train Loss: 13327747.19 | Val Loss: 1776980.06 | Val RMSE: 0.6380\n",
      "Epoch 28/100 | Train Loss: 13325847.19 | Val Loss: 1776528.31 | Val RMSE: 0.3929\n",
      "Epoch 29/100 | Train Loss: 13324170.50 | Val Loss: 1776642.44 | Val RMSE: 0.5324\n",
      "Epoch 30/100 | Train Loss: 13324354.06 | Val Loss: 1776318.75 | Val RMSE: 0.5311\n",
      "Epoch 31/100 | Train Loss: 13321070.44 | Val Loss: 1775892.62 | Val RMSE: 0.3303\n",
      "Epoch 32/100 | Train Loss: 13322232.75 | Val Loss: 1775977.81 | Val RMSE: 0.4662\n",
      "Epoch 33/100 | Train Loss: 13314697.56 | Val Loss: 1775882.56 | Val RMSE: 0.5080\n",
      "Epoch 34/100 | Train Loss: 13313897.12 | Val Loss: 1775763.25 | Val RMSE: 0.4087\n",
      "Epoch 35/100 | Train Loss: 13314470.94 | Val Loss: 1775490.12 | Val RMSE: 0.3726\n",
      "Epoch 36/100 | Train Loss: 13314300.44 | Val Loss: 1775188.00 | Val RMSE: 0.3404\n",
      "Epoch 37/100 | Train Loss: 13313793.25 | Val Loss: 1775112.38 | Val RMSE: 0.5344\n",
      "Epoch 38/100 | Train Loss: 13311030.25 | Val Loss: 1774694.56 | Val RMSE: 0.6063\n",
      "Epoch 39/100 | Train Loss: 13308168.81 | Val Loss: 1774441.38 | Val RMSE: 0.6073\n",
      "Epoch 40/100 | Train Loss: 13306912.38 | Val Loss: 1774156.38 | Val RMSE: 0.3713\n",
      "Epoch 41/100 | Train Loss: 13305053.81 | Val Loss: 1774083.69 | Val RMSE: 0.7025\n",
      "Epoch 42/100 | Train Loss: 13304991.50 | Val Loss: 1773838.56 | Val RMSE: 0.5997\n",
      "Epoch 43/100 | Train Loss: 13302586.69 | Val Loss: 1773586.12 | Val RMSE: 0.3964\n",
      "Epoch 44/100 | Train Loss: 13303421.25 | Val Loss: 1773319.81 | Val RMSE: 0.5381\n",
      "Epoch 45/100 | Train Loss: 13300096.62 | Val Loss: 1773157.31 | Val RMSE: 0.5276\n",
      "Epoch 46/100 | Train Loss: 13296718.38 | Val Loss: 1773475.88 | Val RMSE: 0.5059\n",
      "Epoch 47/100 | Train Loss: 13296847.94 | Val Loss: 1773016.94 | Val RMSE: 1.1333\n",
      "Epoch 48/100 | Train Loss: 13294718.38 | Val Loss: 1772727.81 | Val RMSE: 0.3978\n",
      "Epoch 49/100 | Train Loss: 13294438.44 | Val Loss: 1772149.44 | Val RMSE: 0.3929\n",
      "Epoch 50/100 | Train Loss: 13291307.50 | Val Loss: 1772655.81 | Val RMSE: 0.8373\n",
      "Epoch 51/100 | Train Loss: 13290468.81 | Val Loss: 1772097.94 | Val RMSE: 0.2784\n",
      "Epoch 52/100 | Train Loss: 13290435.56 | Val Loss: 1771876.19 | Val RMSE: 0.3339\n",
      "Epoch 53/100 | Train Loss: 13288675.00 | Val Loss: 1771455.50 | Val RMSE: 0.4433\n",
      "Epoch 54/100 | Train Loss: 13287538.06 | Val Loss: 1771287.75 | Val RMSE: 0.5014\n",
      "Epoch 55/100 | Train Loss: 13283708.00 | Val Loss: 1771416.50 | Val RMSE: 0.3480\n",
      "Epoch 56/100 | Train Loss: 13283542.69 | Val Loss: 1770913.19 | Val RMSE: 0.5804\n",
      "Epoch 57/100 | Train Loss: 13279299.94 | Val Loss: 1770337.19 | Val RMSE: 0.4773\n",
      "Epoch 58/100 | Train Loss: 13278990.88 | Val Loss: 1770492.00 | Val RMSE: 0.5579\n",
      "Epoch 59/100 | Train Loss: 13278209.19 | Val Loss: 1770306.62 | Val RMSE: 0.6293\n",
      "Epoch 60/100 | Train Loss: 13278590.56 | Val Loss: 1770346.50 | Val RMSE: 0.5407\n",
      "Epoch 61/100 | Train Loss: 13275316.94 | Val Loss: 1769666.25 | Val RMSE: 0.3511\n",
      "Epoch 62/100 | Train Loss: 13275011.12 | Val Loss: 1769885.06 | Val RMSE: 0.8351\n",
      "Epoch 63/100 | Train Loss: 13274075.50 | Val Loss: 1769355.25 | Val RMSE: 0.3515\n",
      "Epoch 64/100 | Train Loss: 13270975.81 | Val Loss: 1769334.50 | Val RMSE: 0.3056\n",
      "Epoch 65/100 | Train Loss: 13266271.88 | Val Loss: 1769243.44 | Val RMSE: 0.5697\n",
      "Epoch 66/100 | Train Loss: 13266597.62 | Val Loss: 1769403.81 | Val RMSE: 0.6502\n",
      "Epoch 67/100 | Train Loss: 13265852.06 | Val Loss: 1768806.00 | Val RMSE: 0.4363\n",
      "Epoch 68/100 | Train Loss: 13263572.56 | Val Loss: 1768382.25 | Val RMSE: 0.5933\n",
      "Epoch 69/100 | Train Loss: 13263786.38 | Val Loss: 1767942.56 | Val RMSE: 0.8250\n",
      "Epoch 70/100 | Train Loss: 13260284.12 | Val Loss: 1768109.62 | Val RMSE: 0.7776\n",
      "Epoch 71/100 | Train Loss: 13262241.12 | Val Loss: 1768155.69 | Val RMSE: 0.4686\n",
      "Epoch 72/100 | Train Loss: 13256975.38 | Val Loss: 1767905.94 | Val RMSE: 0.4285\n",
      "Epoch 73/100 | Train Loss: 13255821.00 | Val Loss: 1767252.44 | Val RMSE: 0.4618\n",
      "Epoch 74/100 | Train Loss: 13254011.75 | Val Loss: 1767179.62 | Val RMSE: 0.5499\n",
      "Epoch 75/100 | Train Loss: 13252828.50 | Val Loss: 1767162.38 | Val RMSE: 0.3310\n",
      "Epoch 76/100 | Train Loss: 13254979.56 | Val Loss: 1766710.94 | Val RMSE: 0.5986\n",
      "Epoch 77/100 | Train Loss: 13247726.94 | Val Loss: 1766597.19 | Val RMSE: 0.7198\n",
      "Epoch 78/100 | Train Loss: 13249513.62 | Val Loss: 1765974.56 | Val RMSE: 0.3605\n",
      "Epoch 79/100 | Train Loss: 13251230.62 | Val Loss: 1765892.06 | Val RMSE: 0.3060\n",
      "Epoch 80/100 | Train Loss: 13248878.25 | Val Loss: 1766011.69 | Val RMSE: 0.3914\n",
      "Epoch 81/100 | Train Loss: 13244349.44 | Val Loss: 1765705.00 | Val RMSE: 0.3362\n",
      "Epoch 82/100 | Train Loss: 13242808.19 | Val Loss: 1765416.69 | Val RMSE: 0.4015\n",
      "Epoch 83/100 | Train Loss: 13242178.56 | Val Loss: 1765154.81 | Val RMSE: 0.3546\n",
      "Epoch 84/100 | Train Loss: 13238312.00 | Val Loss: 1765242.62 | Val RMSE: 0.4164\n",
      "Epoch 85/100 | Train Loss: 13236953.44 | Val Loss: 1764916.88 | Val RMSE: 0.3194\n",
      "Epoch 86/100 | Train Loss: 13236488.94 | Val Loss: 1764392.69 | Val RMSE: 0.7290\n",
      "Epoch 87/100 | Train Loss: 13234389.19 | Val Loss: 1764694.19 | Val RMSE: 0.3447\n",
      "Epoch 88/100 | Train Loss: 13235931.19 | Val Loss: 1764184.25 | Val RMSE: 0.6897\n",
      "Epoch 89/100 | Train Loss: 13233112.25 | Val Loss: 1764002.69 | Val RMSE: 0.5220\n",
      "Epoch 90/100 | Train Loss: 13233317.94 | Val Loss: 1763908.88 | Val RMSE: 0.3967\n",
      "Epoch 91/100 | Train Loss: 13229811.56 | Val Loss: 1763541.62 | Val RMSE: 0.5345\n",
      "Epoch 92/100 | Train Loss: 13226322.25 | Val Loss: 1763003.12 | Val RMSE: 0.3621\n",
      "Epoch 93/100 | Train Loss: 13225328.31 | Val Loss: 1763192.56 | Val RMSE: 0.3180\n",
      "Epoch 94/100 | Train Loss: 13222835.19 | Val Loss: 1763342.12 | Val RMSE: 0.3329\n",
      "Epoch 95/100 | Train Loss: 13223499.81 | Val Loss: 1762795.31 | Val RMSE: 0.4537\n",
      "Epoch 96/100 | Train Loss: 13220931.25 | Val Loss: 1762591.38 | Val RMSE: 0.3696\n",
      "Epoch 97/100 | Train Loss: 13218675.69 | Val Loss: 1762521.31 | Val RMSE: 0.3128\n",
      "Epoch 98/100 | Train Loss: 13219478.19 | Val Loss: 1762208.69 | Val RMSE: 0.4106\n",
      "Epoch 99/100 | Train Loss: 13214783.56 | Val Loss: 1761954.19 | Val RMSE: 0.3139\n",
      "Epoch 100/100 | Train Loss: 13212914.75 | Val Loss: 1761577.81 | Val RMSE: 0.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 13212914.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1761577.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.34293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstilted-sweep-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/xrzogen3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092327-xrzogen3/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 33puzsjj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0024787521766663585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092347-33puzsjj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdenim-sweep-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/33puzsjj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdenim-sweep-15\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/33puzsjj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092347-33puzsjj/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092348-33puzsjj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdenim-sweep-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/33puzsjj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 10441983.56 | Val Loss: 1391242.62 | Val RMSE: 1.2844\n",
      "Epoch 2/100 | Train Loss: 10434884.12 | Val Loss: 1390803.31 | Val RMSE: 0.5995\n",
      "Epoch 3/100 | Train Loss: 10428824.56 | Val Loss: 1390519.50 | Val RMSE: 0.6479\n",
      "Epoch 4/100 | Train Loss: 10424312.19 | Val Loss: 1389241.75 | Val RMSE: 1.0744\n",
      "Epoch 5/100 | Train Loss: 10420045.31 | Val Loss: 1389051.88 | Val RMSE: 1.1253\n",
      "Epoch 6/100 | Train Loss: 10418816.00 | Val Loss: 1388649.31 | Val RMSE: 0.7851\n",
      "Epoch 7/100 | Train Loss: 10413674.62 | Val Loss: 1387759.62 | Val RMSE: 0.5266\n",
      "Epoch 8/100 | Train Loss: 10408506.94 | Val Loss: 1387419.81 | Val RMSE: 0.4605\n",
      "Epoch 9/100 | Train Loss: 10403132.81 | Val Loss: 1386740.12 | Val RMSE: 0.3247\n",
      "Epoch 10/100 | Train Loss: 10400569.19 | Val Loss: 1386053.44 | Val RMSE: 0.6764\n",
      "Epoch 11/100 | Train Loss: 10395323.44 | Val Loss: 1385397.31 | Val RMSE: 0.3651\n",
      "Epoch 12/100 | Train Loss: 10391851.00 | Val Loss: 1385208.19 | Val RMSE: 0.4159\n",
      "Epoch 13/100 | Train Loss: 10388402.56 | Val Loss: 1384659.19 | Val RMSE: 0.4105\n",
      "Epoch 14/100 | Train Loss: 10383257.88 | Val Loss: 1384277.81 | Val RMSE: 0.3759\n",
      "Epoch 15/100 | Train Loss: 10379426.38 | Val Loss: 1383427.38 | Val RMSE: 0.4340\n",
      "Epoch 16/100 | Train Loss: 10374528.19 | Val Loss: 1382584.12 | Val RMSE: 0.5122\n",
      "Epoch 17/100 | Train Loss: 10372159.50 | Val Loss: 1382731.12 | Val RMSE: 0.3268\n",
      "Epoch 18/100 | Train Loss: 10369873.94 | Val Loss: 1382071.94 | Val RMSE: 0.4421\n",
      "Epoch 19/100 | Train Loss: 10366495.56 | Val Loss: 1381990.81 | Val RMSE: 0.3851\n",
      "Epoch 20/100 | Train Loss: 10359548.00 | Val Loss: 1380683.00 | Val RMSE: 0.4137\n",
      "Epoch 21/100 | Train Loss: 10354939.31 | Val Loss: 1379902.38 | Val RMSE: 0.4969\n",
      "Epoch 22/100 | Train Loss: 10349559.12 | Val Loss: 1379544.19 | Val RMSE: 0.3575\n",
      "Epoch 23/100 | Train Loss: 10345616.38 | Val Loss: 1379100.12 | Val RMSE: 0.3834\n",
      "Epoch 24/100 | Train Loss: 10343851.56 | Val Loss: 1378560.88 | Val RMSE: 0.4969\n",
      "Epoch 25/100 | Train Loss: 10340346.69 | Val Loss: 1378165.56 | Val RMSE: 0.3592\n",
      "Epoch 26/100 | Train Loss: 10336295.44 | Val Loss: 1377522.81 | Val RMSE: 0.4803\n",
      "Epoch 27/100 | Train Loss: 10332750.88 | Val Loss: 1376749.81 | Val RMSE: 0.6235\n",
      "Epoch 28/100 | Train Loss: 10326177.38 | Val Loss: 1376145.12 | Val RMSE: 0.4058\n",
      "Epoch 29/100 | Train Loss: 10321901.25 | Val Loss: 1375619.94 | Val RMSE: 0.4935\n",
      "Epoch 30/100 | Train Loss: 10319903.50 | Val Loss: 1375533.69 | Val RMSE: 0.8057\n",
      "Epoch 31/100 | Train Loss: 10312127.69 | Val Loss: 1374572.50 | Val RMSE: 0.4773\n",
      "Epoch 32/100 | Train Loss: 10310017.94 | Val Loss: 1374216.00 | Val RMSE: 0.4990\n",
      "Epoch 33/100 | Train Loss: 10307663.00 | Val Loss: 1373635.94 | Val RMSE: 0.3453\n",
      "Epoch 34/100 | Train Loss: 10301979.06 | Val Loss: 1373140.75 | Val RMSE: 0.3506\n",
      "Epoch 35/100 | Train Loss: 10299100.94 | Val Loss: 1372729.06 | Val RMSE: 0.6406\n",
      "Epoch 36/100 | Train Loss: 10292241.44 | Val Loss: 1371631.56 | Val RMSE: 0.4697\n",
      "Epoch 37/100 | Train Loss: 10289227.75 | Val Loss: 1371283.75 | Val RMSE: 0.5878\n",
      "Epoch 38/100 | Train Loss: 10285752.31 | Val Loss: 1370729.69 | Val RMSE: 0.4417\n",
      "Epoch 39/100 | Train Loss: 10279476.19 | Val Loss: 1370215.19 | Val RMSE: 0.6492\n",
      "Epoch 40/100 | Train Loss: 10275185.25 | Val Loss: 1370071.62 | Val RMSE: 0.3570\n",
      "Epoch 41/100 | Train Loss: 10270373.56 | Val Loss: 1369057.31 | Val RMSE: 0.3243\n",
      "Epoch 42/100 | Train Loss: 10267157.38 | Val Loss: 1368689.88 | Val RMSE: 0.3331\n",
      "Epoch 43/100 | Train Loss: 10261208.56 | Val Loss: 1367552.19 | Val RMSE: 0.5204\n",
      "Epoch 44/100 | Train Loss: 10260443.25 | Val Loss: 1367625.50 | Val RMSE: 0.5147\n",
      "Epoch 45/100 | Train Loss: 10255328.50 | Val Loss: 1367112.31 | Val RMSE: 0.4952\n",
      "Epoch 46/100 | Train Loss: 10250201.19 | Val Loss: 1366422.81 | Val RMSE: 0.3269\n",
      "Epoch 47/100 | Train Loss: 10246336.50 | Val Loss: 1365697.94 | Val RMSE: 0.5727\n",
      "Epoch 48/100 | Train Loss: 10239483.44 | Val Loss: 1365076.88 | Val RMSE: 0.3363\n",
      "Epoch 49/100 | Train Loss: 10238707.38 | Val Loss: 1364307.44 | Val RMSE: 0.7097\n",
      "Epoch 50/100 | Train Loss: 10233213.38 | Val Loss: 1363773.38 | Val RMSE: 0.5092\n",
      "Epoch 51/100 | Train Loss: 10230028.75 | Val Loss: 1363928.12 | Val RMSE: 0.5435\n",
      "Epoch 52/100 | Train Loss: 10225672.12 | Val Loss: 1363254.06 | Val RMSE: 0.4301\n",
      "Epoch 53/100 | Train Loss: 10220627.56 | Val Loss: 1362393.75 | Val RMSE: 0.5240\n",
      "Epoch 54/100 | Train Loss: 10215023.00 | Val Loss: 1361459.38 | Val RMSE: 0.5275\n",
      "Epoch 55/100 | Train Loss: 10212227.62 | Val Loss: 1361379.50 | Val RMSE: 0.3843\n",
      "Epoch 56/100 | Train Loss: 10207096.94 | Val Loss: 1360656.25 | Val RMSE: 0.5222\n",
      "Epoch 57/100 | Train Loss: 10204646.50 | Val Loss: 1359872.19 | Val RMSE: 0.3846\n",
      "Epoch 58/100 | Train Loss: 10196954.62 | Val Loss: 1359102.31 | Val RMSE: 0.4805\n",
      "Epoch 59/100 | Train Loss: 10193467.50 | Val Loss: 1358922.94 | Val RMSE: 0.3233\n",
      "Epoch 60/100 | Train Loss: 10191073.00 | Val Loss: 1358085.50 | Val RMSE: 0.4729\n",
      "Epoch 61/100 | Train Loss: 10182350.62 | Val Loss: 1357686.56 | Val RMSE: 0.4229\n",
      "Epoch 62/100 | Train Loss: 10181604.50 | Val Loss: 1357035.12 | Val RMSE: 0.4588\n",
      "Epoch 63/100 | Train Loss: 10177067.94 | Val Loss: 1356608.88 | Val RMSE: 0.3393\n",
      "Epoch 64/100 | Train Loss: 10174512.25 | Val Loss: 1355867.12 | Val RMSE: 0.3660\n",
      "Epoch 65/100 | Train Loss: 10167959.44 | Val Loss: 1355409.19 | Val RMSE: 0.4331\n",
      "Epoch 66/100 | Train Loss: 10165233.56 | Val Loss: 1354499.69 | Val RMSE: 0.3891\n",
      "Epoch 67/100 | Train Loss: 10160055.88 | Val Loss: 1354411.12 | Val RMSE: 0.3598\n",
      "Epoch 68/100 | Train Loss: 10155382.62 | Val Loss: 1353612.62 | Val RMSE: 0.3578\n",
      "Epoch 69/100 | Train Loss: 10152510.94 | Val Loss: 1353125.69 | Val RMSE: 0.4621\n",
      "Epoch 70/100 | Train Loss: 10145570.00 | Val Loss: 1352533.12 | Val RMSE: 0.7073\n",
      "Epoch 71/100 | Train Loss: 10144384.31 | Val Loss: 1351892.06 | Val RMSE: 0.4745\n",
      "Epoch 72/100 | Train Loss: 10136188.12 | Val Loss: 1351384.00 | Val RMSE: 0.3819\n",
      "Epoch 73/100 | Train Loss: 10131614.25 | Val Loss: 1350713.56 | Val RMSE: 0.3778\n",
      "Epoch 74/100 | Train Loss: 10129044.12 | Val Loss: 1350126.56 | Val RMSE: 0.3264\n",
      "Epoch 75/100 | Train Loss: 10126316.50 | Val Loss: 1349597.06 | Val RMSE: 0.4212\n",
      "Epoch 76/100 | Train Loss: 10118229.12 | Val Loss: 1348658.50 | Val RMSE: 0.6036\n",
      "Epoch 77/100 | Train Loss: 10116844.25 | Val Loss: 1348413.94 | Val RMSE: 0.4504\n",
      "Epoch 78/100 | Train Loss: 10111147.88 | Val Loss: 1347677.12 | Val RMSE: 0.3990\n",
      "Epoch 79/100 | Train Loss: 10108959.25 | Val Loss: 1347145.25 | Val RMSE: 0.3536\n",
      "Epoch 80/100 | Train Loss: 10105688.44 | Val Loss: 1346413.06 | Val RMSE: 0.3095\n",
      "Epoch 81/100 | Train Loss: 10096942.12 | Val Loss: 1345987.88 | Val RMSE: 0.3436\n",
      "Epoch 82/100 | Train Loss: 10096394.62 | Val Loss: 1345424.75 | Val RMSE: 0.3589\n",
      "Epoch 83/100 | Train Loss: 10087291.62 | Val Loss: 1344883.88 | Val RMSE: 0.4310\n",
      "Epoch 84/100 | Train Loss: 10084043.38 | Val Loss: 1344580.50 | Val RMSE: 0.6299\n",
      "Epoch 85/100 | Train Loss: 10080311.81 | Val Loss: 1343439.12 | Val RMSE: 0.3597\n",
      "Epoch 86/100 | Train Loss: 10073595.56 | Val Loss: 1342934.00 | Val RMSE: 0.4645\n",
      "Epoch 87/100 | Train Loss: 10072352.38 | Val Loss: 1342719.31 | Val RMSE: 0.4171\n",
      "Epoch 88/100 | Train Loss: 10067761.25 | Val Loss: 1341881.12 | Val RMSE: 0.4222\n",
      "Epoch 89/100 | Train Loss: 10063223.12 | Val Loss: 1340927.56 | Val RMSE: 0.5870\n",
      "Epoch 90/100 | Train Loss: 10058604.81 | Val Loss: 1340832.50 | Val RMSE: 0.9588\n",
      "Epoch 91/100 | Train Loss: 10051251.06 | Val Loss: 1339628.25 | Val RMSE: 0.3404\n",
      "Epoch 92/100 | Train Loss: 10046199.00 | Val Loss: 1338984.62 | Val RMSE: 0.4675\n",
      "Epoch 93/100 | Train Loss: 10039715.31 | Val Loss: 1338270.50 | Val RMSE: 0.3651\n",
      "Epoch 94/100 | Train Loss: 10038583.50 | Val Loss: 1337686.62 | Val RMSE: 0.4256\n",
      "Epoch 95/100 | Train Loss: 10034817.88 | Val Loss: 1337344.06 | Val RMSE: 0.4511\n",
      "Epoch 96/100 | Train Loss: 10028427.56 | Val Loss: 1336564.25 | Val RMSE: 0.4814\n",
      "Epoch 97/100 | Train Loss: 10025472.62 | Val Loss: 1336046.62 | Val RMSE: 0.3871\n",
      "Epoch 98/100 | Train Loss: 10017828.81 | Val Loss: 1335661.62 | Val RMSE: 0.5909\n",
      "Epoch 99/100 | Train Loss: 10014481.56 | Val Loss: 1334716.88 | Val RMSE: 0.5276\n",
      "Epoch 100/100 | Train Loss: 10009559.12 | Val Loss: 1333982.88 | Val RMSE: 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 10009559.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1333982.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.36388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdenim-sweep-15\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/33puzsjj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092348-33puzsjj/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o4cuac4v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_pi: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma1: 0.36787944117144233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprior_sigma2: 0.0009118819655545162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092413-o4cuac4v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-sweep-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/o4cuac4v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Project-ASI' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfine-sweep-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/o4cuac4v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092413-o4cuac4v/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250608_092414-o4cuac4v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-sweep-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/sweeps/6pjj1s0i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/o4cuac4v\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 10651524.69 | Val Loss: 1419626.00 | Val RMSE: 0.4507\n",
      "Epoch 2/100 | Train Loss: 10644025.56 | Val Loss: 1419517.94 | Val RMSE: 0.5636\n",
      "Epoch 3/100 | Train Loss: 10646265.75 | Val Loss: 1419013.94 | Val RMSE: 0.5183\n",
      "Epoch 4/100 | Train Loss: 10642503.12 | Val Loss: 1418808.81 | Val RMSE: 0.3315\n",
      "Epoch 5/100 | Train Loss: 10638602.44 | Val Loss: 1418039.69 | Val RMSE: 0.4204\n",
      "Epoch 6/100 | Train Loss: 10637075.12 | Val Loss: 1418135.38 | Val RMSE: 0.3864\n",
      "Epoch 7/100 | Train Loss: 10634814.88 | Val Loss: 1417350.12 | Val RMSE: 0.3275\n",
      "Epoch 8/100 | Train Loss: 10634539.69 | Val Loss: 1417331.00 | Val RMSE: 0.4271\n",
      "Epoch 9/100 | Train Loss: 10629762.00 | Val Loss: 1416995.56 | Val RMSE: 0.4139\n",
      "Epoch 10/100 | Train Loss: 10627875.06 | Val Loss: 1417011.31 | Val RMSE: 0.4643\n",
      "Epoch 11/100 | Train Loss: 10626108.94 | Val Loss: 1416351.88 | Val RMSE: 0.3194\n",
      "Epoch 12/100 | Train Loss: 10623092.38 | Val Loss: 1416227.81 | Val RMSE: 0.5110\n",
      "Epoch 13/100 | Train Loss: 10620100.56 | Val Loss: 1416234.94 | Val RMSE: 0.6209\n",
      "Epoch 14/100 | Train Loss: 10618726.38 | Val Loss: 1415655.25 | Val RMSE: 0.3306\n",
      "Epoch 15/100 | Train Loss: 10617803.81 | Val Loss: 1415440.06 | Val RMSE: 0.3686\n",
      "Epoch 16/100 | Train Loss: 10614019.12 | Val Loss: 1415406.25 | Val RMSE: 0.3208\n",
      "Epoch 17/100 | Train Loss: 10614175.31 | Val Loss: 1414409.56 | Val RMSE: 0.5292\n",
      "Epoch 18/100 | Train Loss: 10610557.56 | Val Loss: 1414287.50 | Val RMSE: 0.3666\n",
      "Epoch 19/100 | Train Loss: 10610776.81 | Val Loss: 1414308.00 | Val RMSE: 0.4251\n",
      "Epoch 20/100 | Train Loss: 10608487.56 | Val Loss: 1414328.56 | Val RMSE: 0.3486\n",
      "Epoch 21/100 | Train Loss: 10607838.69 | Val Loss: 1413783.81 | Val RMSE: 0.3660\n",
      "Epoch 22/100 | Train Loss: 10603671.75 | Val Loss: 1413028.06 | Val RMSE: 0.3944\n",
      "Epoch 23/100 | Train Loss: 10600486.00 | Val Loss: 1413301.19 | Val RMSE: 0.5254\n",
      "Epoch 24/100 | Train Loss: 10600683.69 | Val Loss: 1413007.88 | Val RMSE: 0.4650\n",
      "Epoch 25/100 | Train Loss: 10599220.94 | Val Loss: 1412817.44 | Val RMSE: 0.5435\n",
      "Epoch 26/100 | Train Loss: 10594302.31 | Val Loss: 1412464.12 | Val RMSE: 0.3446\n",
      "Epoch 27/100 | Train Loss: 10593348.25 | Val Loss: 1412594.25 | Val RMSE: 0.4622\n",
      "Epoch 28/100 | Train Loss: 10591116.31 | Val Loss: 1412005.56 | Val RMSE: 0.8847\n",
      "Epoch 29/100 | Train Loss: 10589941.88 | Val Loss: 1411651.12 | Val RMSE: 0.3756\n",
      "Epoch 30/100 | Train Loss: 10586644.56 | Val Loss: 1411835.38 | Val RMSE: 0.3642\n",
      "Epoch 31/100 | Train Loss: 10583757.69 | Val Loss: 1411558.62 | Val RMSE: 0.7292\n",
      "Epoch 32/100 | Train Loss: 10585399.00 | Val Loss: 1411103.19 | Val RMSE: 0.3642\n",
      "Epoch 33/100 | Train Loss: 10582594.62 | Val Loss: 1410849.62 | Val RMSE: 0.8031\n",
      "Epoch 34/100 | Train Loss: 10579721.75 | Val Loss: 1410269.50 | Val RMSE: 0.4957\n",
      "Epoch 35/100 | Train Loss: 10577730.50 | Val Loss: 1410047.38 | Val RMSE: 0.3277\n",
      "Epoch 36/100 | Train Loss: 10575191.12 | Val Loss: 1409799.00 | Val RMSE: 0.6741\n",
      "Epoch 37/100 | Train Loss: 10575846.75 | Val Loss: 1409863.56 | Val RMSE: 0.3314\n",
      "Epoch 38/100 | Train Loss: 10571329.25 | Val Loss: 1409225.75 | Val RMSE: 0.7541\n",
      "Epoch 39/100 | Train Loss: 10571707.25 | Val Loss: 1409130.81 | Val RMSE: 0.3728\n",
      "Epoch 40/100 | Train Loss: 10569530.06 | Val Loss: 1408847.94 | Val RMSE: 0.3157\n",
      "Epoch 41/100 | Train Loss: 10565142.69 | Val Loss: 1408997.88 | Val RMSE: 0.4681\n",
      "Epoch 42/100 | Train Loss: 10565717.06 | Val Loss: 1408448.12 | Val RMSE: 0.4275\n",
      "Epoch 43/100 | Train Loss: 10563706.62 | Val Loss: 1408601.25 | Val RMSE: 0.5247\n",
      "Epoch 44/100 | Train Loss: 10561184.81 | Val Loss: 1407638.00 | Val RMSE: 0.3438\n",
      "Epoch 45/100 | Train Loss: 10559518.19 | Val Loss: 1407636.62 | Val RMSE: 0.4127\n",
      "Epoch 46/100 | Train Loss: 10558411.81 | Val Loss: 1407554.56 | Val RMSE: 0.3010\n",
      "Epoch 47/100 | Train Loss: 10553617.12 | Val Loss: 1407059.62 | Val RMSE: 0.5100\n",
      "Epoch 48/100 | Train Loss: 10552060.56 | Val Loss: 1406581.31 | Val RMSE: 0.3295\n",
      "Epoch 49/100 | Train Loss: 10551649.50 | Val Loss: 1406183.94 | Val RMSE: 0.3329\n",
      "Epoch 50/100 | Train Loss: 10547485.06 | Val Loss: 1406241.56 | Val RMSE: 0.3159\n",
      "Epoch 51/100 | Train Loss: 10546698.56 | Val Loss: 1405996.00 | Val RMSE: 0.3593\n",
      "Epoch 52/100 | Train Loss: 10544400.12 | Val Loss: 1405518.06 | Val RMSE: 0.3148\n",
      "Epoch 53/100 | Train Loss: 10543959.62 | Val Loss: 1405539.19 | Val RMSE: 0.3830\n",
      "Epoch 54/100 | Train Loss: 10541343.81 | Val Loss: 1405586.44 | Val RMSE: 0.3470\n",
      "Epoch 55/100 | Train Loss: 10538255.25 | Val Loss: 1404830.56 | Val RMSE: 0.3116\n",
      "Epoch 56/100 | Train Loss: 10538172.56 | Val Loss: 1405065.44 | Val RMSE: 0.4251\n",
      "Epoch 57/100 | Train Loss: 10536535.38 | Val Loss: 1404723.88 | Val RMSE: 0.4636\n",
      "Epoch 58/100 | Train Loss: 10535505.19 | Val Loss: 1404627.19 | Val RMSE: 0.4705\n",
      "Epoch 59/100 | Train Loss: 10532864.25 | Val Loss: 1403897.94 | Val RMSE: 0.5191\n",
      "Epoch 60/100 | Train Loss: 10528680.38 | Val Loss: 1403633.12 | Val RMSE: 0.4540\n",
      "Epoch 61/100 | Train Loss: 10530091.38 | Val Loss: 1403538.44 | Val RMSE: 0.3387\n",
      "Epoch 62/100 | Train Loss: 10526337.69 | Val Loss: 1403089.44 | Val RMSE: 0.4321\n",
      "Epoch 63/100 | Train Loss: 10526318.38 | Val Loss: 1403211.62 | Val RMSE: 0.3885\n",
      "Epoch 64/100 | Train Loss: 10522357.31 | Val Loss: 1403011.19 | Val RMSE: 0.4769\n",
      "Epoch 65/100 | Train Loss: 10522724.62 | Val Loss: 1402584.62 | Val RMSE: 0.3815\n",
      "Epoch 66/100 | Train Loss: 10520021.06 | Val Loss: 1402143.88 | Val RMSE: 0.3209\n",
      "Epoch 67/100 | Train Loss: 10515345.44 | Val Loss: 1402039.00 | Val RMSE: 0.4367\n",
      "Epoch 68/100 | Train Loss: 10514026.19 | Val Loss: 1401778.25 | Val RMSE: 0.4891\n",
      "Epoch 69/100 | Train Loss: 10514447.88 | Val Loss: 1401634.38 | Val RMSE: 0.4011\n",
      "Epoch 70/100 | Train Loss: 10510441.88 | Val Loss: 1401141.19 | Val RMSE: 0.3052\n",
      "Epoch 71/100 | Train Loss: 10510468.62 | Val Loss: 1401079.44 | Val RMSE: 0.4171\n",
      "Epoch 72/100 | Train Loss: 10506974.25 | Val Loss: 1400811.44 | Val RMSE: 0.5365\n",
      "Epoch 73/100 | Train Loss: 10507788.44 | Val Loss: 1400552.88 | Val RMSE: 0.3362\n",
      "Epoch 74/100 | Train Loss: 10504769.31 | Val Loss: 1399880.62 | Val RMSE: 0.3165\n",
      "Epoch 75/100 | Train Loss: 10503313.94 | Val Loss: 1400010.06 | Val RMSE: 0.3199\n",
      "Epoch 76/100 | Train Loss: 10498953.25 | Val Loss: 1399896.69 | Val RMSE: 0.6850\n",
      "Epoch 77/100 | Train Loss: 10499156.94 | Val Loss: 1399402.31 | Val RMSE: 0.3904\n",
      "Epoch 78/100 | Train Loss: 10495866.81 | Val Loss: 1399560.88 | Val RMSE: 0.7216\n",
      "Epoch 79/100 | Train Loss: 10494450.06 | Val Loss: 1399054.69 | Val RMSE: 0.2867\n",
      "Epoch 80/100 | Train Loss: 10492199.50 | Val Loss: 1398726.12 | Val RMSE: 0.5217\n",
      "Epoch 81/100 | Train Loss: 10488648.62 | Val Loss: 1398566.00 | Val RMSE: 0.3017\n",
      "Epoch 82/100 | Train Loss: 10490487.56 | Val Loss: 1397742.62 | Val RMSE: 0.3916\n",
      "Epoch 83/100 | Train Loss: 10488182.81 | Val Loss: 1398120.38 | Val RMSE: 0.3920\n",
      "Epoch 84/100 | Train Loss: 10484627.00 | Val Loss: 1397614.38 | Val RMSE: 0.3610\n",
      "Epoch 85/100 | Train Loss: 10484941.94 | Val Loss: 1397351.81 | Val RMSE: 0.4269\n",
      "Epoch 86/100 | Train Loss: 10480621.50 | Val Loss: 1396980.06 | Val RMSE: 0.4083\n",
      "Epoch 87/100 | Train Loss: 10480228.12 | Val Loss: 1397246.06 | Val RMSE: 0.5083\n",
      "Epoch 88/100 | Train Loss: 10473710.31 | Val Loss: 1396954.81 | Val RMSE: 0.3160\n",
      "Epoch 89/100 | Train Loss: 10476136.62 | Val Loss: 1396095.69 | Val RMSE: 0.4121\n",
      "Epoch 90/100 | Train Loss: 10472714.81 | Val Loss: 1396597.25 | Val RMSE: 0.3438\n",
      "Epoch 91/100 | Train Loss: 10472627.56 | Val Loss: 1395835.75 | Val RMSE: 0.3714\n",
      "Epoch 92/100 | Train Loss: 10471076.44 | Val Loss: 1395639.19 | Val RMSE: 0.5266\n",
      "Epoch 93/100 | Train Loss: 10467460.44 | Val Loss: 1395526.00 | Val RMSE: 0.4524\n",
      "Epoch 94/100 | Train Loss: 10465739.88 | Val Loss: 1395440.25 | Val RMSE: 0.5912\n",
      "Epoch 95/100 | Train Loss: 10464301.25 | Val Loss: 1395122.75 | Val RMSE: 0.3147\n",
      "Epoch 96/100 | Train Loss: 10460052.50 | Val Loss: 1395160.06 | Val RMSE: 0.6329\n",
      "Epoch 97/100 | Train Loss: 10458084.94 | Val Loss: 1394667.94 | Val RMSE: 0.4900\n",
      "Epoch 98/100 | Train Loss: 10456671.88 | Val Loss: 1393836.69 | Val RMSE: 0.3035\n",
      "Epoch 99/100 | Train Loss: 10456500.19 | Val Loss: 1393863.38 | Val RMSE: 0.3314\n",
      "Epoch 100/100 | Train Loss: 10455095.44 | Val Loss: 1393360.00 | Val RMSE: 0.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 10455095.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 1393360.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse 0.32271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfine-sweep-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI/runs/o4cuac4v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/miriam-lamari2-eurecom/Project-ASI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250608_092414-o4cuac4v/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "sweep_configuration = {\n",
    "     \"method\": \"grid\",\n",
    "     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "     'name': \"sweep-BBB-Gaussian_regression\",\n",
    "     \"parameters\": {\n",
    "         \"learning_rate\": {'values': [1e-3, 1e-4]},\n",
    "         \"prior_pi\": {'values': [0.25, 0.5]},\n",
    "         \"prior_sigma1\": {'values': [1, math.exp(-1)]},\n",
    "         \"prior_sigma2\": {'values': [math.exp(-6), math.exp(-7)]},\n",
    "     },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"Project-ASI\")\n",
    "wandb.agent(sweep_id, function=train_wrapper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10fb9daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T09:24:42.492511Z",
     "iopub.status.busy": "2025-06-08T09:24:42.491761Z",
     "iopub.status.idle": "2025-06-08T09:24:42.495434Z",
     "shell.execute_reply": "2025-06-08T09:24:42.494706Z"
    },
    "papermill": {
     "duration": 0.058022,
     "end_time": "2025-06-08T09:24:42.496624",
     "exception": false,
     "start_time": "2025-06-08T09:24:42.438602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.init(project=\"Project-ASI\")\n",
    "\n",
    "#train_loop(\n",
    "#    learning_rate = best_hyperparameters['learning_rate'],\n",
    "#    prior_pi = best_hyperparameters['prior_pi'],\n",
    "#    prior_sigma1 = best_hyperparameters['prior_sigma1'],\n",
    "#    prior_sigma2 = best_hyperparameters['prior_sigma2'],\n",
    "#    epochs = 2\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c460f2",
   "metadata": {
    "papermill": {
     "duration": 0.053188,
     "end_time": "2025-06-08T09:24:42.602709",
     "exception": false,
     "start_time": "2025-06-08T09:24:42.549521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 439.348866,
   "end_time": "2025-06-08T09:24:46.127235",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-08T09:17:26.778369",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
